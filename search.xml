<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[第一章: Model and Cost Function]]></title>
    <url>%2F2018%2F11%2F15%2F%E7%AC%AC%E4%B8%80%E7%AB%A0-Model-and-Cost-Function%2F</url>
    <content type="text"><![CDATA[第一章： Model and Cost FunctionModel Representation:模型表示 我们的第一个学习算法是线性回归算法,了解监督学习过程完整的流程 例子 这个例子是预测住房价格的 我们要使用一个数据集 数据集包含俄勒冈州波特兰市的住房价格 在这里 我要根据不同房屋尺寸所售出的价格 画出我的数据集 比方说 我们来看这个数据集 你有一个朋友正想出售自己的房子 如果你朋友的房子是1250平方尺大小 你要告诉他们 这房子能卖多少钱 那么 你可以做的一件事就是 构建一个模型 也许是条直线 从这个数据模型上来看 也许你可以告诉你的朋友 他能以大约220000(美元)左右的价格 卖掉这个房子 那么这就是监督学习算法的一个例子 它被称作监督学习是因为对于每个数据来说 我们给出了 “正确的答案” 即告诉我们 根据我们的数据来说 房子实际的价格是多少 而且 更具体来说 这是一个回归问题 回归一词指的是我们根据之前的数据预测出一个准确的输出值 对于这个例子就是价格 同时 还有另一种最常见的监督学习方式 叫做分类问题 当我们想要预测离散的输出值 例如 如果我们正在寻找 癌症肿瘤并想要确定 肿瘤是良性的还是恶性的 这就是0/1离散输出的问题 更进一步来说 在监督学习中我们有一个数据集 这个数据集被称训练集 因此对于房价的例子 我们有一个训练集 包含不同的房屋价格 我们的任务就是从这个数据中学习预测房屋价格 现在我们给出这门课中经常使用的一些符号定义 我们要定义颇多符号 不过没关系 现在你记不住所有的符号也没关系 随着课程的进展 你会发现记住这些符号会很有用 我将在整个课程中用小写的 m:来表示训练样本的数目 因此 在这个数据集中 如果表中有47行 那么我们就有47组训练样本 m就等于47 x来表示输入变量 往往也被称为特征量 这就是用x表示输入的特征 y来表示输出变量或者目标变量 也就是我的预测结果 那么这就是第二列 在这里使用(x, y)来表示一个训练样本 所以 在这个表格中的单独的一行对应于一个训练样本 表示某个训练样本 我将使用x上标(i)与y上标(i)来表示 ,即 这就是一个监督学习算法的工作方式 我们可以看到这里有我们的训练集里房屋价格 我们把它喂给我们的学习算法 这就是学习算法的工作了 然后输出一个函数 按照惯例 通常表示为小写h h代表hypothesis(假设) h表示一个函数 输入是房屋尺寸大小 就像你朋友想出售的房屋 因此 h 根据输入的 x 值来得出 y 值 y值对应房子的价格 因此 h是一个从x到y的函数映射 人们经常问我为什么这个函数被称作假设(hypothesis)你们中有些人可能知道hypothesis的意思 从字典或者其它什么方式可以查到 其实在机器学习中 这是一个在早期被用于机器学习的名称 它有点绕口 对这类函数来说 这可能不是一个很恰当的名字 对表示从房屋的大小到价格的函数映射 我认为这个词”hypothesis” 可能不是最好的名称 但是这是人们在机器学习中使用的标准术语 所以不用太纠结人们为什么这么叫它 当设计学习算法的时候 我们接下来需要去思考的是 怎样得到这个假设h 对于这一点在接下来的几个视频中 我将选择最初的使用规则 h代表hypothesis 我们将会这么写为了方便 有时非书面形式也可以这么写 hθ(x) 我就写成h(x) 这是缩写方式 但一般来说我会保留这个下标θ 从这个图片中 所有这一切意味着我们要预测一个关于x的 线性函数 y 对吧? 所以这就是数据集和函数的作用:用来预测 这里是y关于x的线性函数 hθ(x)=θ0+θ1*x 那么为什么是一个线性函数呢? 有时候 我们会有更复杂的函数 也许是非线性函数 但是 由于线性方程是简单的形式 我们将先从线性方程的例子入手 当然 最终我们将会建立更复杂的模型 以及更复杂的学习算法 好吧 让我们也给这模型 起一个名字 这个模型被称为线性回归(linear regression)模型 另外 这实际上是关于单个变量的线性回归 这个变量就是x 根据x来预测所有的价格函数 同时 对于这种模型有另外一个名称 称作单变量线性回归 单变量是对一个变量的一种 特别的表述方式 总而言之 这就是线性回归 在接下来的视频中 我们将开始讨论如何去实现这种模型 笔记–Model Representation模型表示 为了建立将来使用的符号，我们将使用 x^i来表示“输入”变量（在这个例子中是居住区域），也被称为输入要素，而 y^i 表示我们试图预测的“输出”或目标变量（价格）。一对 (x^i，y^i)被称为训练样例，我们将用来学习的数据集----m个训练样例的列表(x^i，y^i);i= 1，。 。 。 ，m—- 被称为训练集。请注意，符号中的上标“（i）”仅仅是训练集的索引，与幂运算无关。我们也将用X来表示输入值的空间，用Y来表示输出值的空间。在这个例子中，X = Y =ℝ。 为了更形式化地描述监督学习问题，我们的目标是在给定训练集的情况下，去学习一个函数h：X→Y，使得h（x）是y的相应值的“好”预测器。由于历史原因，这个函数h被称为假设。从形象上看，这个过程是这样的： 当我们试图预测的目标变量是连续的，比如在我们的住房例子中，我们把学习问题称为回归问题。当y只能接受少量的离散值时（比如，如果考虑到居住面积，我们想要预测一个住宅是房子还是公寓），我们称之为分类问题。 Cost Function(代价函数) 这里将定义代价函数的概念 这有助于我们 弄清楚如何把最有可能的直线与我们的数据相拟合 在线性回归中我们有一个像这样的训练集 记住M代表了训练样本的数量 所以 比如说M = 47 而我们的假设函数 也就是用来进行预测的函数 是这样的线性函数形式 接下来我们会引入一些术语 这些θ0和θ1 这些θi我把它们称为模型参数 在这个视频中 我们要做的就是谈谈如何选择这两个参数值θ0和θ1 选择不同的参数θ0和θ1 我们会得到不同的假设函数，如果θ0是1.5 θ1是0 那么假设函数会看起来是这样， 因为你的假设函数是h(x)=1.5+0*x 是这样一个常数函数 恒等于1.5 如果θ0=0并且θ1=0.5 那么假设会看起来像这样 它会通过点(2,1) 这样你又得到了h(x) 或者hθ(x) 但是有时我们为了简洁会省略θ 因此 h(x)将等于0.5倍的x 就像这样 最后 如果θ0=1并且θ1=0.5 我们最后得到的假设会看起来像这样 让我们来看看 它应该通过点(2,2) 这是我的新的h(x)或者写作hθ(x) 对吧？ 你还记得之前我们提到过·hθ(x)的 但作为简写 我们通常只把它写作h(x)· 在线性回归中 我们有一个训练集 可能就像我在这里绘制的 我们要做的就是 得出θ0 θ1这两个参数的值 来让假设函数表示的直线 尽量地与这些数据点很好的拟合 也许就像这里的这条线一样 那么我们如何得出θ0 θ1的值 来使它很好地拟合数据的呢？我们的想法是 我们要选择 能使h(x) 也就是 输入x时我们预测的值 最接近该样本对应的y值的参数θ0 θ1所以 在我们的训练集中我们会得到一定数量的样本 我们知道x表示卖出哪所房子 并且知道这所房子的实际价格 所以 我们要尽量选择参数值 使得 在训练集中 给出训练集中的x值 我们能合理准确地预测y的值 让我们给出标准的定义 在线性回归中 我们要解决的是一个最小化问题 所以我要写出关于θ0 θ1的最小化 而且 我希望这个式子极其小 是吧 我想要h(x)和y之间的差异要小 我要做的事情是尽量减少假设的输出与房子真实价格 之间的差的平方 接下来我会详细的阐述 别忘了 我用符号( x(i),y(i) )代表第i个样本 所以我想要做的是对所有训练样本进行一个求和 对i=1到i=M的样本 将对假设进行预测得到的结果 此时的输入是第i号房子的面积 对吧 将第i号对应的预测结果 减去第i号房子的实际价格 所得的差的平方相加得到总和 而我希望尽量减小这个值 也就是预测值和实际值的差的平方误差和 或者说预测价格和 实际卖出价格的差的平方 我说了这里的m指的是训练集的样本容量 对吧 这个井号是训练样本“个数”的缩写 对吧 而为了让表达式的数学意义 变得容易理解一点 我们实际上考虑的是 这个数的1/m 因此我们要尝试尽量减少我们的平均误差 也就是尽量减少其1/2m 通常是这个数的一半 前面的这些只是为了使数学更直白一点 因此对这个求和值的二分之一求最小值 应该得出相同的θ0值和相同的θ1值来 请大家一定弄清楚这个道理 没问题吧？在这里hθ(x)的这种表达 这是我们的假设 它等于θ0加上θ1与x(i)的乘积 而这个表达 表示关于θ0和θ1的最小化过程 这意味着我们要找到θ0和θ1 的值来使这个表达式的值最小 这个表达式因θ0和θ1的变化而变化对吧？ 因此 简单地说 我们正在把这个问题变成 找到能使 我的训练集中预测值和真实值的差的平方的和 的1/2M最小的θ0和θ1的值 因此 这将是我的线性回归的整体目标函数 为了使它更明确一点 我们要改写这个函数 按照惯例 我要定义一个代价函数 正如屏幕中所示 这里的这个公式 我们想要做的就是关于θ0和θ1 对函数J(θ0,θ1)求最小值 这就是我的代价函数 代价函数也被称作平方误差函数 有时也被称为 平方误差代价函数 事实上 我们之所以要求出 误差的平方和 是因为误差平方代价函数 对于大多数问题 特别是回归问题 都是一个合理的选择 还有其他的代价函数也能很好地发挥作用 但是平方误差代价函数可能是解决回归问题最常用的手段了 在后续课程中 我们还会谈论其他的代价函数 但我们刚刚讲的选择是对于大多数线性回归问题非常合理的 好吧 所以这是代价函数 到目前为止 我们已经 介绍了代价函数的数学定义 也许这个函数J(θ0,θ1)有点抽象 可能你仍然不知道它的内涵 在接下来的几个视频里 我们要更进一步解释 代价函数J的工作原理 并尝试更直观地解释它在计算什么 以及我们使用它的目的 Cost Function(代价函数)笔记 我们可以通过使用代价函数来衡量我们的假设函数的准确性。 这个假设的所有结果的平均差异（实际上是一个平均值的更漂亮的版本）与来自x的输入和实际输出y的输入。 To break it apart，结果是，其中是 $h_\theta(x_i)-y_i$ 平方的平均值，或预测值与实际值之间的差值。 该函数被称为“平方误差函数(Squared error function)”或“均方误差(Mean squared error)”。 由于平方函数的导数项将抵消该项，平均值被减半以作为计算梯度下降的便利。 以下图片总结了成本函数的作用： Cost Function - Intuition I 在上一个视频中 我们给了代价函数一个数学上的定义 在这个视频里 让我们通过一些例子来获取一些直观的感受 看看代价函数到底是在干什么 回顾一下 这是我们上次所讲过的内容 我们想找一条直线来拟合我们的数据 所以我们用 θ0 θ1 等参数 得到了这个假设 而且通过选择不同的参数 我们会得到不同的直线拟合 所以拟合出的数据就像这样 然后我们还有一个代价函数 这就是我们的优化目标 在这个视频里 为了更好地 将代价函数可视化 我将使用一个简化的假设函数 就是下面这个函数 然后我将会用这个简化的假设 也就是 θ1*x 我们可以将这个函数看成是 把 θ0 设为0 所以我只有一个参数 也就是 θ1 代价函数看起来与之前的很像 唯一的区别是现在 h(x) 等于 θ1*x 只有一个参数 θ1 所以我的 优化目标是将 J(θ1)最小化 用图形来表示就是 如果 θ0 等于零 也就意味这我们选择的假设函数 会经过原点 也就是经过坐标 (0,0) 通过利用简化的假设得到的代价函数 我们可以试着更好地理解 代价函数这个概念 我们要理解的是这两个重要的函数 第一个是假设函数 第二个是代价函数 注意这个假设函数 h(x) 对于一个固定的 θ1 ，h(x)是关于 x 的函数 所以这个假设函数就是一个关于 x 这个房子大小的函数 与此不同的是 代价函数 J 是一个关于参数 θ1 的函数 而 θ1 控制着这条直线的斜率 现在我们把这写函数都画出来 试着更好地理解它们 我们从假设函数开始 比如说这里是我的训练样本 它包含了三个点 (1,1) (2,2) 和 (3,3) 现在我们随便选择一个值 θ1 ，这里选择 θ1 等于1 ，选择 θ1=1之后， 那么我的假设函数看起来就会像是这条直线 我将要指出的是 当我想要描绘出我的假设函数时， 我的横轴被标定为X轴 X轴是表示房子大小的量 现在暂时把 θ1 定为1 我想要做的就是 算出在 θ1 等于 1 的时候 J(θ1) 等于多少 所以我们 按照这个思路来计算代价函数的大小 和之前一样 代价函数定义如下 是吧 对这个误差平方项进行求和 这就等于 这样一个形式 简化以后就等于 三个0的平方和 当然还是0 现在 在代价函数里 我们发现所有这些值都等于0 因为对于我所选定的这三个训练样本 ( 1 ,1 ) (2,2) 和 (3,3) 如果 θ1 等于 1 那么 h(x(i)) 就会正好等于 y(i) 所以 h(x) - y 所有的这些值都会等于零 这也就是为什么J(1) 等于零 所以 我们现在知道了 J(1) 是0 让我把这个画出来 我将要在屏幕右边画出我的代价函数 J 要注意的是 因为我的代价函数是关于参数 θ1 的函数 当我描绘我的代价函数时 X轴就是 θ1 现在我有 J(1) 等于零 让我们继续把函数画出来 结果我们会得到这样一个点 现在我们来看其它一些样本 θ1 可以被设定为 某个范围内各种可能的取值 所以 θ1 可以取负数 0 或者正数 所以如果 θ1 等于0.5会发生什么呢 继续把它画出来 现在要把 θ1 设为0.5 在这个条件下 我的假设函数看起来就是这样 这条线的斜率等于0.5 现在让我们计算 J(0.5) 所以这将会等于1除以2m 乘以那一块 其实我们不难发现后面的求和 就是这条线段的高度的平方 加上这条线段高度的平方 再加上这条线段高度的平方 三者求和 对吗？ 就是 y(i) 与预测值 h(x(i)) 的差 对吗 所以第一个样本将会是0.5减去1的平方 因为我的假设函数预测的值是0.5 而实际值则是1 第二个样本 我得到的是1减去2的平方 因为我的假设函数预测的值是1 但是实际房价是2 最后 加上 1.5减去3的平方 那么这就等于1除以2乘以3 因为训练样本有三个点所以 m 等于3 对吧 然后乘以括号里的内容 简化后就是3.5 所以这就等于3.5除以6 也就约等于0.68 让我们把这个点画出来 不好意思 有一个计算错误 这实际上该是0.58 所以我们把点画出来 大约会是在这里 对吗 现在 让我们再多做一个点 让我们试试θ1等于0 J(0) 会等于多少呢 如果θ1等于0 那么 h(x) 就会等于一条水平的线 对了 就会像这样是水平的 所以 测出这些误差 我们将会得到 J(0) 等于 1除以 2m 乘以1的平方 加上2的平方 加上3的平方 也就是 1除以6乘以14 也就是2.3左右 所以让我们接着把这个点也画出来 所以这个点最后是2.3 当然我们可以接着设定 θ1 等于别的值 进行计算 你也可以把 θ1 设定成一个负数 所以如果 θ1 是负数 那么 h(x) 将会等于 打个比方说 －0.5 乘以x 然后 θ1 就是 -0.5 那么这将会 对应着一个斜率为-0.5的假设函数 而且你可以 继续计算这些误差 结果你会发现 对于0.5 结果会是非常大的误差 最后会得到一个较大的数值 类似于5.25 等等 对于不同的 θ1 你可以计算出这些对应的值 对吗 结果你会发现 你算出来的这些值 你得到一条这样的曲线 通过计算这些值 你可以慢慢地得到这条线 这就是 J(θ) 的样子了 我们来回顾一下 任何一个 θ1 的取值对应着一个不同的 假设函数 或者说对应着左边一条不同的拟合直线。 对于任意的θ1 你可以算出一个不同的 J(θ1) 的取值 举个例子 你知道的 θ1 等于1时对应着穿过这些数据的这条直线 当 θ1 等于0.5 也就是这个玫红色的点 也许对应着这条线 然后 θ1 等于0 也就是蓝色的这个点 对应着 这条水平的线 对吧 所以对于任意一个 θ1 的取值 我们会得到 一个不同的 J(θ1) 而且我们可以利用这些来描出右边的这条曲线 现在你还记得 学习算法的优化目标 是我们想找到一个 θ1 的值 来将 J(θ1) 最小化 对吗 这是我们线性回归的目标函数 嗯 看这条曲线 让 J(θ1) 最小化的值 是 θ1 等于1 然后你看 这个确实就对应着最佳的通过了数据点的拟合直线 这条直线就是由 θ1=1 的设定而得到的 然后 对于这个特定的训练样本 我们最后能够完美地拟合 这就是为什么最小化 J(θ1) 对应着寻找一个最佳拟合直线的目标 总结一下 在这个视频里 我们看到了一些图形 来理解代价函数 要做到这个 我们简化了算法 让这个函数只有一个参数 θ1 也就是说我们把 θ0 设定为0 在下一个视频里 我们将回到原来的问题的公式 然后看一些 带有 θ0 和 θ1 的图形 也就是说不把 θ0 设置为0了 希望这会让你更好地理解在原来的线性回归公式里 代价函数 J 的意义 Cost Function - Intuition I笔记 如果我们试图用视觉术语来思考它，我们的训练数据集就散布在x-y平面上。 我们试图做一条直线（由$h_\theta(x)$定义）来穿过这些散布的数据点。 我们的目标是获得最佳线路。 尽可能最好的线是这样的，以便线上散射点的平均垂直距离将是最小的。 理想情况下，该线应该通过我们训练数据集的所有点。 在这种情况下，$J(\theta_0,\theta_1)$值将为0.以下示例显示了代价函数为0的理想情况。 当$\theta_1=1$我们得到1的斜率时，它会经历我们模型中的每个单一数据点。 相反，当$\theta_1=0.5$我们看到从适合度到数据点的垂直距离增加时。 这使我们的代价函数增加到0.58。 绘制几个其他点产生到以下图表： 因此，作为一个目标，我们应该尽量减少代价函数。 在这种情况下，$\theta_1=1$是我们整体最低的。 Cost Function - Intuition II 这节课中 我们将更深入地学习代价函数的作用 这段视频的内容假设你已经认识轮廓图 如果你对轮廓图不太熟悉的话 这段视频中的某些内容你可能会听不懂 但不要紧 如果你跳过这段视频的话 也没什么关系 不听这节课对后续课程理解影响不大 和之前一样 这是我们的几个重要公式 包括了假设h、参数θ、代价函数J 以及优化目标 跟前一节视频不同的是 我还是把θ写成$\theta_0$,$\theta_1$的形式 便于这里我们要对代价函数进行的可视化 和上次一样 首先来理解假设h和代价函数J 这是房价数据组成的训练集数据 让我们来构建某种假设 就像这条线一样 很显然这不是一个很好的假设 但不管怎样 如果我假设θ0等于50 θ1等于0.06的话 那么我将得到这样一个假设函数 对应于这条直线 给出θ0和θ1的值 我们要在右边画出代价函数的图像 上一次 我们是只有一个θ1 也就是说 画出的代价函数是关于θ1的函数 但现在我们有两个参数 θ0和θ1 因此图像就会复杂一些了 当只有一个参数θ1的时候 我们画出来是这样一个弓形函数 而现在我们有了两个参数 那么代价函数 仍然呈现类似的某种弓形 实际上这取决于训练样本 你可能会得到这样的图形 因此这是一个三维曲面图 两个轴分别表示θ0和θ1 随着你改变θ0和θ1的大小 你便会得到不同的代价函数 J(θ0,θ1) 对于某个特定的点 (θ0,θ1) 这个曲面的高度 也就是竖直方向的高度 就表示代价函数 J(θ0,θ1) 的值 不难发现这是一个弓形曲面 我们来看看三维图 这是这个曲面的三维图 水平轴是θ0、θ1 竖直方向表示 J(θ0,θ1) 旋转一下这个图 你就更能理解这个弓形曲面所表示的代价函数了 在这段视频的后半部分 为了描述方便 我将不再像这样给你用三维曲面图的方式解释代价函数J 而还是用轮廓图来表示 .contour plot 或 contour figure 意思一样 下图就是一个轮廓图 两个轴分别表示 θ0 和 θ1 而这些一圈一圈的椭圆形 每一个圈就表示 J(θ0,θ1) 相同的所有点的集合 具体举例来说 我们选三个点出来 这三个桃红色的点 都表示相同的 J(θ0,θ1) 的值 对吧 横纵坐标分别是θ0 θ1 这三个点的 J(θ0,θ1) 值是相同的 如果你之前没怎么接触轮廓图的话 你就这么想 你就想象一个弓形的函数从屏幕里冒出来 因此最小值 也就是这个弓形的最低点就是这个点 对吧 也就是这一系列同心椭圆的中心点 想象一下这个弓形从屏幕里冒出来 所以这些椭圆形 都从我的屏幕上冒出相同的高度 弓形的最小值点是这个位置 因此轮廓图是一种很方便的方法 能够直观地观察 代价函数J 接下来让我们看几个例子 在这里有一点 这个点表示θ0等于800 θ1大概等于-0.15 那么这个红色的点 代表了某个 (θ0,θ1) 组成的数值组 而这个点也对应于左边这样一条线 对吧 θ0等于800 也就是跟纵轴相交于大约800 斜率大概是-0.15 当然 这条线并不能很好地拟合数据 对吧 以这组 θ0 θ1 为参数的这个假设 h(x) 并不是数据的较好拟合 并且你也发现了 这个代价值 就是这里的这个值 距离最小值点还很远 也就是说这个代价值还是算比较大的 因此不能很好拟合数据 让我们再来看几个例子 这是另一个假设 你不难发现 这依然不是一个好的拟合 但比刚才稍微好一点 这是我的 θ0 θ1 点 这是 θ0 的值 大约为360 θ1 的值为0 我们把它写下来 θ0=360 θ1=0 因此这组θ值对应的假设是 这条水平的直线 也就是h(x) = 360 + 0 × x 这就是假设 这个假设同样也有某个代价值 而这个代价值就对应于这个代价函数在这一点的高度 最后一个例子 这个点其实不是最小值 但已经非常靠近最小值点了 这个点对数据的拟合就很不错 它对应这样两个θ0 和 θ1 的值 同时也对应这样一个 h(x) 这个点虽然不在最小值点 但非常接近了 因此误差平方和 或者说 训练样本和假设的距离的平方和 这个距离值的平方和 非常接近于最小值 尽管它还不是最小值 好的 通过这些图形 我希望你能更好地 理解这些代价函数 J 所表达的值 它们是什么样的 它们对应的假设是什么样的 以及什么样的假设对应的点 更接近于代价函数J的最小值 当然 我们真正需要的是一种有效的算法 能够自动地找出这些使代价函数J取最小值的参数θ0和θ1来 对吧 我想我们也不希望编个程序 把这些点画出来 然后人工的方法来读出这些点的数值 这很明显不是一个好办法 事实上 我们后面就会学到 我们会遇到更复杂、更高维度、更多参数的情况 这在我们在后面的视频中很快就会遇到 而这些情况是很难画出图的 因此更无法将其可视化 因此我们真正需要的 是编写程序来找出这些最小化代价函数的θ0和θ1的值 在下一节视频中 我们将介绍一种算法 能够自动地找出能使代价函数 J 最小化的参数θ0和θ1的值 Cost Function(代价函数) - Intuition II笔记 轮廓图（或者叫等值线图）是包含许多等高线的图。两个变量函数的轮廓线在同一行的所有点上具有恒定值。这种图的一个例子就是下面的图。 采取任何颜色并沿着“圆”走，人们会期望获得相同的成本函数值。例如，上面绿线上的三个绿色点对于J（θ0，θ1）具有相同的值，因此它们沿着同一条线发现。带圆圈的x显示θ0= 800和θ1= -0.15时左侧图形的成本函数值。再取一个h（x）并绘制其轮廓图（等值线图），可以得到以下图表： 当θ0= 360且θ1= 0时，等值线图中J（θ0，θ1）的值更靠近中心，从而降低了代价函数误差。现在给我们的假设函数一个稍微正向的斜率会导致更好的数据拟合。 上面的图尽可能地降低了成本函数，因此，θ1和θ0的结果分别趋于0.12和250左右。将我们图上的这些值绘制在右侧似乎将我们的观点置于最内层“圈子”的中心。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习入门</category>
      </categories>
      <tags>
        <tag>Cost Function</tag>
        <tag>代价函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[零:机器学习介绍]]></title>
    <url>%2F2018%2F11%2F15%2F%E9%9B%B6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[零：机器学习介绍 视频地址 定义定义1给予计算机学习能力的领域。 Samuel的定义可以回溯到50年代，他编写了一个西洋棋程序。 这程序神奇之处在于，编程者自己并不是个下棋高手。 但因为他太菜了，于是就通过编程， 让西洋棋程序自己跟自己下了上万盘棋。通过观察 哪种布局（棋盘位置）会赢，哪种布局会输， 久而久之，这西洋棋程序明白了什么是好的布局， 什么样是坏的布局。然后就牛逼大发了，程序通过学习后， 玩西洋棋的水平超过了Samuel。这绝对是令人注目的成果。 尽管编写者自己是个菜鸟，但因为 计算机有着足够的耐心，去下上万盘的棋， 没有人有这耐心去下这么多盘棋。通过这些练习， 计算机获得无比丰富的经验，于是渐渐成为了 比Samuel更厉害的西洋棋手。定义2: 由Tom Mitchell提出，来自卡内基梅隆大学.一个好的学习一个程序被认为能从经验E中学习，解决任务 T，达到 性能度量值P，当且仅当，有了经验E后，经过P评判， 程序在处理 T 时的性能有所提升 在西洋棋那例子中， 经验E :程序上万次的自我练习的经验 任务T :就是下棋。 性能度量值p:，就是它在与一些新的对手比赛时，赢得比赛的概率。 目主要的两种类型学习算法被我们称之为监督学习和无监督学习。 监督学习:我们将教计算机如何去完成任务 无监督学习:让它自己进行学习。监督学习(Supervised Learning)例子1: 假设你想预测房价， 之前，某学生已经从某地收集了数据集其中一个数据集是这样的。 这是横坐标，即不同房子的面积，单位平方脚（^-^） 纵轴上是房价，单位 千美元。 根据给定数据，假设你朋友有栋房子，750平尺（70平米） 想知道这房子能卖多少，好卖掉。 那么，学习算法怎么帮你呢？学习算法可以： 绘出一条直线，让直线尽可能匹配到所有数据。 基于此，看上去，那个房子应该、可能、也许、大概 卖到15万美元（一平米两千刀！）。 但这不是唯一的学习算法。 可能还有更好的。比如不用直线了， 可能平方函数会更好， 即二次多项式更符合数据集。如果你这样做， 预测结果就应该是20万刀（一平三千刀，涨价好快）。 后面我们会介绍到如何选择 是选择直线还是平方函数来拟合。 没有明确的选择，就不知哪个能给你的朋友 更好的卖房建议。只是这些每个都是很好的学习算法例子。 也是监督学习的例子。 术语监督学习，意指给出一个算法， 需要部分数据集已经有正确答案。比如给定房价数据集， 对于里面每个数据，算法都知道对应的正确房价， 即这房子实际卖出的价格。算法的结果就是 算出更多的正确价格，比如那个新房子， 你朋友想卖的那个。用更术语的方式来定义， 监督学习又叫回归问题(regression problem)，（应该是回归属于监督中的一种） 意指要预测一个连续值的输出，比如房价。 虽然从技术上，一般把房价记到美分单位。 所以实际还是个离散值，但通常把它看作实际数字， 是一个标量值，一个连续值的数，而术语回归， 意味着要预测这类连续值属性的种类。 例子2: 另一个监督学习的例子，我和一些朋友 之前研究的领域。让我们来看医学记录， 并预测胸部肿瘤是恶性良性。 如果某人发现有胸部肿瘤，恶性肿瘤有害又危险， 良性肿瘤则是少害。 显然人们很关注这个。让我们看一个收集好的数据集， 假设在数据集中，横轴表示肿瘤的大小， 纵轴我打算圈上0或1，是或否， 即肿瘤是恶性的还是良性的。 所以如图所示，可以看到这个大小的肿瘤块 是良性的，还有这些大小的都是良性的。 不幸地是也看到一些恶性肿瘤，比如这些大小的肿瘤。 所以，有5个良性块，在这一块， 还有5个恶性的，它们纵轴值为1. 现在假设某人杯具地得胸部肿瘤了， 大小大概是这么大。 对应的机器学习问题就是，你能否估算出一个概率， 即肿瘤为恶或为良的概率？ 专业地说，这是个分类问题(classification problem)。 分类是要预测一个离散值输出。 这里是0或1，恶性或良性。 事实证明， 在分类问题中，有时会有超过两个的值， 输出的值可能超过两种。举个具体例子， 胸部肿瘤可能有三种类型，所以要预测离散值0，1，2，3 ，假设总共有三种癌症。 0就是良性肿瘤，没有癌症。 1 表示1号癌症 2 是2号癌症， 3 就是3号癌症 这同样是个分类问题，因为它的输出的离散值集合 分别对应于无癌，1号，2号，3号癌症 我再露一小手，在分类问题中，还有另一种作图方式 来描述数据—我画你猜。要用到些许不同的符号集合来描绘数据。如果肿瘤大小作为唯一属性， 被用于预测恶性良性，可以把数据作图成这样。 使用不同的符号来表示良性和 恶性，即阴性和阳性。所以，不再统一画叉叉了， 改用圈圈来代表良性肿瘤，就像这样。 仍沿用X（叉叉）代表恶性肿瘤。希望你能明白。 我所做的就是，把在上面的数据， 映射下来。再用不同的符号， 圈和叉来分别代表良性和恶性。 在上例中，只使用了一个特征属性，即肿瘤块大小， 来预测肿瘤是恶性良性。在其它机器学习问题里， 有着不只一个的特征和属性。 例子，现在不只是知道肿瘤大小， 病人年龄和肿瘤大小都知道了。这种情况下， 数据集如表图所示，有些病人，年龄、肿瘤已知， 不同的病人，会有一点不一样， 肿瘤恶性，则用叉来代表。所以，假设 有一朋友得了肿瘤。肿瘤大小和年龄 落在此处。那么依据这个给定的数据集，学习算法 所做的就是画一条直线，分开 恶性肿瘤和良性肿瘤，所以学习算法会 画条直线，像这样，把两类肿瘤分开。 然后你就能判断你朋友的肿瘤是…了 如果它在那边，学习算法就说 你朋友的肿瘤在良性一边，因此更可能 是良性的。 好，本例中，总共有两个特征， 即病人年龄和肿瘤大小。在别的ML问题中， 经常会用到更多特征，我朋友研究这个问题时， 通常使用这些特征：比如块的厚度，即胸部肿瘤的厚度 肿瘤细胞大小和形状的一致性， 等等。它表明， 最有趣的学习算法（本课中将学到） 能够处理，无穷多个特征。不是3到5个这么少。 在这张幻灯片中，我已经列举了总共5个不同的特征。 但对于一些学习问题， 真要用到的不只是三五个特征， 要用到无数多个特征，非常多的属性， 所以，你的学习算法要使用很多的属性 或特征、线索来进行预测。 那么，你如何处理无限多特征呢？甚至你如何存储无数的东西 进电脑里，又要避免内存不足？ 事实上，等我们介绍一种叫支持向量机的算法时， 就知道存在一个简洁的数学方法，能让电脑处理无限多的特征。 想像下，我不是这边写两个特征， 右边写三个特征。而是，写一个无限长的特征表， 不停地写特征，似乎是个无限长的特征的表。 但是，我们也有能力设计一个算法来处理这个问题。 本课中，我们介绍监督学习。 其基本思想是，监督学习中，对于数据集中的每个数据， 都有相应的正确答案（训练集） ,算法就是基于这些来做出预测.就像那个房价， 或肿瘤的性质。后面介绍了回归问题。 即通过回归来预测一个连续值输出。 我们还谈到了分类问题， 目标是预测离散值输出。 下面是个小测验题目： 假设你有家公司，希望研究相应的学习算法去 解决两个问题。第一个问题，你有一堆货物的清单。 假设一些货物有几千件可卖， 你想预测出，你能在未来三个月卖出多少货物。 第二个问题，你有很多用户， 你打算写程序来检查每个用户的帐目。 对每个用户的帐目， 判断这个帐目是否被黑过（hacked or compromised）。 请问，这两个问题是分类问题，还是回归问题？ 问题一是个回归问题 因为如果我有几千件货物， 可能只好把它当作一个实际的值，一个连续的值。 也把卖出的数量当作连续值。 第二个问题，则是分类问题，因为可以把 我想预测的一个值设为0，来表示账目没有被hacked 另一个设为1，表示已经被hacked。 就像乳癌例子中，0表示良性，1表示恶性。 所以这个值为0或1，取决于是否被hacked， 有算法能预测出是这两个离散值中的哪个。 因为只有少量的离散值，所以这个就是个分类问题。 笔记 在监督式学习中，我们给了一个数据集，并且已经知道我们的正确输出应该是什么样子，并且有输入和输出之间有关系的想法。 监督学习问题分为“回归”和“分类”问题。 在回归问题中，我们试图预测连续输出中的结果，这意味着我们试图将输入变量映射到某个连续函数。 在分类问题中，我们试图预测离散输出的结果。换句话说，我们试图将输入变量映射到离散类别 例子 回归 - 给定一个人的照片，我们必须根据给定的图片来预测他们的年龄 分类 - 给予患有肿瘤的患者，我们必须预测肿瘤是恶性的还是良性的。 无监督学习(Unsupervised Learning) 在上一节视频中 我们已经讲过了监督学习 回想起上次的数据集 每个样本 都已经被标明为 正样本或者负样本 即良性或恶性肿瘤 因此 对于监督学习中的每一个样本 我们已经被清楚地告知了 什么是所谓的正确答案 即它们是良性还是恶性 在无监督学习中 我们用的数据会和监督学习里的看起来有些不一样 在无监督学习中 没有属性或标签这一概念 也就是说所有的数据都是一样的,没有区别 所以在无监督学习中 我们只有一个数据集 没人告诉我们该怎么做 我们也不知道每个数据点究竟是什么意思,相反 它只告诉我们:现在有一个数据集 你能在其中找到某种结构吗? 对于给定的数据集,无监督学习算法可能判定,该数据集包含两个不同的聚类,这是第一个聚类,然后这是另一个聚类 .无监督学习算法 会把这些数据分成两个不同的聚类,所以这就是所谓的聚类算法 例子1 我们来举一个聚类算法的例子 Google 新闻的例子 如果你还没见过这个页面的话 你可以到这个URL:news.google.com 去看看谷歌新闻每天都在干什么呢？ 他们每天会去收集成千上万的网络上的新闻然后将他们分组组成一个个新闻专题 比如 让我们来看看这里,这里的URL链接连接着不同的有关BP油井事故的报道,所以 让我们点击 这些URL中的一个 恩 让我们点一个 然后我们会来到这样一个网页 这是一篇来自华尔街日报的 有关……你懂的 有关BP油井泄漏事故的报道 标题为《BP杀死了Macondo》 Macondo 是个地名 就是那个漏油事故的地方 如果你从这个组里点击一个不同的URL 那么你可能会得到不同的新闻 这里是一则CNN的新闻 是一个有关BP石油泄漏的视频 如果你再点击第三个链接 又会出现不同的新闻 这边是英国卫报的报道 也是关于BP石油泄漏 所以 谷歌新闻所做的就是 去搜索成千上万条新闻 然后自动的将他们聚合在一起 因此 有关同一主题的 新闻被显示在一起 实际上 聚类算法和无监督学习算法 也可以被用于许多其他的问题 例子2 这里我们举个它在基因组学中的应用 下面是一个关于基因芯片的例子 基本的思想是 给定一组不同的个体,对于每个个体,检测它们是否拥有某个特定的基因 也就是说，你要去分析有多少基因显现出来了 因此 这些颜色 红 绿 灰 等等 它们 展示了这些不同的个体 是否拥有一个特定基因 的不同程度 然后你能做的就是 运行一个聚类算法 把不同的个体归入不同的类 或归为不同类型的人 这就是无监督学习 我们没有提前告知这个算法 这些是第一类的人 这些是第二类的人 这些是第三类的人等等 相反我们只是告诉算法 你看 这儿有一堆数据 我不知道这个数据是什么东东 我不知道里面都有些什么类型 叫什么名字 我甚至不知道都有哪些类型 但是 请问你可以自动的找到这些数据中的类型吗？ 然后自动的 按得到的类型把这些个体分类 虽然事先我并不知道哪些类型 因为对于这些数据样本来说 我们没有给算法一个正确答案,所以这就是无监督学习 其他例子 无监督学习或聚类算法在其他领域也有着大量的应用 它被用来组织大型的计算机集群 我有一些朋友在管理 大型数据中心 也就是 大型计算机集群 并试图 找出哪些机器趋向于 协同工作 如果你把这些机器放在一起 你就可以让你的数据中心更高效地工作 用于社交网络的分析 所以 如果可以得知 哪些朋友你用email联系的最多 或者知道你的Facebook好友 或者你Google+里的朋友 知道了这些之后 我们是否可以自动识别 哪些是很要好的朋友组 哪些仅仅是互相认识的朋友组 还有在市场分割中的应用 许多公司拥有庞大的客户信息数据库,那么,给你一个客户数据集,你能否自动找出不同的市场分割,并自动将你的客户分到不同的细分市场中.从而有助于我在不同的细分市场中进行更有效的销售 这也是无监督学习 我们现在有 这些客户数据 但我们预先并不知道 有哪些细分市场 而且 对于我们数据集的某个客户 我们也不能预先知道 谁属于细分市场一 谁又属于细分市场二等等 但我们必须让这个算法自己去从数据中发现这一切 天文数据分析 通过这些聚类算法 我们发现了许多 惊人的、有趣的 以及实用的 关于星系是如何诞生的理论 所有这些都是聚类算法的例子 而聚类只是无监督学习的一种 鸡尾酒宴问题 恩 我想你参加过鸡尾酒会的 是吧？ 嗯 想象一下 有一个宴会 有一屋子的人 大家都坐在一起 而且在同时说话 有许多声音混杂在一起 因为每个人都是在同一时间说话的 在这种情况下你很难听清楚你面前的人说的话 因此 比如有这样一个场景 宴会上只有两个人 两个人 同时说话 恩 这是个很小的鸡尾酒宴会 我们准备好了两个麦克风 把它们放在房间里 然后 因为这两个麦克风距离这两个人 的距离是不同的 每个麦克风都记录下了 来自两个人的声音的不同组合 也许A的声音 在第一个麦克风里的声音会响一点 也许B的声音 在第二个麦克风里会比较响一些 因为2个麦克风 的位置相对于 2个说话者的位置是不同的 但每个麦克风都会录到 来自两个说话者的重叠部分的声音 一个研究员录下的两个说话者的声音,无监督学习算法可以将两种混合在一起的音频分开 所以,你可以看到,像这样的无监督学习算法,看起来 为了 构建这个应用程序 做这个音频处理 似乎需要写好多代码啊 或者需要链接到 一堆处理音频的Java库 貌似需要一个非常复杂的程序,分离出音频等 实际上,要实现混合音频分离的效果,只需要一行代码就可以了 当然 研究人员 花了很长时间才想出这行代码的,我不是说这是一个简单的问题 但事实上 如果你 使用正确的编程环境 许多学习 算法是用很短的代码写出来的 所以这也是为什么在 这门课中我们要 使用Octave的编程环境 Octave是一个免费的 开放源码的软件 使用Octave或Matlab这类的工具 许多学习算法 都可以用几行代码就可以实现 在后续课程中 我会教你如何使用Octave 你会学到 如何在Octave中实现这些算法 或者 如果你有Matlab 你可以用它 事实上 在硅谷 很多的机器学习算法 我们都是先用Octave 写一个程序原型 因为在Octave中实现这些 学习算法的速度快得让你无法想象 在这里 每一个函数 例如 SVD 意思是奇异值分解 但这其实是解线性方程 的一个惯例 它被内置在Octave软件中了 如果你试图 在C + +或Java中做这个 将需要写N多代码 并且还要连接复杂的C + +或Java库 所以 你可以在C++或 Java或Python中 实现这个算法 只是会 更加复杂而已 如果你使用Octave的话 会学的更快 并且如果你用 Octave作为你的学习工具 和开发原型的工具 你的学习和开发过程 会变得更快 而事实上在硅谷 很多人会这样做 他们会先用Octave 来实现这样一个学习算法原型 只有在确定 这个算法可以工作后 才开始迁移到 C++ Java或其它编译环境 事实证明 这样做 实现的算法 比你一开始就用C++ 实现的算法要快多了 我们谈到了无监督学习 它是一种学习机制 你给算法大量的数据 要求它找出数据中 蕴含的类型结构 以下的四个例子中 哪一个 您认为是 无监督学习算法 而不是监督学习问题 * 恩 没忘记垃圾邮件文件夹问题吧？ 如果你已经标记过数据 那么就有垃圾邮件和 非垃圾邮件的区别 我们会将此视为一个监督学习问题 * 新闻故事的例子 正是我们在本课中讲到的 谷歌新闻的例子 我们介绍了你可以如何使用 聚类算法这些文章聚合在一起 所以这是无监督学习问题 * 市场细分的例子 我之前有说过 这也是一个无监督学习问题 因为我是要 拿到数据 然后要求 它自动发现细分市场 * 最后一个例子 糖尿病 这实际上就像我们 上节课讲到的乳腺癌的例子 只不过这里不是 好的或坏的癌细胞 良性或恶性肿瘤我们 现在是有糖尿病或 没有糖尿病 所以这是 有监督的学习问题 像处理那个乳腺癌的问题一样 我们会把它作为一个 有监督的学习问题来处理 笔记 无监督的学习使我们能够很少或根本不知道我们的结果应该是什么样子。 我们可以从数据中推导出结构，我们不一定知道变量的影响。 我们可以通过基于数据中变量之间的关系对数据进行聚类来推导出这种结构。 在无监督学习的基础上，没有基于预测结果的反馈。 例： 聚类(Clustering)：搜集一百万个不同的基因，并找到一种方法，将这些基因自动分组，这些基因组通过不同的变量（例如寿命，位置，角色等）相似或相关。 非聚类(Non-clustering)：“鸡尾酒会算法”，可以让你在混乱的环境中找到结构。 （即在鸡尾酒会上从声音网格中识别个别的声音和音乐）。]]></content>
      <categories>
        <category>机器学习</category>
        <category>机器学习入门</category>
      </categories>
      <tags>
        <tag>机器学习简介</tag>
      </tags>
  </entry>
</search>
