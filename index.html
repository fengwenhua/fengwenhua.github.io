<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">






  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-center-atom.min.css?v=1.0.2" rel="stylesheet">




  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



















  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.5.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.5.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.5.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.5.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.5.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '6.5.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="记录日常学习与生活">
<meta property="og:type" content="website">
<meta property="og:title" content="江南小虫虫的博客">
<meta property="og:url" content="http://www.fengwenhua.top/index.html">
<meta property="og:site_name" content="江南小虫虫的博客">
<meta property="og:description" content="记录日常学习与生活">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="江南小虫虫的博客">
<meta name="twitter:description" content="记录日常学习与生活">






  <link rel="canonical" href="http://www.fengwenhua.top/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>江南小虫虫的博客</title>
  











  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">江南小虫虫的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br />关于</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br />分类</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />搜索</a>
        </li>
      
    </ul>
  

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.fengwenhua.top/2018/11/15/第四章-多变量线性回归(Linear-Regression-with-Multiple-Variables)/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="冯文华">
      <meta itemprop="description" content="记录日常学习与生活">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江南小虫虫的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/15/第四章-多变量线性回归(Linear-Regression-with-Multiple-Variables)/" class="post-title-link" itemprop="http://www.fengwenhua.top/index.html">第四章 多变量线性回归(Linear Regression with Multiple Variables)</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-14 23:22:59 / 修改时间：16:22:43" itemprop="dateCreated datePublished" datetime="2018-11-14T23:22:59Z">2018-11-14</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/机器学习入门/" itemprop="url" rel="index"><span itemprop="name">机器学习入门</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/11/15/第四章-多变量线性回归(Linear-Regression-with-Multiple-Variables)/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/11/15/第四章-多变量线性回归(Linear-Regression-with-Multiple-Variables)/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/11/15/第四章-多变量线性回归(Linear-Regression-with-Multiple-Variables)/" class="leancloud_visitors" data-flag-title="第四章 多变量线性回归(Linear Regression with Multiple Variables)">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数：</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">19k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">32 分钟</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h1 id="第四章-多变量线性回归-Linear-Regression-with-Multiple-Variables"><a href="#第四章-多变量线性回归-Linear-Regression-with-Multiple-Variables" class="headerlink" title="第四章 多变量线性回归(Linear Regression with Multiple Variables)"></a>第四章 多变量线性回归(Linear Regression with Multiple Variables)</h1><h2 id="Multiple-Features（多维特征）"><a href="#Multiple-Features（多维特征）" class="headerlink" title="Multiple Features（多维特征）"></a>Multiple Features（多维特征）</h2><blockquote>
<p> 在这段视频中 我们将开始 介绍一种新的 <strong>更为有效的线性回归形式 这种形式适用于多个变量或者多特征量的情况</strong></p>
</blockquote>
<ul>
<li>在之前我们学习过的 线性回归中 我们只有一个单一特征量 房屋面积 x 我们希望用这个特征量 来预测 房子的价格 这就是我们的假设</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526603422_2029872331_1540481948" alt="_1526603422_2029872331_1540481948_1526603422_2029872331.png"></p>
<ul>
<li>但是想象一下 如果我们<strong>不仅有房屋面积 作为预测房屋 价格的特征量 或者变量 我们还知道 卧室的数量 楼层的数量以及房子的使用年限</strong> 这样就给了我们 更多可以用来 预测房屋价格的信息</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526603455_433733145_1540741332" alt="_1526603455_433733145_1540741332_1526603455_433733145.png"></p>
<ul>
<li>先简单介绍一下记法 我们开始的时候就提到过 我要用 x 下标1 x 下标2 等等 来表示 这种情况下的四个特征量 然后仍然用 Y来表示我们 所想要预测的输出变量</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526603471_617319148_1540741358" alt="_1526603471_617319148_1540741358_1526603471_617319148.png"></p>
<ul>
<li>让我们来看看更多的表示方式 现在我们有四个特征量 我要用<code>小写n</code> 来表示<strong>特征量的数目</strong> 因此在这个例子中 我们的n等于4 因为你们看 我们有 1 2 3 4 共4个特征量 这里的n和我们之前 使用的n不同 之前我们是用的“<code>m</code>”来<strong>表示样本的数量</strong> 所以如果你有47行 那么m就是这个表格里面的行数 或者说是训练样本数</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526603493_859265827_1540741432" alt="_1526603493_859265827_1540741432_1526603493_859265827.png"></p>
<ul>
<li>然后我要用<code>x 上标 (i)</code> 来表示<strong>第i个 训练样本的 输入特征值</strong> 举个具体的例子来说 <code>x上标 (2)</code> 就是表示<strong>第二个 训练样本的特征向量</strong> 因此这里 x(2)就是向量 [1416, 3, 2, 40] 因为这四个数字对应了 我用来预测房屋价格的 第二个房子的 四个特征量 因此在这种记法中 这个上标2 就是训练集的一个索引 而不是x的2次方 这个2就对应着 你所看到的表格中的第二行 即我的第二个训练样本</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104203747.png" alt=""></p>
<ul>
<li>x上标(2) 这样表示 就是一个四维向量 事实上更普遍地来说 这是n维的向量 用这种表示方法 x上标2就是一个向量 因此 我用<code>x上标(i) 下标j</code> 来表示 <strong>第i个训练样本的 第j个特征量</strong> 因此具体的来说 <code>x上标(2)下标3</code>代表着 <strong>第2个训练样本里的第3个特征量</strong> 对吧？ 这个是3 我写的不太好看 所以说x上标(2)下标3就等于2</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204137.png" alt=""></p>
<ul>
<li>既然我们有了多个特征量 让我们继续讨论一下 我们的假设形式应该是怎样的 这是我们之前使用的假设形式 x就是我们唯一的特征量 但现在我们有了多个特征量 我们就不能再 使用这种简单的表示方式了 取而代之的 我们将把线性回归的假设 改成这样 θ0加上 θ1 乘以 x1 加上 θ2乘以x2 加上 θ3 乘以x3 加上θ4乘以x4 然后如果我们有n个特征量 那么我们要将所有的n个特征量相加 而不是四个特征量 我们需要对n个特征量进行相加</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204157.png" alt=""></p>
<ul>
<li>举个具体的例子 在我们的设置的参数中 我们可能有h(x)等于 <code>80 + 0.1 x1 + 0.01x2 + 3x3 - 2x4</code> 这就是一个 假设的范例 别忘了 假设是为了预测 大约以千刀为单位的房屋价格 就是说 一个房子的价格 可以是 80 k加上 0.1乘以x1 也就是说 每平方尺100美元 然后价格 会随着楼层数的增加 再继续增长 x2是楼层数 接着价格会继续增加 随着卧室数的增加 因为x3是 卧室的数量 但是呢 房子的价格会 随着使用年数的增加 而贬值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204236.png" alt=""></p>
<ul>
<li>这是重新改写过的假设的形式 接下来 我要来介绍一点 简化这个等式的表示方式</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204258.png" alt=""></p>
<ul>
<li>为了表示方便 我要将<code>x下标0的值设为1</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204355.png" alt=""></p>
<ul>
<li>具体而言 这意味着 对于第i个样本 都有一个向量x上标(i) 并且x上标(i) 下标0等于1 你可以认为我们 定义了一个额外的第0个特征量 因此 我过去有n个特征量 因为我们有x1 x2 直到xn 由于我另外定义了 额外的第0个特征向量 并且它的取值 总是1 所以我现在的特征向量x 是一个从0开始标记的 n+1维的向量</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204411.png" alt=""></p>
<ul>
<li>所以现在就是一个 n+1维的特征量向量 但我要从0开始标记 同时 我也想把我的参数 都看做一个向量 所以我们的参数就是 我们的<code>θ0 θ1 θ2 等等</code> 直到θn 我们要把 所有的参数都写成一个向量 <code>θ0 θ1...</code>一直到 直到θn 这里也有一个从0开始标记的矢量 下标从0开始 这是另外一个</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204425.png" alt=""></p>
<ul>
<li>所以我的假设 现在可以写成θ0乘以x0 加上θ1乘以x1直到 θn 乘以xn 这个等式 和上面的等式是一样的 因为你看 x0等于1</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204451.png" alt=""></p>
<ul>
<li>下面 我要 把这种形式<code>假设等式</code> 写成 <code>θ转置乘以X</code> 取决于你对 向量内积有多熟悉 如果你展开 θ转置乘以X 那么就得到 θ0 θ1直到θn 这个就是θ转置 实际上 这就是一个 n+1乘以1维的矩阵 也被称为行向量 用行向量 与X向量相乘 X向量是 x0 x1等等 直到xn 因此内积就是 θ转置乘以X 就等于这个等式</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204531.png" alt=""></p>
<ul>
<li>这就为我们提供了一个 表示假设的 更加便利的形式 即<strong>用参数向量θ以及 特征向量X的内积</strong> 这就是改写以后的 表示方法 这样的表示习惯 就让我们 可以以这种紧凑的形式写出假设 这就是多特征量情况下的假设形式 起另一个名字 就是 所谓的<code>多元线性回归</code></li>
<li><code>多元</code>一词 也就是<strong>用来预测的多个特征量 或者变量 就是一种更加好听的说法</strong>罢了</li>
</ul>
<h2 id="Gradient-Descent-for-Multiple-Variables-多变量梯度下降"><a href="#Gradient-Descent-for-Multiple-Variables-多变量梯度下降" class="headerlink" title="Gradient Descent for Multiple Variables(多变量梯度下降)"></a>Gradient Descent for Multiple Variables(多变量梯度下降)</h2><blockquote>
<p>在之前的视频中 我们谈到了一种线性回归的假设形式 这是一种有多特征或者是多变量的形式 在本节视频中 我们将会谈到如何找到满足这一假设的<strong>参数</strong> 尤其是<strong>如何使用梯度下降法 来解决多特征的线性回归问题</strong></p>
</blockquote>
<ul>
<li>为尽快让你理解 现假设现有多元线性回归 并约定 x0=1 该模型的参数是从 θ0 到 θn 不要认为这是 n+1 个单独的参数 你可以把这 n+1 个 θ 参数想象成一个 n+1 维的向量 θ 所以 你现在就可以把这个模型的参数 想象成其本身就是一个<code>n+1 维的向量</code> 我们的代价函数是从 θ0 到 θn 的函数 J 并给出了误差项平方的和 但同样地 不要把函数 J 想成是一个关于 n+1 个自变量的函数 而是看成带有一个 <code>n+1 维向量的函数</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204601.png" alt=""></p>
<ul>
<li>这就是梯度下降法 我们将会<strong>不停地用 θj 减去 α 倍的导数项 来替代 θj</strong> 同样的方法 我们写出函数J(θ) 因此 θj 被更新成 θj 减去学习率 α 与对应导数的乘积 就是代价函数的对参数 θj 的偏导数 当我们实现梯度下降法后 你可以仔细观察一下 尤其是它的偏导数项</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204617.png" alt=""></p>
<ul>
<li>下面是我们当特征 n=1 时 梯度下降的情况 我们有两条针对参数 θ0 和 θ1 不同的更新规则 希望这些对你来说并不陌生 这一项是代价函数里部分求导的结果 就是<strong>代价函数相对于 θ0 的偏导数</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204640.png" alt=""></p>
<ul>
<li>同样 对参数 θ1 我们有另一个更新规则  仅有的一点区别是 当我们之前只有一个特征 我们称该特征为x(i) 但现在我们在新符号里 我们会标记它为 x 上标 (i) 下标1 来表示我们的特征 以上就是当我们仅有一个特征时候的算法</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204701.png" alt=""></p>
<ul>
<li>下面我们来讲讲<strong>当有一个以上特征时候的算法</strong> 现有数目远大于1的很多特征 我们的梯度下降更新规则变成了这样 有些同学可能知道微积分 如果你看看代价函数 <strong>代价函数 J 对参数 θj 求偏导数</strong> 你会发现 求其偏导数的那一项 我已经用蓝线圈出来了 如果你实现了这一步 你将会得到多元线性回归的梯度下降算法</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204716.png" alt=""></p>
<ul>
<li>最后 我想让你明白 为什么新旧两种算法实际上是一回事儿 或者说为什么这两个是类似的算法 为什么它们都是梯度下降算法 考虑这样一个情况 有两个或以上个数的特征 同时我们有对θ1、θ2、θ3的三条更新规则 当然可能还有其它参数 如果你观察θ0的更新规则  你会发现这跟之前 n=1的情况相同 它们之所以是等价的 这是因为在我们的标记约定里有 x(i)0=1 也就是 我用品红色圈起来的两项是等价的 同样地 如果你观察 θ1 的更新规则 你会发现这里的这一项是 和之前对参数θ1的更新项是等价的</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204732.png" alt=""></p>
<ul>
<li>在这里我们只是用了新的符号x(i)1来表示我们的第一个特征 现在我们有个更多的特征 那么就可以用与之前相同的更新规则 我们可以用同样的规则来处理 θ2 等其它参数 这张幻灯片的内容不少 请务必仔细理解 如果觉得幻灯片上数学公式没看懂 尽管暂停视频 请确保理解了再继续后面的学习 如果你将这些算法都实现了 那么你就可以直接应用到多元线性回归中了</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204746.png" alt=""></p>
<h3 id="多变量梯度下降笔记"><a href="#多变量梯度下降笔记" class="headerlink" title="多变量梯度下降笔记"></a>多变量梯度下降笔记</h3><p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204804.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204811.png" alt=""></p>
<h2 id="Gradient-Descent-in-Practice-1-Feature-Sacling-梯度下降法实践1-特征缩放"><a href="#Gradient-Descent-in-Practice-1-Feature-Sacling-梯度下降法实践1-特征缩放" class="headerlink" title="Gradient Descent in Practice 1-Feature Sacling(梯度下降法实践1-特征缩放)"></a>Gradient Descent in Practice 1-Feature Sacling(梯度下降法实践1-特征缩放)</h2><blockquote>
<p>在这段视频 以及下一段视频中 我想告诉你一些关于 梯度下降运算中的实用技巧 在这段视频中 我会告诉你一个称为<code>特征缩放 (feature scaling)</code> 的方法</p>
</blockquote>
<ul>
<li>如果你有一个机器学习问题 这个问题有多个特征 如果你能确保这些特征 都处在一个相近的范围 我的意思是<strong>确保不同特征的取值 在相近的范围内,这样梯度下降法就能更快地收敛</strong></li>
<li>具体地说 假如你有一个具有两个特征的问题 其中 <code>x1</code> 是房屋面积大小 它的取值 在<code>0到2000</code>之间 <code>x2</code> 是卧室的数量 可能这个值 取值范围在<code>1到5</code>之间 如果你画出代价函数 J(θ) 的轮廓图 那么这个轮廓看起来 应该是像这样的 J(θ) 是一个关于 参数 θ0 θ1 和 θ2 的函数 但我要忽略 θ0 所以暂时不考虑 θ0 并假想一个函数的变量 只有 θ1 和 θ2 但如果 x1 的取值范围 远远大于 x2 的取值范围的话 那么最终画出来的 代价函数 J(θ) 的轮廓图 就会呈现出这样一种 <strong>非常偏斜 并且椭圆的形状 2000 和 5的比例</strong> 会让这个椭圆更加瘦长 所以 这是一个又瘦又高的 椭圆形轮廓图 就是这些非常高大细长的椭圆形 构成了代价函数 J(θ)</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204851.png" alt=""></p>
<ul>
<li>而如果你用这个代价函数 来运行梯度下降的话 你要得到梯度值 <strong>最终可能 需要花很长一段时间 并且可能会来回波动 然后会经过很长时间 最终才收敛到<code>全局最小值</code></strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204920.png" alt=""></p>
<ul>
<li>事实上 你可以想像 如果这些 轮廓再被放大一些的话 如果你画的再夸张一些 把它画的更细更长 那么可能情况会更糟糕 梯度下降的过程 可能更加缓慢 需要花更长的时间 反复来回振荡 最终才找到一条正确通往全局最小值的路</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204943.png" alt=""></p>
<ul>
<li>在这样的情况下 一种有效的方法是进行<code>特征缩放(feature scaling)</code> 具体来说 把特征 x 定义为 房子的面积大小 除以2000的话 并且把 x2 定义为 卧室的数量除以5 那么这样的话 表示代价函数 J(θ) 的轮廓图的形状 就会变得偏移没那么严重 可能看起来更圆一些了</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104204956.png" alt=""></p>
<ul>
<li>如果你用这样的代价函数 来执行梯度下降的话 那么 梯度下降算法</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205010.png" alt=""></p>
<ul>
<li>你可以从数学上来证明 梯度下降算法 就会找到一条 更捷径的路径通向全局最小 而不是像刚才那样 沿着一条让人摸不着头脑的路径 一条复杂得多的轨迹 来找到全局最小值 因此 通过特征缩放 通过”消耗掉”这些值的范围 在这个例子中 我们最终得到的两个特征 x1 和 x2 都在<code>0和1之间</code> 这样你得到的梯度下降算法 就会更快地收敛</li>
<li>更一般地 我们执行特征缩放时 我们通常的目的是 <strong>将特征的取值约束到 <code>-1 到 +1</code> 的范围内</strong> 你的特征 x0 是总是等于1 因此 这已经是在这个范围内 但<strong>对其他的特征 你可能需要通过<code>除以</code>不同的数 来让它们处于同一范围内</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205024.png" alt=""></p>
<ul>
<li><strong>-1 和 +1 这两个数字并不是太重要</strong> 所以 如果你有一个特征  x1 它的取值 在0和3之间 这没问题 如果你有另外一个特征 取值在-2 到 +0.5之间 这也没什么关系 这也非常接近 -1 到 +1的范围 这些都可以</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205036.png" alt=""></p>
<ul>
<li>但如果你有另一个特征 比如叫 x3 假如它的范围 在 -100 到 +100之间 那么 这个范围 跟-1到+1就有很大不同了 所以 这可能是一个 不那么好的特征 类似地 如果你的特征在一个 非常非常小的范围内 比如另外一个特征 x4 它的范围在 0.0001和+0.0001之间 那么 这同样是一个 比-1到+1小得多的范围 比-1到+1小得多的范围 因此 我同样会认为这个特征也不太好</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205059.png" alt=""></p>
<ul>
<li>所以 可能你认可的范围 也许可以大于 或者小于 -1 到 +1 但是也别太大 只要大得不多就可以接受 比如 +100 或者也别太小 比如这里的0.001 不同的人有不同的经验 但是我一般是这么考虑的 <strong>如果一个特征是在 <code>-3 到 +3</code> 的范围内 那么你应该认为 这个范围是可以接受的</strong> 但如果这个范围 大于了 -3 到 +3 的范围 我可能就要开始注意了 如果它的取值 在-1/3 到+1/3的话 我觉得 还不错 可以接受 或者是0到1/3 或-1/3到0 这些典型的范围 我都认为是可以接受的 但如果特征的范围 取得很小的话 比如像这里的 x4 你就要开始考虑进行特征缩放了</li>
<li>因此 总的来说 不用过于担心 你的特征是否在完全 相同的范围或区间内 但是只要他们都 <strong>只要它们足够接近的话 梯度下降法就会正常地工作</strong></li>
<li>除了在特征缩放中 将特征除以最大值以外 有时候我们也会进行一个 称为<code>均值归一化的工作(mean normalization)</code> 我的意思是这样的 如果你有一个特征 <code>xi</code> 你就用 <code>xi - μi</code> 来替换 通过这样做 <strong>让你的特征值 具有为0的平均值</strong> 很明显 我们不需要 把这一步应用到 x0中 因为 x0 总是等于1的 所以它不可能有 为0的的平均值</li>
<li>但是 对其他的特征来说 比如房子的大小 取值介于0到2000 并且假如 房子面积 的平均值 是等于1000的 那么你可以用这个公式 将 x1 的值变为 x1 减去平均值 μ1 再除以2000 类似地 如果你的房子有 五间卧室 并且平均一套房子有 两间卧室 那么你可以 使用这个公式 来归一化你的第二个特征 x2 在这两种情况下 你可以算出新的特征 x1 和 x2 这样它们的范围 可以在-0.5和+0.5之间 当然这肯定不对 x2的值实际上肯定会大于0.5 但很接近</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205132.png" alt=""></p>
<ul>
<li>更一般的规律是 你可以用这样的公式 你可以用 <code>(x1 - μ1)/S1</code> 来替换原来的特征 x1 其中定义<code>μ1</code>的意思是 在训练集中 <strong>特征 x1 的平均值</strong> 而 <code>S1</code> 是 该特征值的范围 我说的范围是指 <strong>最大值减去最小值</strong>或者学过 标准差的同学可以记住 也可以把 S1 设为 变量的<code>标准差</code> 但其实用最大值减最小值就可以了 类似地 对于第二个 特征 x2 你也可以用同样的这个 特征减去平均值 再除以范围 来替换原特征 范围的意思依然是最大值减最小值 这类公式将 把你的特征 变成这样的范围 也许不是完全这样 但大概是这样的范围</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205148.png" alt=""></p>
<ul>
<li>顺便提一下 有些同学可能比较仔细 如果我们用最大值减最小值 来表示范围的话 这里的5有可能应该是4 如果最大值为5 那么减去最小值1 这个范围值就是4 但不管咋说 这些取值 都是非常近似的 <strong>只要将特征转换为 相近似的范围 就都是可以的</strong> <strong>特征缩放其实 并不需要太精确 只是为了让梯度下降 能够运行得更快一点而已</strong></li>
<li>好的 现在你知道了 什么是特征缩放 通过使用这个简单的方法 你可以将梯度下降的速度变得更快 让梯度下降收敛所需的循环次数更少 这就是特征缩放 在接下来的视频中 我将介绍另一种技巧来使梯度下降 在实践中工作地更好</li>
</ul>
<h2 id="Gradient-Descent-in-Practice-II-Learning-Rate-梯度下降法实践2-学习率"><a href="#Gradient-Descent-in-Practice-II-Learning-Rate-梯度下降法实践2-学习率" class="headerlink" title="Gradient Descent in Practice II-Learning Rate(梯度下降法实践2-学习率)"></a>Gradient Descent in Practice II-Learning Rate(梯度下降法实践2-学习率)</h2><blockquote>
<p>在本段视频中 我想告诉大家 一些关于梯度下降算法的实用技巧 我将集中讨论 <code>学习率 α</code> 具体来说 这是梯度下降算法的 更新规则 这里我想要 告诉大家 如何调试 也就是我认为应该如何确定 梯度下降是正常工作的 此外我还想告诉大家 如何选择学习率 α 也就是我平常 如何选择这个参数 我通常是怎样确定 梯度下降正常工作的</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205201.png" alt=""></p>
<ul>
<li><strong>梯度下降算法所做的事情 就是为你找到 一个 θ 值 并希望它能够最小化代价函数 J(θ) </strong>我通常会在 梯度下降算法运行时 绘出代价函数 J(θ) 的值 这里的 <code>x 轴</code>是表示 <strong>梯度下降算法的 迭代步数</strong> 你可能会得到 这样一条曲线 注意 这里的 x 轴 是迭代步数 在我们以前看到的 J(θ) 曲线中 x 轴 也就是横轴 曾经用来表示参数 θ 但这里不是 具体来说 这一点的含义是这样的 当我运行完100步的梯度下降迭代之后 无论我得到 什么 θ 值 总之 100步迭代之后 我将得到 一个 θ 值 根据100步迭代之后 得到的这个 θ 值 我将算出 代价函数 J(θ) 的值 而这个点的垂直高度就代表 梯度下降算法 100步迭代之后 得到的 θ 算出的 J(θ) 值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205220.png" alt=""></p>
<ul>
<li>而这个点 则是梯度下降算法 迭代200次之后 得到的 θ 算出的 J(θ) 值 所以这条曲线 显示的是 梯度下降算法迭代过程中代价函数 J(θ) 的值 <strong>如果梯度下降算法 正常工作 那么每一步迭代之后 J(θ) 都应该下降</strong> 这条曲线 的一个用处在于 它可以告诉你 如果你看一下 我画的这条曲线 当你达到 300步迭代之后 也就是300步到400步迭代之间 就是曲线的这一段 看起来 J(θ) 并没有下降多少 所以当你 到达400步迭代时 这条曲线看起来已经很平坦了 也就是说 在这里400步迭代的时候 梯度下降算法 基本上已经收敛了 因为代价函数并没有继续下降 所以说 看这条曲线 可以帮助你判断 梯度下降算法是否已经收敛</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205248.png" alt=""></p>
<ul>
<li>顺便说一下 对于每一个特定的问题 梯度下降算法所需的迭代次数 可以相差很大 也许对于某一个问题 梯度下降算法 只需要30步迭代就可以收敛 然而换一个问题 也许梯度下降算法就需要3000步迭代 对于另一个机器学习问题 则可能需要三百万步迭代 实际上 我们很难提前判断</li>
<li><strong>梯度下降算法 需要多少步迭代才能收敛</strong> 通常我们需要画出这类曲线 <strong>画出代价函数随迭代步数数增加的变化曲线</strong> 通常 我会通过看这种曲线 来试着判断 梯度下降算法是否已经收敛 另外 也可以 进行一些<strong>自动的收敛测试</strong> 也就是说用一种算法 来告诉你梯度下降算法 是否已经收敛 自动收敛测试 一个非常典型的例子是 如果代价函数 J(θ) 的下降小于 一个很小的值 ε 那么就认为已经收敛 比如可以选择 1e-3 但我发现 通常要选择一个合适的阈值 ε 是相当困难的 因此 为了检查 梯度下降算法是否收敛 我实际上还是 通过看 左边的这条曲线图 而不是依靠自动收敛测试</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205302.png" alt=""></p>
<ul>
<li>此外 这种曲线图 也可以 在算法没有正常工作时 提前警告你 具体地说 如果代价函数 J(θ) 随迭代步数 的变化曲线是这个样子 J(θ) 实际上在不断上升 那就很明显的表示 梯度下降算法没有正常工作 而这样的曲线,通常意味着你应该使用<strong>较小的学习率a</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205314.png" alt=""></p>
<ul>
<li>这样的曲线图表示你的学习率a太大了,因此得到的结果也会不断跳过最小值,变得越来越大,因此应该使用<strong>较小的学习率a</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205324.png" alt=""></p>
<ul>
<li>这种图也是要选择<strong>小的a</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205332.png" alt=""></p>
<ul>
<li>a不可以太大,也不可以太小</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205430.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205448.png" alt=""></p>
<ul>
<li>一般尝试的a值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205502.png" alt=""></p>
<h2 id="Features-and-Polynomial-Regression-特征和多项式回归"><a href="#Features-and-Polynomial-Regression-特征和多项式回归" class="headerlink" title="Features and Polynomial Regression 特征和多项式回归"></a>Features and Polynomial Regression 特征和多项式回归</h2><blockquote>
<p>你现在了解了多变量的线性回归 在本段视频中 我想告诉你 一些<strong>用来 选择特征的方法</strong>以及 <strong>如何得到不同的学习算法</strong> 当选择了合适的特征后 这些算法往往是非常有效的 另外 我也想 给你们讲一讲<code>多项式回归</code> 它使得你们能够<strong>使用 线性回归的方法来拟合 非常复杂的函数 甚至是非线性函数</strong></p>
</blockquote>
<ul>
<li>以预测房价为例 假设你有两个特征 分别是房子<code>临街的宽度</code>和<code>垂直宽度</code> 这就是我们想要卖出的房子的图片 临街宽度 被定义为这个距离 其实就是它的宽度 或者说是 你拥有的土地的宽度 如果这块地都是你的的话 而这所房子的 纵向深度就是 你的房子的深度 这是正面的宽度 这是深度 我们称之为临街宽度和纵深 你可能会 像这样 建立一个 线性回归模型 其中临街宽度 是你的第一个特征x1 纵深是你的第二个 特征x2</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205534.png" alt=""></p>
<ul>
<li>但当我们在 运用线性回归时 你不一定非要直接用 给出的 x1 和 x2 作为特征 其实你<strong>可以自己创造新的特征</strong> 因此 如果我要预测 房子的价格 我真正要需做的 也许是 确定真正能够决定 我房子大小 或者说我土地大小 的因素是什么 因此 我可能会创造一个新的特征 我称之为 x 它是临街宽度与纵深的乘积 这是一个乘法符号 它是临街宽度与纵深的乘积 这得到的就是我拥有的土地的面积 然后 我可以把 假设选择为 使其只使用 一个特征 也就是我的 土地的面积 对吧？</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205550.png" alt=""></p>
<ul>
<li>由于矩形面积的 计算方法是 矩形长和宽相乘 因此 这取决于 你从什么样的角度 去审视一个特定的问题 而不是 只是直接去使用临街宽度和纵深 这两个我们只是碰巧在开始时 使用的特征 有时 通过定义 新的特征 你确实会得到一个更好的模型</li>
<li>与选择特征的想法 密切相关的一个概念 被称为<code>多项式回归(polynomial regression)</code> 比方说 你有这样一个住房价格的数据集 为了拟合它 可能会有多个不同的模型供选择 其中一个你可以选择的是像这样的<code>二次模型</code> 因为直线似乎并不能很好地拟合这些数据 因此 也许你会想到 用这样的二次模型去拟合数据 你可能会考量 是关于价格的一个二次函数 也许这样做 会给你一个 像这样的拟合结果</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205609.png" alt=""></p>
<ul>
<li>但是 然后你可能会觉得 二次函数的模型并不好用 因为 一个二次函数最终 会降回来 而我们并不认为 房子的价格在高到一定程度后 会下降回来</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205635.png" alt=""></p>
<ul>
<li>因此 也许我们会 选择一个不同的多项式模型 并转而选择使用一个 <code>三次函数</code> 在这里 现在我们有了一个三次的式子 我们用它进行拟合 我们可能得到这样的模型 也许这条绿色的线 对这个数据集拟合得更好 因为它不会在最后下降回来</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205715.png" alt=""></p>
<ul>
<li>那么 我们到底应该如何将模型与我们的数据进行拟合呢？ 使用<code>多元线性回归的方法</code> 我们可以 通过将我们的算法做一个非常简单的修改来实现它 按照我们以前假设的形式 我们知道如何对 这样的模型进行拟合 其中 <code>ħθ(x)</code> 等于 <code>θ0 +θ1×x1 + θ2×x2 + θ3×x3</code> 那么 如果我们想 拟合这个三次模型 就是我用绿色方框框起来的这个 现在我们讨论的是 为了预测一栋房子的价格 我们用 θ0 加 θ1 乘以房子的面积 加上 θ2 乘以房子面积的平方 因此 这个式子与那个式子是相等的 然后再加 θ3 乘以 房子面积的立方 为了将这两个定义 互相对应起来 为了做到这一点 我们自然想到了 将 x1 特征设为 房子的面积 将第二个特征 x2 设为 房屋面积的平方 将第三个特征 x3 设为 房子面积的立方</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205728.png" alt=""></p>
<ul>
<li>那么 仅仅通过将 这三个特征这样设置 然后再应用线性回归的方法 我就可以拟合 这个模型 并最终 将一个三次函数拟合到我的数据上 我还想再说一件事 那就是 如果你像这样选择特征 那么<code>特征的归一化</code> 就变得更重要了 因此 如果 房子的大小范围在 <code>1到1000</code>之间 那么 比如说 从1到1000平方尺 那么 房子面积的平方 的范围就是 <code>一到一百万</code> 也就是 1000的平方 而你的第三个特征 x的立方 抱歉 你的第三个特征 x3 它是房子面积的 立方 范围会扩大到 1到<code>10的9次方</code> 因此 <strong>这三个特征的范围 有很大的不同 因此 如果你使用梯度下降法 应用<code>特征值的归一化</code>是非常重要的</strong> 这样才能将他们的 值的范围变得具有可比性</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205743.png" alt=""></p>
<ul>
<li>最后 这里是最后一个例子 关于如何使你 真正选择出要使用的特征 此前我们谈到 一个像这样的二次模型 并不是理想的 因为 你知道 也许一个二次模型能很好地拟合 这个数据 但二次 函数最后会下降 这是我们不希望的 就是住房价格往下走 像预测的那样 出现房价的下降 但是 除了转而 建立一个三次模型以外 你也许有其他的选择 特征的方法 这里有很多可能的选项 但是给你另外一个 合理的选择的例子</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205757.png" alt=""></p>
<ul>
<li>另一种合理的选择 可能是这样的 一套房子的价格是 <code>θ0 加 θ1 乘以 房子的面积 然后 加 θ2 乘以房子面积的平方根</code> 可以吧？</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205805.png" alt=""></p>
<ul>
<li>平方根函数是 这样的一种函数 也许θ1 θ2 θ3 中会有一些值 会捕捉到这个模型 从而使得这个曲线看起来 是这样的 趋势是上升的 但慢慢变得 平缓一些 而且永远不会 下降回来</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205813.png" alt=""></p>
<ul>
<li>因此 通过深入地研究 在这里我们研究了平方根 函数的形状 并且 更深入地了解了选择不同特征时数据的形状 有时可以得到更好的模型 在这段视频中 我们探讨了多项式回归 也就是 如何将一个 多项式 如一个二次函数 或一个三次函数拟合到你的数据上 除了这个方面 我们还讨论了 在使用特征时的选择性 例如 我们不使用 房屋的临街宽度和纵深 也许 你可以 把它们乘在一起 从而得到 房子的土地面积这个特征 实际上 这似乎有点 难以抉择 这里有这么多 不同的特征选择 我该如何决定使用什么特征呢</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205823.png" alt=""></p>
<ul>
<li>在之后的课程中 我们将 探讨一些算法 它们能够 自动选择要使用什么特征 因此 你可以使用一个算法 观察给出的数据 并自动为你选择 到底应该选择 一个二次函数 或者一个三次函数 还是别的函数 但是 在我们 学到那种算法之前 现在我希望你知道 你需要选择 使用什么特征 并且通过设计不同的特征 你能够用更复杂的函数 去拟合你的数据 而不是只用 一条直线去拟合 特别是 你也可以使用多项式 函数 有时候 通过采取适当的角度来观察 特征就可以 得到一个更符合你的数据的模型</li>
</ul>
<h2 id="Normal-Equation-正规方程"><a href="#Normal-Equation-正规方程" class="headerlink" title="Normal Equation 正规方程"></a>Normal Equation 正规方程</h2><blockquote>
<p>在这段视频中 我们要讲 <code>正规方程 (Normal Equation)</code>对于某些线性回归问题 用<strong>正规方程法求解参数 θ 的最优值</strong>更好</p>
</blockquote>
<ul>
<li>具体而言 到目前为止 我们一直在使用的<code>线性回归</code>的算法 是梯度下降法 就是说 为了最小化代价函数 J(θ) 来最小化这个 我们使用的迭代算法 需要经过很多步  也就是说通过多次迭代来计算梯度下降 来收敛到<strong>全局最小值</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205846.png" alt=""></p>
<ul>
<li>相反地  正规方程法提供了一种求 θ 的解析解法 所以与其使用迭代算法   我们<strong>可以直接一次性求解θ的最优值</strong> 所以说基本上 一步就可以得到优化值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205857.png" alt=""></p>
<ul>
<li>正规方程法有一些优点 也有一些缺点 但是在我们讲解这个 和何时使用标准方程之前  让我们先对这个算法有一个直观的理解</li>
<li>我们举一个例子来解释这个问题 我们假设 有一个非常简单的代价函数 <code>J(θ)</code> 它就是一个<code>实数 θ</code> 的函数  所以现在 假设 <strong>θ 只是一个标量</strong> 或者说 θ 只有一行 它是一个数字 不是向量 假设我们的<code>代价函数 J</code> 是这个实参数 θ 的二次函数 所以 J(θ) 看起来是这样的</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205908.png" alt=""></p>
<ul>
<li>那么如何最小化一个二次函数呢? 对于那些了解一点微积分的同学来说 你可能知道 <code>最小化的一个函数</code>的方法是 <strong>对它求导 并且将导数置零</strong>  所以对 J 求关于 θ 的导数 我不打算推导那些公式 你把那个导数置零 这样你就可以求得  使得 J(θ) 最小的 θ 值 这是数据为实数的 一个比较简单的例子</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205924.png" alt=""></p>
<ul>
<li>在这个问题中 我们感兴趣的是 <strong>θ不是一个实数的情况 它是一个<code>n+1维</code>的参数向量</strong>  并且 <code>代价函数 J</code> 是这个向量的函数  也就是 θ0 到 θm 的函数 一个代价函数看起来是这样 像右边的这个平方代价函数 我们如何最小化这个代价函数J? 实际上 微积分告诉我们一种方法  <strong>对每个参数 θ 求 J 的偏导数  然后把它们全部置零 如果你这样做 并且求出θ0 θ1 一直到θn的值</strong> 这样就能得到能够最小化代价函数 J 的 θ 值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205935.png" alt=""></p>
<ul>
<li><p>如果你真的做完微积分和求解参数 θ0 到 θn   你会发现这个偏微分最终可能很复杂  接下来我在视频中要做的 实际上不是遍历所有的偏微分 因为这样太久太费事 我只是想告诉你们 你们想要实现这个过程所需要知道内容  这样你就可以解出 偏导数为0时 θ的值  换个方式说 或者等价地 这个 θ 能够使得代价函数 J(θ) 最小化  我发现可能只有熟悉微积分的同学 比较容易理解我的话 所以 如果你不了解 或者不那么了解微积分 也不必担心 我会告诉你 要实现这个算法并且使其正常运行 你所需的必要知识</p>
</li>
<li><p>举个例子 我想运行这样一个例子 假如说我有 m=4 个训练样本 假如说我有 m=4 个训练样本</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104205946.png" alt=""></p>
<ul>
<li>为了实现<code>正规方程法</code> 我要这样做 看我的训练集 在这里就是这四个训练样本 在这种情况下 我们假设 这四个训练样本就是我的所有数据 我所要做的是 <strong>在我的训练集中加上一列对应额外特征变量的x0 就是那个取值永远是1的</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210007.png" alt=""></p>
<ul>
<li>接下来我要做的是 构建一个<code>矩阵 X</code> 这个矩阵<strong>基本包含了训练样本的所有特征变量</strong>  所以具体地说  这里有我所有的特征变量 我们要把这些数字 全部放到矩阵中 X 中 所以只是 每次复制一列的数据</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210034.png" alt=""></p>
<ul>
<li>我要对 y 做类似的事情  我要对我们将要预测的值 构建一个向量 像这样的 并且称之为<code>向量 y</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210111.png" alt=""></p>
<ul>
<li>所以 <code>X 会是一个 m*(n+1) 维矩阵</code> <code>y 会是一个 m 维向量</code>  其中 <code>m</code> 是<code>训练样本数量</code> <code>n</code> 是<code>特征变量数</code> n+1 是因为我加的这个额外的特征变量 x0  最后 如果你用矩阵 X 和向量 y 来计算这个 最后   <code>θ</code> 等于 <code>X 转置乘以 X 的逆 乘以 X 转置 乘以 y</code>  这样就得到能够使得代价函数最小化的 θ  幻灯片上的内容比较多 我讲解了这样一个数据组的一个例子 让我把这个写成更加通用的形式  在之后的视频中 我会仔细介绍这个方程</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210153.png" alt=""></p>
<ul>
<li>在一般情况下 假如我们有 m 个训练样本 x(1) y(1) 直到 x(m) y(m) n 个特征变量 所以每一个训练样本 xi 可能看起来像一个向量 像这样一个 n+1 维特征向量</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210255.png" alt=""></p>
<ul>
<li>我要构建矩阵 X 的方法  也被称为<code>设计矩阵</code> 如下所示  每个训练样本给出一个这样的特征向量 也就是说 这样的 n+1 维向量 我构建我的设计矩阵 X 的方法 就是构建这样的矩阵 接下来我要做的是将 取第一个训练样本 也就是一个向量 取它的转置 它最后是这样 扁长的样子 让 x1 转置作为我设计矩阵的第一行 然后我要把我的 第二个训练样本 x2 进行转置 让它作为 X 的第二行  以此类推 直到最后一个训练样本 取它的转置作为矩阵 X 的最后一行  这样矩阵 X 就是一个 m*(n+1) 维矩阵</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210323.png" alt=""></p>
<ul>
<li>举个具体的例子 假如我只有一个特征变量 就是说除了 x0 之外只有一个特征变量  而 x0 始终为1 所以如果我的特征向量 xi等于1 也就是x0 和某个实际的特征变量 比如说房屋大小 那么我的设计矩阵 X 会是这样 第一行 就是这个的转置  所以最后得到1 然后 x(1)1 对于第二行 我们得到1 然后 x(1)2  这样直到1 然后 x(1)m 这就会是一个 <code>m*2</code> 维矩阵</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210426.png" alt=""></p>
<ul>
<li>所以 <strong>这就是如何构建矩阵X 和向量y</strong>  有时我可能会在上面画一个箭头  来表示这是一个向量 但很多时候 我就只写y 是一样的 向量y 是这样求得的 把所有标签 所有训练集中正确的房子价格  放在一起 得到一个 m 维向量 y</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210443.png" alt=""></p>
<ul>
<li><strong>最后 构建完矩阵 X 和向量 y 我们就可以通过计算 X转置 乘以X的逆 乘以X转置 乘以y 来得到θ</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210456.png" alt=""></p>
<ul>
<li>我现在就想确保你明白这个等式  并且知道如何实现它 所以具体来说 什么是 X 的转置乘以 X 的逆？ <strong>X的转置 乘以 X的逆 是 X转置 乘以X的逆矩阵</strong> 具体来说 如果你令A等于 X转置乘以X  X的转置是一个矩阵  阵 我们把这个矩阵称为 A 那么 X转置乘以X的逆 就是矩阵 A 的逆  也就是 <code>1/A</code> 这就是计算过程 先计算 X转置乘以X 然后计算它的逆</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210506.png" alt=""></p>
<ul>
<li>我们还没有谈到Octave 我们将在之后的视频中谈到这个 但是在 Octave 编程语言  或者类似的 MATLAB 编程语言里  计算这个量的命令是基本相同的 X转置 乘以X的逆 乘以X转置 乘以y 的代码命令如下所示</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210525.png" alt=""></p>
<ul>
<li>在 Octave 中 <code>X’</code> 表示 <code>X 转置</code>  这个用红色框起来的表达式 计算的是 X 转置乘以 X  <code>函数 pinv</code> 是用来计算<code>逆矩阵</code>的函数 所以这个计算 X转置 乘以X的逆  然后乘以X转置 再乘以y  这样就算完了这个式子</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210536.png" alt=""></p>
<ul>
<li>我没有证明这个式子 尽管我并不打算这么做 但是数学上是可以证明的 这个式子会给出最优的 θ 值  就是说如果你令 θ 等于这个 就是说如果你令 θ 等于这个 这个 θ 值会最小化这个线性回归的代价函数 J(θ)</li>
<li>最后一点 在之前视频中我提到<code>特征变量归一化</code> 和<code>让特征变量在相似的范围内</code>的想法 将所有的值归一化在类似范围内 <strong>如果你使用正规方程法 那么就不需要归一化特征变量</strong> 实际上这是没问题的 如果某个特征变量 x1 在 0到1的区间  某个特征变量 x2 在0到1000的区间  某个特征变量x3 在0到10^-5的区间 然后如果使用正规方程法 这样就没有问题 不需要做特征变量归一化 <strong>但如果你使用梯度下降法  特征变量归一化就很重要</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210549.png" alt=""></p>
<ul>
<li>最后 你何时应该使用梯度下降法 而何时应该使用正规方程法呢？ 这里列举了一些它们的优点和缺点 假如你有 m 个训练样本和 n 个特征变量 <strong>梯度下降法的缺点之一</strong>就是 你需要选择学习速率 α 这通常表示需要运行多次 尝试不同的学习速率 α 然后找到运行效果最好的那个 所以这是一种额外的工作和麻烦 <strong>梯度下降法的另一个缺点是</strong> 它需要更多次的迭代 因为一些细节 计算可能会更慢 我们一会儿会看到更多的东西</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210615.png" alt=""></p>
<ul>
<li><p>至于正规方程 你不需要选择学习速率 α 所以就非常方便 也容易实现 你只要运行一下 通常这就够了 并且你也不需要迭代 所以不需要画出 $J(\theta)$ 的曲线  来检查收敛性或者采取所有的额外步骤 到目前为止 天平似乎倾向于正规方程法</p>
</li>
<li><p>这里列举一些正规方程法的缺点 和梯度下降法的优点 <strong>梯度下降法在有很多特征变量的情况下也能运行地相当好</strong>  所以即使你有上百万的特征变量 所以即使你有上百万的特征变量 你可以运行梯度下降法 并且通常很有效 它会正常的运行 相对地 正规方程法 为了求解参数θ 需要求解这一项  我们需要计算这项 <code>X转置乘以X的逆</code> 这个是一个 <code>n*n</code> 的矩阵 如果你有 n 个特征变量的话 因为如果你看一下 X转置乘以X 的维度 你可以发现他们的<code>积的维度</code>  X转置乘以X 是一个 <code>n*n</code> 的矩阵 其中 n是特征变量的数量  <strong>实现逆矩阵计算所需要的计算量  大致是矩阵维度的三次方 因此计算这个逆矩阵需要计算大致 n 的三次方</strong>  有时稍微比计算 n 的三次方快一些 但是对我们来说很接近 所以如果特征变量的数量 n 很大的话 那么计算这个量会很慢  实际上标准方程法会慢很多  因此<strong>如果 n 很大 我可能还是会使用梯度下降法 因为我们不想花费 n 的三次方的时间 但如果 n 比较小 那么标准方程法可能更好地求解参数 θ</strong></p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210624.png" alt=""></p>
<ul>
<li><strong>那么怎么叫大或者小呢？</strong> 那么 如果 n 是上百的  计算百位数乘百位数的矩阵 对于现代计算机来说没有问题 如果 n 是上千的 我还是会使用正规方程法 千位数乘千位数的矩阵做逆变换 对于现代计算机来说实际上是非常快的 但如果 n 上万 那么我可能会开始犹豫 上万乘上万维的矩阵作逆变换 会开始有点慢 此时我可能开始倾向于 梯度下降法 但也不绝对 n 等于一万 你可以 逆变换一个一万乘一万的矩阵 但如果 n 远大于此 我可能就会使用梯度下降法了 所以如果 n 等于10^6 有一百万个特征变量 那么做百万乘百万的矩阵的逆变换 就会变得非常费时间 在这种情况下我一定会使用梯度下降法 所以很难给出一个确定的值 来决定何时该换成梯度下降法  但是 对我来说通常是 <strong>在一万左右 我会开始考虑换成梯度下降法</strong>  或者我们将在以后讨论到的其他算法</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210638.png" alt=""></p>
<ul>
<li>总结一下 只要特征变量的数目并不大 正规方程是一个很好的 计算参数 θ 的替代方法 具体地说 只要特征变量数量小于一万  我通常使用正规方程法  而不使用梯度下降法 预告一下在之后的课程中我们要讲的  随着我们要讲的学习算法越来越复杂  例如 当我们讲到分类算法 像逻辑回归算法 我们会看到 实际上对于那些算法 并不能使用正规方程法 对于那些更复杂的学习算法 我们将不得不仍然使用梯度下降法  因此 梯度下降法是一个非常有用的算法 可以用在有大量特征变量的线性回归问题 或者我们以后在课程中 会讲到的一些其他的算法 因为 标准方程法不适合或者不能用在它们上   但对于这个特定的线性回归模型 正规方程法是一个 比梯度下降法更快的替代算法 所以 根据具体的问题 所以  以及你的特征变量的数量 这两算法都是值得学习的<h3 id="正规方程笔记"><a href="#正规方程笔记" class="headerlink" title="正规方程笔记"></a>正规方程笔记</h3></li>
</ul>
<p>到目前为止，我们都在使用梯度下降算法，但是对于某些线性回归问题，正规方程方法是更好的解决方案。如：</p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210651.png" alt=""></p>
<p>正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\frac{\partial}{\partial{\theta_{j}}}J\left( {\theta_{j}} \right)=0$ 。<br> 假设我们的训练集特征矩阵为 $X$（包含了 ${{x}_{0}}=1$）并且我们的训练集结果为向量 $y$，则利用正规方程解出向量 $\theta ={{\left( {X^T}X \right)}^{-1}}{X^{T}}y$ 。<br>上标T代表矩阵转置，上标-1 代表矩阵的逆。设矩阵$A={X^{T}}X$，则：${{\left( {X^T}X \right)}^{-1}}={A^{-1}}$<br>以下表示数据为例：</p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210731.png" alt=""></p>
<p>即：</p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210742.png" alt=""></p>
<p>运用正规方程方法求解参数：</p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210754.png" alt=""></p>
<p>在 Octave 中，正规方程写作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pinv(X&apos;*X)*X&apos;*y</span><br></pre></td></tr></table></figure>
<p>注：对于那些不可逆的矩阵（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量），正规方程方法是不能用的。</p>
<p>梯度下降与正规方程的比较：</p>
<table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td>需要选择学习率$\alpha$</td>
<td>不需要</td>
</tr>
<tr>
<td>需要多次迭代</td>
<td>一次运算得出</td>
</tr>
<tr>
<td>当特征数量$n$大时也能较好适用</td>
<td>需要计算${{\left( {{X}^{T}}X \right)}^{-1}}$ 如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O\left( {{n}^{3}} \right)$，通常来说当$n$小于10000 时还是可以接受的</td>
</tr>
<tr>
<td>适用于各种类型的模型</td>
<td>只适用于线性模型，不适合逻辑回归模型等其他模型</td>
</tr>
</tbody>
</table>
<p>总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数$\theta $的替代方法。具体地说，只要特征变量数量小于一万，我通常使用标准方程法，而不使用梯度下降法。</p>
<p>随着我们要讲的学习算法越来越复杂，例如，当我们讲到分类算法，像逻辑回归算法，我们会看到，<br>实际上对于那些算法，并不能使用标准方程法。对于那些更复杂的学习算法，我们将不得不仍然使用梯度下降法。因此，梯度下降法是一个非常有用的算法，可以用在有大量特征变量的线性回归问题。或者我们以后在课程中，会讲到的一些其他的算法，因为标准方程法不适合或者不能用在它们上。但对于这个特定的线性回归模型，标准方程法是一个比梯度下降法更快的替代算法。所以，根据具体的问题，以及你的特征变量的数量，这两种算法都是值得学习的。</p>
<p>正规方程的python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">normalEqn</span><span class="params">(X, y)</span>:</span></span><br><span class="line"></span><br><span class="line">   theta = np.linalg.inv(X.T@X)@X.T@y <span class="comment">#X.T@X等价于X.T.dot(X)</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">return</span> theta</span><br></pre></td></tr></table></figure>
<h2 id="正规方程及不可逆性"><a href="#正规方程及不可逆性" class="headerlink" title="正规方程及不可逆性"></a>正规方程及不可逆性</h2><p>在这段视频中谈谈正规方程 ( normal equation )，以及它们的不可逆性。<br>由于这是一种较为深入的概念，并且总有人问我有关这方面的问题，因此，我想在这里来讨论它，由于概念较为深入，所以对这段可选材料大家放轻松吧，也许你可能会深入地探索下去，并且会觉得理解以后会非常有用。但即使你没有理解正规方程和线性回归的关系，也没有关系。</p>
<p>我们要讲的问题如下：$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$</p>
<p>有些同学曾经问过我，当计算 $\theta$=<code>inv(X&#39;X ) X&#39;y</code> ，那对于矩阵$X'X$的结果是不可逆的情况咋办呢?<br>如果你懂一点线性代数的知识，你或许会知道，有些矩阵可逆，而有些矩阵不可逆。我们称那些不可逆矩阵为<code>奇异</code>或<code>退化矩阵</code>。</p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210822.png" alt=""></p>
<p>问题的重点在于$X'X$的不可逆的问题很少发生，在Octave里，如果你用它来实现$\theta$的计算，你将会得到一个正常的解。在Octave里，有两个函数可以求解矩阵的逆，一个被称为<code>pinv()</code>，另一个是<code>inv()</code>，这两者之间的差异是些许计算过程上的，一个是所谓的<code>伪逆</code>，另一个被称为<code>逆</code>。使用<code>pinv()</code> 函数可以展现数学上的过程，这将计算出$\theta$的值，即便矩阵$X'X$是不可逆的。</p>
<p>在<code>pinv()</code> 和 <code>inv()</code> 之间，又有哪些具体区别呢 ?</p>
<p>其中<code>inv()</code> 引入了先进的数值计算的概念。</p>
<p>出现不可逆的一般有两个原因,第一个原因是:<strong>如果不知何故,再你的学习问题中,你有多余的功能</strong> 例如，在预测住房价格时，如果${x_{1}}$是以英尺为尺寸规格计算的房子，${x_{2}}$是以平方米为尺寸规格计算的房子，同时，你也知道1米等于3.28英尺 ( 四舍五入到两位小数 )，这样，你的这两个特征值将始终满足约束：${x_{1}}={x_{2}}*{{\left( 3.28 \right)}^{2}}$。<br>实际上，你可以用这样的一个线性方程，来展示那两个相关联的特征值，矩阵<code>X&#39;X</code>将是不可逆的。</p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/20181104210836.png" alt=""></p>
<p>第二个原因是，<strong>在你想用大量的特征值，尝试实践你的学习算法的时候，可能会导致矩阵$X'X$的结果是不可逆的</strong>。<br>具体地说，<strong>在$m$小于或等于n的时候</strong>，例如，有$m$等于10个的训练样本也有$n$等于100的特征数量。要找到适合的$(n +1)$ 维参数矢量$\theta$，这将会变成一个101维的矢量，尝试从10个训练样本中找到满足101个参数的值，这工作可能会让你花上一阵子时间，但这并不总是一个好主意。因为，正如我们所看到你只有10个样本，以适应这100或101个参数，数据还是有些少。</p>
<p>稍后我们将看到，如何使用小数据样本以得到这100或101个参数，通常，我们会使用一种叫做正则化的线性代数方法，通过删除某些特征或者是使用某些技术，来解决当$m$比$n$小的时候的问题。即使你有一个相对较小的训练集，也可使用很多的特征来找到很多合适的参数。</p>
<p>总之当你发现的矩阵$X'X$的结果是奇异矩阵，或者找到的其它矩阵是不可逆的，我会建议你这么做。</p>
<p>首先，看特征值里是否有一些多余的特征，像这些${x_{1}}$和${x_{2}}$是线性相关的，互为线性函数。同时，当有一些多余的特征时，可以删除这两个重复特征里的其中一个，无须两个特征同时保留，将解决不可逆性的问题。因此，首先应该通过观察所有特征检查是否有多余的特征，如果有多余的就删除掉，直到他们不再是多余的为止，如果特征数量实在太多，我会删除些 用较少的特征来反映尽可能多内容，否则我会考虑使用正规化方法。<br>如果矩阵$X'X$是不可逆的，（通常来说，不会出现这种情况），如果在Octave里，可以用伪逆函数<code>pinv()</code> 来实现。这种使用不同的线性代数库的方法被称为伪逆。即使$X'X$的结果是不可逆的，但算法执行的流程是正确的。总之，出现不可逆矩阵的情况极少发生，所以在大多数实现线性回归中，出现不可逆的问题不应该过多的关注${X^{T}}X$是不可逆的。</p>
<p><strong>增加内容：</strong></p>
$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$ 的推导过程：<br><br>$J\left( \theta  \right)=\frac{1}{2m}\sum\limits_{i=1}^{m}{{{\left( {h_{\theta}}\left( {x^{(i)}} \right)-{y^{(i)}} \right)}^{2}}}$
<p>其中：${h_{\theta}}\left( x \right)={\theta^{T}}X={\theta_{0}}{x_{0}}+{\theta_{1}}{x_{1}}+{\theta_{2}}{x_{2}}+...+{\theta_{n}}{x_{n}}$</p>
<p>将向量表达形式转为矩阵表达形式，则有$J(\theta )=\frac{1}{2}{{\left( X\theta -y\right)}^{2}}$ ，其中$X$为$m$行$n$列的矩阵（$m$为样本个数，$n$为特征个数），$\theta$为$n$行1列的矩阵，$y$为$m$行1列的矩阵，对$J(\theta )$进行如下变换</p>
$J(\theta )=\frac{1}{2}{{\left( X\theta -y\right)}^{T}}\left( X\theta -y \right)$
<p>​     $=\frac{1}{2}\left( {{\theta }^{T}}{{X}^{T}}-{{y}^{T}} \right)\left(X\theta -y \right)$</p>
<p>​     $=\frac{1}{2}\left( {{\theta }^{T}}{{X}^{T}}X\theta -{{\theta}^{T}}{{X}^{T}}y-{{y}^{T}}X\theta -{{y}^{T}}y \right)$</p>
<p>接下来对$J(\theta )$偏导，需要用到以下几个矩阵的求导法则:</p>
$\frac{dAB}{dB}={{A}^{T}}$
$\frac{d{{X}^{T}}AX}{dX}=2AX$
<p>所以有:</p>
$\frac{\partial J\left( \theta  \right)}{\partial \theta }=\frac{1}{2}\left(2{{X}^{T}}X\theta -{{X}^{T}}y -{{X}^{T}}y -0 \right)$
<p>​           $={{X}^{T}}X\theta -{{X}^{T}}y$</p>
<p>令$\frac{\partial J\left( \theta  \right)}{\partial \theta }=0$,</p>
<p>则有$\theta ={{\left( {X^{T}}X \right)}^{-1}}{X^{T}}y$</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.fengwenhua.top/2018/11/15/第三章-线性代数回顾-Linear-Algebra-Review/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="冯文华">
      <meta itemprop="description" content="记录日常学习与生活">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江南小虫虫的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/15/第三章-线性代数回顾-Linear-Algebra-Review/" class="post-title-link" itemprop="http://www.fengwenhua.top/index.html">第三章 线性代数回顾(Linear Algebra Review)</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-14 22:33:11 / 修改时间：14:41:50" itemprop="dateCreated datePublished" datetime="2018-11-14T22:33:11Z">2018-11-14</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/机器学习入门/" itemprop="url" rel="index"><span itemprop="name">机器学习入门</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/11/15/第三章-线性代数回顾-Linear-Algebra-Review/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/11/15/第三章-线性代数回顾-Linear-Algebra-Review/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/11/15/第三章-线性代数回顾-Linear-Algebra-Review/" class="leancloud_visitors" data-flag-title="第三章 线性代数回顾(Linear Algebra Review)">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数：</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">9.2k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">15 分钟</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h1 id="第三章-线性代数回顾-Linear-Algebra-Review"><a href="#第三章-线性代数回顾-Linear-Algebra-Review" class="headerlink" title="第三章 线性代数回顾(Linear Algebra Review)"></a>第三章 线性代数回顾(Linear Algebra Review)</h1><h2 id="Matrices-and-Vectors（矩阵和向量）"><a href="#Matrices-and-Vectors（矩阵和向量）" class="headerlink" title="Matrices and Vectors（矩阵和向量）"></a>Matrices and Vectors（矩阵和向量）</h2><blockquote>
<p>我们先复习一下线性代数的知识 在这段视频中 我会向大家介绍矩阵和向量的概念</p>
</blockquote>
<ul>
<li><code>矩阵</code>是指 <strong>由数字组成的矩形阵列 并写在方括号中间</strong> 例如 屏幕中所示的一个矩阵 先写一个左括号 然后是一些数字 这些数字可能是 机器学习问题的特征值 也可能表示其他意思 不过现在不用管具体的数字 然后我用右方括号将其括起来 这样就得到了一个矩阵 接下来 看一下其他矩阵的例子 依次写下1 2 3 4 5 6 因此实际上矩阵 可以说是二维数组的 另一个名字</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562400_1378345848_1539953128" alt="_1526562400_1378345848_1539953128_1526562400_1378345848.png"></p>
<ul>
<li>另外 我们还需要知道的是 <code>矩阵的维度</code>=<strong>矩阵的行数乘以列数</strong> 具体到这个例子 看左边 包括1 2 3 4共4行 以及2列 因此 这个例子是一个 4 × 2的矩阵 即行数乘以列数 4行乘2列</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562414_1674067162_1539953152" alt="_1526562414_1674067162_1539953152_1526562414_1674067162.png"></p>
<ul>
<li>右边的矩阵有两行 这是第一行 这是第二行 此外包括三列 这是第一列 第二列 第三列 因此 我们把 这个矩阵称为一个 2 × 3维的矩阵 所以我们说这个矩阵的维度是2 × 3维</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562425_1017138353_1539953170" alt="_1526562425_1017138353_1539953170_1526562425_1017138353.png"></p>
<ul>
<li>有时候大家会发现 书写有些不同 比如左边的矩阵 写成了<code>R4 × 2</code> 具体而言 大家会将该矩阵称作 是集合R4×2的元素 因此 也就是说 这个<strong>矩阵 R4×2代表所有4×2的矩阵的集合</strong> 而右边的这个矩阵 有时候也写作一个<code>R2×3</code>的矩阵 因此 如果你看到2×3 如果你看到 有些地方表达为 4×2的或者2×3的 <strong>一般都是指 一个特定维度的矩阵</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562469_2073201965_1539953199" alt="_1526562469_2073201965_1539953199_1526562469_2073201965.png"></p>
<ul>
<li>接下来 让我们来谈谈<strong>如何表达矩阵的某个特定元素</strong> 这里我说矩阵元素 而不是矩阵 我的意思是 矩阵的条目数 也就是<strong>矩阵内部的某个数</strong></li>
<li>所以 标准的表达是 <strong>如果A是 这个矩阵 那么A下标 ij 表示的是 i j对应的那个数字 意思是矩阵的第i行和第j列 对应的那个数</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562486_126334539_1539953234" alt="_1526562486_126334539_1539953234_1526562486_126334539.png"></p>
<ul>
<li>例如 <code>A11</code> 表示的是<code>第1行 第1列</code>所对应的那个元素 所以这是 第一行和第一列 因此<strong>A11 就等于 1402</strong> 另一个例子<code>A12</code> 表示的是<code>第一行第二列</code> 对应的那个数 所以<strong>A12 将等于191</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562514_858056936_1539953253" alt="_1526562514_858056936_1539953253_1526562514_858056936.png"></p>
<ul>
<li>我希望你不会犯下面的错误 但如果你这么写的话 如果你写出了<code>A43</code> 这应该表示的是 <code>第四行第三列</code> 而你知道 这个矩阵没有第三列 因此<strong>这是未定义的</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562526_1857626394_1539953574" alt="_1526562526_1857626394_1539953574_1526562526_1857626394.png"></p>
<ul>
<li>接下来让我们来谈谈<strong>什么是向量</strong> 一个<code>向量</code>是一种特殊的矩阵 <strong>向量是只有一列的矩阵</strong> 所以 你有一个 <code>n×1</code> 矩阵 还记得吗 <strong>N是行数 而这里的1 表示的是列数</strong> 所以 <strong>只有一列的矩阵 就是我们所说的向量</strong></li>
<li>因此 这里是一个向量的 例子 比如说 我有 n = 4 个元素</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562579_388138141_1539953600" alt="_1526562579_388138141_1539953600_1526562579_388138141.png"></p>
<ul>
<li>所以我们也把这个称为 另一个术语是 这是一个<strong>四维的向量</strong> 也就意味着 <strong>这是一个含有 4个元素的向量</strong> 而且 前面我们讲 矩阵的时候提到过 这个符号<code>R3×2</code> 表示的是<code>一个3行2列的矩阵</code> 而对于这个向量 我们也同样可以 表示为集合<code>R4</code> 因此 这个<strong>R4是指 一个四维向量的集合</strong></li>
<li>接下来让我们来谈谈<strong>如何引用向量的元素</strong> 我们将使用符号 <code>yi</code>来代表 <code>向量y的第i个元素</code> 所以 如果这个向量是y 那么<code>y下标i</code> 则表示<code>它的第i个元素</code> 所以y1表示第一个元素 460 y2表示第二个元素 232 这是第二个元素 还有y3等于 315 等等 只有y1至y4是有意义的 因为这定义的是一个四维向量</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562604_140050420_1539953644" alt="_1526562604_140050420_1539953644_1526562604_140050420.png"></p>
<ul>
<li>此外 事实上 有两种方法来表达 某个向量中某个索引 是这两种</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562615_528774851_1539953673" alt="_1526562615_528774851_1539953673_1526562615_528774851.png"></p>
<ul>
<li>有时候 人们会使用 <code>1-索引</code> 有时候用<code>0-索引</code> 因此 左边这个例子 是一个<code>1-索引向量</code> 它的元素写作<code>y1 y2 y3 y4</code> 而右边这个向量 是<code>0-索引</code>的一个例子 我们的索引 <code>从下标0开始</code> 因此 元素从<code>y0至y3</code></li>
<li><strong>对于很多机器学习的应用问题来说 0-索引向量为我们提供了一个更方便的符号表达</strong>  所以你通常应该 做的是 除非特别指定 你应该<strong>默认我们使用的是<code>1-索引法</code>表示向量</strong> 在本课程的后面所有 <strong>关于线性代数的视频中 我都将使用<code>1-索引法</code>表示向量</strong></li>
<li>按照惯例 通常在书写矩阵和向量时 大多数人会使用<code>大写字母</code> 来表示<code>矩阵</code> 因此 我们要<strong>使用 大写字母 如 A B C X 来表示矩阵</strong> 而通常我们会使用<code>小写字母</code> 像<code>a b x y</code> 来表示<code>数字 或是原始的数字 或标量 或向量</code> 这是实际的使用习惯 我们也经常看到 <strong>使用小写字母y 来表示向量 但我们平时 是用大写字母来表示矩阵</strong></li>
</ul>
<h3 id="Matrices-and-Vectors笔记"><a href="#Matrices-and-Vectors笔记" class="headerlink" title="Matrices and Vectors笔记"></a>Matrices and Vectors笔记</h3><ul>
<li>矩阵是二维数组</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562640_1052555579_1539953704" alt="_1526562640_1052555579_1539953704_1526562640_1052555579.png"></p>
<p>上面的矩阵有四行三列，所以它是一个4 x 3的矩阵</p>
<ul>
<li>向量是·一列和许多行·的矩阵：</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562653_1211280669_1539953713" alt="_1526562653_1211280669_1539953713_1526562653_1211280669.png"></p>
<p>所以向量是矩阵的一个子集。上述向量是一个4×1矩阵。</p>
<ul>
<li>表示法和条款：</li>
<li>$A_{ij}$引用矩阵A的第i行和第j列中的元素。</li>
<li>具有’n’行的矢量被称为’n’维矢量。</li>
<li>$v_i$指矢量的第i行中的元素。</li>
<li>一般来说，我们所有的向量和矩阵都是1索引的。请注意，对于某些编程语言，这些数组是0索引的。</li>
<li>矩阵通常用大写字母表示，而向量则用小写字母表示。</li>
<li>“标量”表示对象是单个值，而不是矢量或矩阵。</li>
<li><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563183_300594797_1539953734" alt="_1526563183_300594797_1539953734_1526563183_300594797.png">是指一组标量实数。</li>
<li><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563193_1988191010_1539953742" alt="_1526563193_1988191010_1539953742_1526563193_1988191010.png">是指实数的n维向量的集合。</li>
</ul>
<h2 id="Addition-and-Scalar-Multiplication"><a href="#Addition-and-Scalar-Multiplication" class="headerlink" title="Addition and Scalar Multiplication"></a>Addition and Scalar Multiplication</h2><blockquote>
<p>在这段视频中 我们要讲 我们将讨论矩阵的加法和减法运算 以及如何进行 数和矩阵的乘法 也就是标量乘法 让我们从下面这个例子开始</p>
</blockquote>
<ul>
<li>假设有这样两个矩阵 如果想对它们做<strong>求和运算</strong> 应该怎么做呢？ 或者说 矩阵的加法到底是如何进行的？ 答案是 如果你想将两个矩阵相加 你<strong>只需要将这两个矩阵的 每一个元素都逐个相加</strong> 因此 两个矩阵相加 所得到的结果 就是一个新的矩阵 它的第一个元素 是1和4相加的结果 因此我们得到5 接下来是第二个元素 用2和2相加 因此得到4 然后是3加0得到3 以此类推 这里我用不同颜色区别一下 接下来右边这一列元素 就是0.5 10和2</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563239_390601298_1539953757" alt="_1526563239_390601298_1539953757_1526563239_390601298.png"></p>
<ul>
<li>这里大家不难发现 <strong>只有相同维度的两个矩阵才能相加</strong> 对于这个例子而言 一個3 x 2的矩陣 也就是说矩阵的行数为3 列数是2 因此是3行2列 第二个矩阵 也是一个3行2列的矩阵 因此这两个矩阵相加的结果 也是一个3行2列的矩阵 所以你只能将相同维度的矩阵 进行相加运算 同时 所得到的结果 将会是一个新的矩阵 <strong>这个矩阵与相加的两个矩阵维度相同</strong></li>
<li>反过来 如果你想将这样两个矩阵相加 这是一个3行2列的矩阵 行数为3 列数为2 而这一个是2行2列的矩阵 那么由于这两个矩阵 维度是不相同的 这就出现错误了 所以我们不能将它们相加 也就是说 <strong>这两个矩阵的和是没有意义的</strong> 这就是矩阵的加法运算</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563246_1151303969_1539953785" alt="_1526563246_1151303969_1539953785_1526563246_1151303969.png"></p>
<ul>
<li>接下来 我们讨论<strong>矩阵和标量的乘法运算</strong> 这里所说的<code>标量</code> 可能是一个复杂的结构 或者只是一个简单的数字 或者说实数 <strong>标量在这里指的就是实数</strong> 如果我们用数字3来和这个矩阵相乘 那么结果是显而易见的 你只需要将矩阵中的所有元素 都和3相乘 每一个都逐一与3相乘 因此 1和3相乘 结果是3 2和3相乘 结果是6 最后3乘以3得9 我再换一下颜色 0乘以3得0 3乘以5得15 最后3乘以1得3 这样得到的这个矩阵 就是左边这个矩阵和3相乘的结果 我们再次注意到 这是一个3行2列的矩阵 得到的结果矩阵 维度也是相同的 也就是说这两个矩阵 都是3行2列 这也是3行2列</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563253_1190942378_1539953799" alt="_1526563253_1190942378_1539953799_1526563253_1190942378.png"></p>
<ul>
<li>另外 你也可以写成另一种方式 这里是3和这个矩阵相乘 你也可以把这个矩阵写在前面 把左边这个矩阵照抄过来 我们也可以用这个矩阵乘以3 也就是说 3乘以这个矩阵 和这个矩阵乘以3 结果都是一回事 都是中间的这个矩阵</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563264_1347157564_1539953811" alt="_1526563264_1347157564_1539953811_1526563264_1347157564.png"></p>
<ul>
<li>你也可以用矩阵除以一个数 那么 我们可以看到 用这个矩阵除以4 实际上就是 用四分之一 来和这个矩阵相乘 4 0 6 3 不难发现 相乘的结果是 1/4和4相乘为1 1/4和0相乘得0 1/4乘以6 结果是3/2 6/4也就是3/2 最后1/4乘以3得3/4 这样我们就得到了 这个矩阵除以4的结果 结果就是是右边这个矩阵</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563275_475609234_1539953825" alt="_1526563275_475609234_1539953825_1526563275_475609234.png"></p>
<ul>
<li>最后 我们来看一个稍微复杂一点的例子 我们可以把所有这些运算结合起来 在这个运算中 需要用3来乘以这个向量 然后加上一个向量 再减去另一个向量除以3的结果 让我们先来整理一下这几项运算 首先第一个运算 很明显这是标量乘法的例子 因为这里是用3来乘以一个矩阵 然后这一项 很显然这是另一个标量乘法 或者可以叫标量除法 其实也就是1/3乘以这个矩阵 因此 如果我们先考虑这两项运算</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563285_884588442_1539953837" alt="_1526563285_884588442_1539953837_1526563285_884588442.png"></p>
<ul>
<li>那么我们将得到的是 我们看一下 3乘以这个矩阵 结果是3 12 6 然后和中间的矩阵相加 也就是0 0 5 最后再减去1 0 2/3 同样地 为了便于理解 我们再来梳理一下这几项 这里的这个加号 表明这是一个矩阵加法 对吧？ 当然这里是向量 别忘了 向量是特殊的矩阵 对吧？ 或者你也可以称之为 向量加法运算 同样 这里的减号表明 这是一个矩阵减法运算 但由于这是一个n行1列的矩阵 实际上是3行1列 因此这个矩阵 实际上是也一个向量 一个列向量 因此也可以把它称作<strong>向量的减法运算</strong> 明白了吗</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563297_1736936881_1539953857" alt="_1526563297_1736936881_1539953857_1526563297_1736936881.png"></p>
<ul>
<li>最后再整理一下 最终的结果依然是一个向量 向量的第一个元素 是3+0-1 就是3-1 也就是2 第二个元素是12+0-0 也就是12 最后第三个元素 6+5-(2/3) 也就是11-(2/3) 结果是10又三分之一 关闭右括号 我们得到了最终的结果 这是一个3行1列的矩阵 或者也可以说是 一个维度为3的向量 这就是这个运算式的计算结果</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563308_1386235624_1539953868" alt="_1526563308_1386235624_1539953868_1526563308_1386235624.png"></p>
<ul>
<li>所以 你学会了矩阵或向量的加减运算 以及矩阵或向量跟标量 或者说实数 的乘法运算 到目前为止 我只介绍了如何进行 矩阵或向量与数的乘法运算 在下一讲中 我们将讨论一个更有趣的话题 那就是如何进行 两个矩阵的乘法运算<h3 id="加法和标量乘法笔记"><a href="#加法和标量乘法笔记" class="headerlink" title="加法和标量乘法笔记"></a>加法和标量乘法笔记</h3></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563322_945561965_1539953878" alt="_1526563322_945561965_1539953878_1526563322_945561965.png"></p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563328_710017541_1539953883" alt="_1526563328_710017541_1539953883_1526563328_710017541.png"></p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563335_879312404_1539953887" alt="_1526563335_879312404_1539953887_1526563335_879312404.png"></p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563343_57921380_1539953892" alt="_1526563343_57921380_1539953892_1526563343_57921380.png"></p>
<h2 id="Matrix-Vector-Multiplication"><a href="#Matrix-Vector-Multiplication" class="headerlink" title="Matrix Vector Multiplication"></a>Matrix Vector Multiplication</h2><blockquote>
<p>在本节课的视频中 讨论<strong>如何 将两个矩阵相乘</strong> 我们将从矩阵相乘的 特例 向量相乘开始 即 一个矩阵与一个向量相乘</p>
</blockquote>
<ul>
<li>让我们从一个例子开始 左边是一个矩阵 右边是一个向量 假如我们 将这个矩阵 与这个向量相乘 结果会怎样呢？ 我先快速计算出结果 然后我们再 退回去 查看每一个步骤 很明显 相乘的结果 将是 一个向量 我先将这部分完成 然后再来解释 我刚刚是怎么做的</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563414_2041992383_1539953906" alt="_1526563414_2041992383_1539953906_1526563414_2041992383.png"></p>
<ul>
<li>要计算出结果向量的第一个元素 我将会 <strong>取这两个数字 并把他们 把矩阵 A 的 然后把对应相乘的结果加起来</strong> 取1乘以1 同时取3 乘以 5 计算得到1和15 相加得16 我将在这儿写上16 要计算第二行 的第二个元素 我需要将第二行 与这个向量相乘 所以我得到 4乘以1 加上0乘以5 结果等于4 因此在这里写上4 对于最后一个元素 我需要计算(2, 1) 乘以 (1, 5) 所以先计算2乘以1 再加上 1乘以5 最后结果为7 所以我在这儿写上7</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563424_2094856845_1539953936" alt="_1526563424_2094856845_1539953936_1526563424_2094856845.png"></p>
<ul>
<li>事实证明 3x2的矩阵 和一个2x1的矩阵 即一个二维向量 相乘的结果 我们得到的 将是一个3×1 的矩阵 这个3×1的矩阵 就是这么得来的 也就是一个三维向量</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563438_1477954396_1539953945" alt="_1526563438_1477954396_1539953945_1526563438_1477954396.png"></p>
<ul>
<li>我想 我可能计算时做得很快 你们并不一定能够 自己重复这个过程 下面让我们更加仔细的看一下 刚刚我做了些什么以及一个向量 和一个矩阵相乘的计算过程是怎样的 下面详细介绍了如何 计算一个矩阵与一个向量相乘 假设这是一个矩阵A 我希望将它乘以 一个向量x 结果记为 向量y 所以 矩阵A是一个 m×n维矩阵 有m行和n列 我们让它与一个 n×1的矩阵相乘 换言之 一个n维向量 明显地 这里的两个n是相等的 也就是说 <strong>这个矩阵的列数 有n列 必须要与 另一个相乘矩阵的行数相同</strong> 即必须匹配这个向量的维数。 这样相乘的结果 将会是一个n维 向量y m 将与 矩阵A的行数 相同</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563449_1514757581_1539953956" alt="_1526563449_1514757581_1539953956_1526563449_1514757581.png"></p>
<ul>
<li>那么如何计算这个向量y呢？ 事实上 计算y的过程可以分解为 计算 yi 的值 让 A 的第 i 行元素 分别乘以向量 x 中的元素 并且相加 就是这样子 为了得到 y 的第一个元素 无论是多少 我们将会 把矩阵 A 的 第一行元素 每次同一个向量 x 的元素 相乘 我取第一个数 与第一个数相乘 然后取第二个数同第二个数相乘 取第三个数 与第三个数相乘 直到全部乘完 最后 将这些相乘的结果 加起来 这样我们就得到了 y 的第一个元素</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563458_960937756_1539954008" alt="_1526563458_960937756_1539954008_1526563458_960937756.png"></p>
<ul>
<li>然后我们 来计算 y 的第二个元素 接下来我们 取A的第二行 然后重复整个过程 现在 我们取A的第二行 将它 与其他元素相乘 也就是 x 的元素 将结果相加 这样我们就得到了 y 的第二个元素 依次计算下去 我们取A得第三行 逐行地与 向量x相乘 将结果加起来 然后得到第三个元素 以此类推 直到最后一行</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563471_8553957_1539954029" alt="_1526563471_8553957_1539954029_1526563471_8553957.png"></p>
<ul>
<li>所以 上述就是具体步骤 让我们再举一个例子 在这个例子中 我们先看一下矩阵的维度 左边是一个 3×4矩阵 右边是一个四维向量 也就是4×1矩阵 所以这样相乘的结果 将是 一个三维向量 我们在写的时候要给这个向量 留三个元素的空间</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563479_2071501370_1539954037" alt="_1526563479_2071501370_1539954037_1526563479_2071501370.png"></p>
<ul>
<li>现在让我们一起来算一下 首先是第一个元素 我将会取这四个数 并将它们与向量x相乘 所以我需要计算 1×1 加上2×3 加1×2 加5×1 等于 1 +6 再加上2 +6 也就是14 而对边距来说 第二个元素 我要 取这一行 然后与向量 (0×1)+3相乘 我们将得到 0×1 + 3×3 0×2 + 4×1 等于 9 + 4 也就是13 最后 对最后一个元素 我将取最后一行 所以我得到了-1×1 -2×3 加上0×2 加上0×1 所以 我们将得到-1和-6 相加得 -7 明白? 所以我最后的答案是 一个向量 其中的元素为 14 我将不给这些字涂上颜色 13 -7</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563488_238534241_1539954070" alt="_1526563488_238534241_1539954070_1526563488_238534241.png"></p>
<ul>
<li>如前面说的 计算结果是一个3×1的矩阵 上述就是矩阵和向量相乘的方法 我知道 这张幻灯片上内容很多 如果你在看的过程中 不是很确定这些数字怎么来的 你可以随时暂停视频 慢慢地 仔细琢磨 整个计算过程 尽量 确保自己理解了 得到14 13 11 这些结果的每一个步骤</li>
<li>最后 我将教你们一个<code>小技巧</code> 假设我 有四间房子 这些房子有四种大小 我有一个 假设函数 用于预测房子的价格 我需要计算 四间房子的大小作为 h(x) 的大小即预测的房价 这里有一种简单的方法 <strong>可以同时计算四间房子的预测价格</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563507_1294767834_1539954092" alt="_1526563507_1294767834_1539954092_1526563507_1294767834.png"></p>
<ul>
<li>我可以将它简单地 利用 矩阵向量相乘的思想来计算 所以 对于这个问题我会这么计算 首先我要构建一个 如下所示的矩阵 元素是1 1 1 1 然后我把四个房子的大小 写在这儿 我还需要构造一个向量 我的向量 它将是一个 二维向量 即 40 和 0.25 这是预测函数的两个系数 θ0 和 θ1 接下来</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563536_1519908066_1539954138" alt="_1526563536_1519908066_1539954138_1526563536_1519908066.png"></p>
<ul>
<li>我要做的就是 将我构造好的矩阵和向量相乘 这是相乘符号 我将得到什么结果呢？ 左边是一个 4×2 矩阵 右边是一个 2×1 矩阵 所以结果 将是一个4×1向量 对吧 所以 让我在幻灯片上写上 结果将是 一个4×1的矩阵 输出结果也就是 一个四维向量 让我来把它写出来 用四个实数表示我的四个元素 事实上 结果的第一个元素 我的计算方式 是 将这一行同我的向量相乘 结果将是 -40×1 + 4.25×2104 顺便说一下 在先前的幻灯片中 我写的是 1× (-40) + 2104 × 0.25 但是顺序无关紧要 对吧？ -40×1 和 1×(-40)是一样的 这第一个元素 就是当x为2104时的 h 值 因此 这是我的第一个房子的预测价格 那么 第二个元素呢？ 你应该已经想到了 我要怎么计算第二个元素了 对吧? 我要把这个乘以我的向量 所以就是 -40×1 + 0.25×1416 这就是x为1416的 h 对吧? 这是第三个 和第四个 后面就依次计算这个4×1矩阵的第三和第四个元素 得出结果 这里 我画了绿色边框的部分 是一个实数 对吧？ 它是一个实数 这里 我画了洋红色边框的部分 紫色 洋红色 边框 是一个实数 对吧？ 所以右边 最右边 就是一个 4×1矩阵 是一个4维向量</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563587_1184137581_1539954167" alt="_1526563587_1184137581_1539954167_1526563587_1184137581.png"></p>
<ul>
<li>这个例子的一个小技巧是 当你 在程序中实现这个过程的时候 当你有四间房子 你想使用自己的预测函数 来预测房子的价格 完成这些工作 你可以用一行代码搞定 我们后面会谈到Octave 以及编程语言 你可以只写一行代码就完成整个过程 你可以这样写 <code>prediction = DataMatrix × Parameters</code> 对吧 数据矩阵是这一部分 参数 是这一部分 这就是一个<code>矩阵向量乘法</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563599_1602442101_1539954208" alt="_1526563599_1602442101_1539954208_1526563599_1602442101.png"></p>
<ul>
<li>如果你这么做了 这个变量prediction 抱歉 我的字写得很潦草 只需要实现 这一行代码 如果你有一个做矩阵向量相乘的函数库的话 如果你这样做的话 右侧的prediction变量就会形成 一个4维向量 给你所有的预测价格 另一种计算方式是 作为一种矩阵向量相乘的方式 实际上就是一种 通过for循环 for 1 to 4 对吧？ 如果说你有一千间房子 就将是 for 1 to 1000 或者别的任何数 然后如果i等于的话 你必须写一个 有一个假设条件 I相等 然后需要做 比矩阵向量相乘多得多的工作 当你有 大量的房子的时候 如果你试图预测 不只是四座 或许是一千座房子的时候 事实证明 当你使用矩阵向量相乘的方法时 在计算机中 使用任何语言 不仅仅是Octave 还有C++ Java Python 等高级语言 以及其他语言 都可以很快的实现 事实证明 像左边这样子写代码 不仅可以 简化你的代码 现在你只需要 写一行代码 而不是一堆代码 而且 还有一个微妙的好处 我们后面将会了解到 就是基于你所有的房子 这样做计算效率将会更高 比你像右边那样 用代码实现公式 的方式效率 将会高很多 我后面在讨论向量化的时候 会详细地 讨论这个问题 所以 通过这种方式计算预测值 不仅代码更加简洁 而且效率更高 基本用不上 我们在后面 在其他模型中 计算实例的回归的时候 将会有效地利用到这一讲的内容 在接下来的视频中 我将会从特殊到一般 讲讲矩阵与矩阵相乘的情况</li>
</ul>
<h3 id="矩阵向量乘法笔记"><a href="#矩阵向量乘法笔记" class="headerlink" title="矩阵向量乘法笔记"></a>矩阵向量乘法笔记</h3><ul>
<li>矩阵和向量的乘法如图：m×n 的矩阵乘以 n×1 的向量，得到的是 m×1 的向量</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563623_126184414_1539954234" alt="_1526563623_126184414_1539954234_1526563623_126184414.png"></p>
<ul>
<li>举例</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563631_819280345_1539954242" alt="_1526563631_819280345_1539954242_1526563631_819280345.png"></p>
<h2 id="Matrix-Matrix"><a href="#Matrix-Matrix" class="headerlink" title="Matrix Matrix"></a>Matrix Matrix</h2><blockquote>
<p>在这段视频中我们将会讨论 矩阵 <strong>矩阵的乘法以及 如何将两个矩阵相乘</strong> 我们会使用这样一种方法 在线性回归中用以解决 参数计算的问题 <strong>这种方法会把θ0、θ1等参数都放在一起来计算</strong> 也就是说 <strong>我们不需要一个迭代的梯度下降算法</strong></p>
</blockquote>
<ul>
<li>当我们谈到这个算法的时候 就会发现矩阵以及矩阵间的乘法运算 是你必须理解的关键步骤之一 所以让我们像往常那样 从一个例子开始 比方说 我有两个矩阵 我想将它们相乘 让我先只是按照这个例子做一遍（乘法） 然后告诉你这其中运算的细节</li>
<li>那么 我要做的第一件事是 我先把 右边这个矩阵的第一列 提取出来 然后我将会把 左边的这个矩阵和 之前取出来的这一列（前面提过的，向量）相乘 这只是第一列 是吧？ 然后我们可以看到 如果我 这么做 我就会得到向量（11,9） 所以这是与上个视频的矩阵 和向量的乘法是一样的 我已经提前算出了这个结果 是（11,9） 那么 之后的第二件事 我要做的就是 我将把第二列再单独提出出来 右边这个矩阵的第二列 然后我将要把它和 左边这个矩阵相乘 是的吧 所以 这就是那个矩阵 用右边的第二列 来乘以这个矩阵 因此 同样的 这是一个矩阵和 向量的乘法运算 这 就是你从上一个视频所学到的 如果你这么做 把这个矩阵和这个向量相乘 你会得到 （10,14）这个结果 顺便说一下 如果你想练习 矩阵和向量的乘法运算 那么就先暂停下视频 自己算一算结果对不对 好吧 现在我仅仅需要 将得到的这两个结果放在一起 那么这就是我的答案了 那么 我们可以看到 计算结果是 一个2 x 2的矩阵 我用来填充这个矩阵的方法 就是 把我的（11,9） 填在这里 把（10，14）填在 第二列 是的吧？ 所以 这就是如何 将两个矩阵相乘的 详细方法与过程 每次你只需要看 第二个矩阵的一列 然后把你的答案拼凑起来</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563661_226179955_1539954253" alt="_1526563661_226179955_1539954253_1526563661_226179955.png"></p>
<ul>
<li>再次强调下 我们将一步步的来计算 几秒中的时间里需要非常仔细 但我也要指出 我也要指出的是 第一个例子是一个2X3矩阵 乘以一个 3x2的矩阵 他们相乘 得到的结果 是一个2x2的 矩阵 我们将很快知道为什么是这个结果 好的 这是计算的技巧 让我们再看看 这其中的细节 看看究竟发生了什么</li>
<li>下面就是详细的过程 我有一个矩阵A 我要把它乘以 矩阵B 其结果 会是一个新的矩阵C 并且你会发现你只能 相乘那些维度 匹配的矩阵 因此如果A是一个<code>m×n</code>的矩阵 就是说m行n列 我将要用它与 一个<code>n×o</code>的矩阵相乘 并且实际上这里的n 必须匹配这里的这个n 所以<strong>第一个矩阵的列的数目 必须等于第二矩阵中的行的数目</strong> 并且相乘得到的结果 结果会是一个<code>m×o</code>的矩阵 就像这个矩阵C这样</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563670_496687926_1539954288" alt="_1526563670_496687926_1539954288_1526563670_496687926.png"></p>
<ul>
<li>并且 在前面的视频中 我们所做的一切都符合这个规则 这是一种当矩阵B的o值 等于1的特殊情况（指的是矩阵和向量相乘） 明白了吗？ 这是在B是一个向量的情况下 但是现在 我们要处理 O的值大于1的情况</li>
<li>所以 这里就是你怎样 把两个矩阵相乘 为了得到结果 我要做的就是 我将要取 B矩阵的第一列 把取出的这列看成一个向量 并乘以矩阵A 用B矩阵的第一列 这个计算结果将是 m×1的矩阵（也就是一个向量） 我们把结果先放在这里</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563690_1096939048_1539954295" alt="_1526563690_1096939048_1539954295_1526563690_1096939048.png"></p>
<ul>
<li>然后 我将要取 B矩阵的 第二列 那么我会又得到一个n×1的向量 也就是 这里的这一列 这是正确的 n×1的矩阵 也就是n维的向量 我将要把这个矩阵 和这些n乘1的向量相乘 其结果将是 一个m维的向量 然后我会把结果先放在那里 依此类推 对吧？</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563701_1971649980_1539954309" alt="_1526563701_1971649980_1539954309_1526563701_1971649980.png"></p>
<ul>
<li>那么 你知道的 我开始取第三列 把它和这个矩阵相乘 我又得到了一个M维向量 依此类推 直到你计算到了 最后一列 矩阵乘以 你取到的最后一列 就是C的最后一列 再说一遍 矩阵C的第i列 是根据把 矩阵A与 矩阵B的第i列 相乘得到的 结果 依次相加 从1,2到o依次相加的 对吧？</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563712_1758839025_1539954330" alt="_1526563712_1758839025_1539954330_1526563712_1758839025.png"></p>
<ul>
<li>那么 我们在这里做一个总结 我们总结了我们为了 计算矩阵C所做的步骤 让我们再看一个例子 比方说我想把这两个矩阵相乘</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563727_676793122_1539954340" alt="_1526563727_676793122_1539954340_1526563727_676793122.png"></p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563734_828514080_1539954344" alt="_1526563734_828514080_1539954344_1526563734_828514080.png"></p>
<h3 id="矩阵乘法笔记"><a href="#矩阵乘法笔记" class="headerlink" title="矩阵乘法笔记"></a>矩阵乘法笔记</h3><p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563746_251656436_1539954349" alt="_1526563746_251656436_1539954349_1526563746_251656436.png"></p>
<h2 id="Matrix-Multiplication-Properties"><a href="#Matrix-Multiplication-Properties" class="headerlink" title="Matrix Multiplication Properties"></a>Matrix Multiplication Properties</h2><p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563766_2059918306_1539954356" alt="_1526563766_2059918306_1539954356_1526563766_2059918306.png"></p>
<h3 id="矩阵乘法的性质笔记"><a href="#矩阵乘法的性质笔记" class="headerlink" title="矩阵乘法的性质笔记"></a>矩阵乘法的性质笔记</h3><p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563800_776821059_1539954362" alt="_1526563800_776821059_1539954362_1526563800_776821059.png"></p>
<h2 id="Inverse-adn-Transpose"><a href="#Inverse-adn-Transpose" class="headerlink" title="Inverse adn Transpose"></a>Inverse adn Transpose</h2><ul>
<li>只有方阵才有<code>逆阵</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563816_1312285084_1539954375" alt="_1526563816_1312285084_1539954375_1526563816_1312285084.png"></p>
<ul>
<li>Octave求逆阵</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563829_456329215_1539954391" alt="_1526563829_456329215_1539954391_1526563829_456329215.png"></p>
<h3 id="矩阵的逆和转置笔记"><a href="#矩阵的逆和转置笔记" class="headerlink" title="矩阵的逆和转置笔记"></a>矩阵的逆和转置笔记</h3><p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526563839_527465345_1539954398" alt="_1526563839_527465345_1539954398_1526563839_527465345.png"></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.fengwenhua.top/2018/11/15/第二章-单变量线性回归-Linear-Regression-with-One-Variable/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="冯文华">
      <meta itemprop="description" content="记录日常学习与生活">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江南小虫虫的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/15/第二章-单变量线性回归-Linear-Regression-with-One-Variable/" class="post-title-link" itemprop="http://www.fengwenhua.top/index.html">第二章 单变量线性回归(Linear Regression with One Variable)</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-14 20:22:59 / 修改时间：12:24:03" itemprop="dateCreated datePublished" datetime="2018-11-14T20:22:59Z">2018-11-14</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/机器学习入门/" itemprop="url" rel="index"><span itemprop="name">机器学习入门</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/11/15/第二章-单变量线性回归-Linear-Regression-with-One-Variable/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/11/15/第二章-单变量线性回归-Linear-Regression-with-One-Variable/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/11/15/第二章-单变量线性回归-Linear-Regression-with-One-Variable/" class="leancloud_visitors" data-flag-title="第二章 单变量线性回归(Linear Regression with One Variable)">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数：</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">8.7k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">15 分钟</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h1 id="第二章-单变量线性回归-Linear-Regression-with-One-Variable"><a href="#第二章-单变量线性回归-Linear-Regression-with-One-Variable" class="headerlink" title="第二章: 单变量线性回归(Linear Regression with One Variable)"></a>第二章: 单变量线性回归(Linear Regression with One Variable)</h1><h2 id="Gradient-Descennt"><a href="#Gradient-Descennt" class="headerlink" title="Gradient Descennt"></a>Gradient Descennt</h2><blockquote>
<p>我们已经定义了<code>代价函数J</code> 而在这段视频中 我想向你们介绍<code>梯度下降</code>这种算法 <strong>这种算法可以将代价函数J最小化</strong> 梯度下降是很常用的算法 它不仅被用在线性回归上 它实际上被广泛的应用于机器学习领域中的众多领域 在后面课程中 为了解决其他线性回归问题 我们&amp;&amp;<strong>也将使用梯度下降法 最小化其他函数</strong> 而不仅仅是只用在本节课的代价函数J 因此在这个视频中 我将讲解用梯度下降算法最小化函数 J 在后面的视频中 我们还会将此算法应用于具体的 代价函数J中来解决线性回归问题 下面是问题概述</p>
</blockquote>
<ul>
<li>在这里 我们有一个函数$J(\theta_0, \theta_1)$ 也许这是一个线性回归的代价函数 也许是一些其他函数 要使其最小化 我们需要用一个算法 来最小化函数$J(\theta_0, \theta_1)$ 就像刚才说的 事实证明 梯度下降算法可应用于 多种多样的函数求解 所以想象一下如果你有一个函数 J(θ0, θ1, θ2, …,θn ) 你希望可以通过最小化 θ0到θn 来最小化此代价函数J(θ0 到θn) 用n个θ是为了证明梯度下降算法可以解决更一般的问题 但为了简洁起见 为了简化符号 在接下来的视频中 我只用两个参数</li>
<li>下面就是关于梯度下降的构想 我们要做的是 我们<strong>要开始对θ0和θ1 进行一些初步猜测</strong> 它们到底是什么其实并不重要 但<strong>通常的选择是将 θ0设为0 将θ1也设为0 将它们都初始化为0</strong> 我们在梯度下降算法中要做的 就是<strong>不停地一点点地改变 θ0和θ1 试图通过这种改变使得J(θ0, θ1)变小 直到我们找到 J 的最小值 或许是局部最小值</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526560718_677839576_1539945526" alt="_1526560718_677839576_1539945526_1526560718_677839576.png"></p>
<ul>
<li>让我们通过一些图片来看看梯度下降法是如何工作的 我在试图让这个函数值最小 注意坐标轴 θ0和θ1在水平轴上 而函数 J在垂直坐标轴上 图形表面高度则是 J的值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526560743_1565219096_1539945560" alt="_1526560743_1565219096_1539945560_1526560743_1565219096.png"></p>
<ul>
<li>我们希望最小化这个函数 所以我们从 θ0和θ1的某个值出发 所以想象一下 <strong>对 θ0和θ1赋以某个初值 也就是对应于从这个函数表面上的某个起始点出发</strong> 对吧 所以<strong>不管 θ0和θ1的取值是多少 我将它们初始化为0 但有时你也可把它初始化为其他值</strong> 现在我希望大家把这个图像想象为一座山 想像类似这样的景色 公园中有两座山 想象一下你正站立在山的这一点上 站立在你想象的公园这座红色山上 在梯度下降算法中 我们<strong>要做的就是旋转360度 看看我们的周围 并问自己 我要在某个方向上 用小碎步尽快下山 这些小碎步需要朝什么方向?</strong></li>
<li>如果我们站在山坡上的这一点 你看一下周围 你会发现<strong>最佳的下山方向</strong> 大约是那个方向 好的 现在你在山上的新起点上</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526560798_666068590_1539945598" alt="_1526560798_666068590_1539945598_1526560798_666068590.png"></p>
<ul>
<li>你再看看周围 然后再一次想想 我应该从什么方向迈着小碎步下山? 然后你按照自己的判断又迈出一步 往那个方向走了一步</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526560828_2138414653_1539945611" alt="_1526560828_2138414653_1539945611_1526560828_2138414653.png"></p>
<ul>
<li><strong>然后重复上面的步骤</strong> 从这个新的点 你环顾四周 并<strong>决定从什么方向将会最快下山</strong> 然后又迈进了一小步 又是一小步 并依此类推 直到你接近这里 <strong>直到局部最低点的位置</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526560836_1542066151_1539945624" alt="_1526560836_1542066151_1539945624_1526560836_1542066151.png"></p>
<ul>
<li>此外 这种下降有一个有趣的特点 第一次我们是从这个点开始进行梯度下降算法的 是吧 在这一点上从这里开始 现在想象一下 我们在刚才的右边一些的位置 对梯度下降进行初始化 想象我们在右边高一些的这个点 开始使用梯度下降 如果你重复上述步骤 停留在该点 并环顾四周 往下降最快的方向迈出一小步 然后环顾四周 又迈出一步 然后如此往复 如果你从右边不远处开始 梯度下降算法将会带你来到 这个右边的第二个局部最优处</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526560853_1272028194_1539945641" alt="_1526560853_1272028194_1539945641_1526560853_1272028194.png"></p>
<ul>
<li>如果从刚才的第一个点出发 你会得到这个局部最优解 但如果你的起始点偏移了一些 起始点的位置略有不同 你<strong>会得到一个 非常不同的局部最优解 这就是梯度下降算法的一个特点</strong> 我们会在之后继续探讨这个问题</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526560865_949036406_1539945662" alt="_1526560865_949036406_1539945662_1526560865_949036406.png"></p>
<ul>
<li>好的 这是我们从图中得到的直观感受 看看这个图 这是梯度下降算法的定义 我们<strong>将会反复做这些 直到收敛</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526560876_637224678_1539945680" alt="_1526560876_637224678_1539945680_1526560876_637224678.png"></p>
<ul>
<li>我们要<strong>更新参数 θj 方法是 用 θj 减去 α乘以这一部分</strong><ul>
<li><code>:= 表示赋值</code> 这是一个赋值运算符</li>
<li><code>等号 =</code> :写出<code>a=b</code> 那么这是一个判断为真的<strong>声明</strong> 如果我<strong>写 a=b 就是在断言 a的值是等于 b的值的</strong> 这是声明 声明 a的值 与b的值相同</li>
<li><code>α</code> :一个数字 被称为<code>学习速率</code><ul>
<li>什么是α呢? 在梯度下降算法中 它控制了 <strong>我们下山时会迈出多大的步子</strong> 因此如果 α值很大 那么相应的梯度下降过程中 我们会试图用大步子下山 如果α值很小 那么我们会迈着很小的小碎步下山 关于如何设置 α的值等内容 在之后的课程中 我会回到这里并且详细说明</li>
</ul>
</li>
<li>最后 是公式的这一部分<img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561001_1205108635_1539945758" alt="_1526561001_1205108635_1539945758_1526561001_1205108635.png">这是一个<strong>微分项</strong> 我现在不想谈论它 但我会推导出这个微分项 并告诉你到底这要如何计算 你们中有人大概比较熟悉微积分 但即使你不熟悉微积分 也不用担心 我会告诉你 对这一项 你最后需要做什么</li>
</ul>
</li>
<li>现在 在梯度下降算法中 还有一个更微妙的问题 在梯度下降中 我们要更新 θ0和θ1 当 j=0 和 j=1 时 会产生更新 所以你将更新 J θ0还有θ1 实现梯度下降算法的微妙之处是 在这个表达式中 如果你要更新这个等式 你<strong>需要同时更新 θ0和θ1</strong> 我的意思是在这个等式中 我们要这样更新 <code>θ0:=θ0 -</code> 一些东西 并更新 <code>θ1:=θ1 -</code> 一些东西 实现方法是 <strong>你应该计算公式右边的部分 通过那一部分计算出θ0和θ1的值 然后同时更新 θ0和θ1</strong> 让我进一步阐述这个过程</li>
<li>在梯度下降算法中 这是正确实现同时更新的方法 我要设 temp0等于这些 设temp1等于那些 所以首先计算出公式右边这一部分 然后将计算出的结果 一起存入 temp0和 temp1 之中 然后同时更新 θ0和θ1 因为这才是正确的实现方法</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561065_1055239768_1539945798" alt="_1526561065_1055239768_1539945798_1526561065_1055239768.png"></p>
<ul>
<li>与此相反 下面是不正确的实现方法 因为它没有做到同步更新 在这种不正确的实现方法中 我们计算 temp0 然后我们更新θ0 然后我们计算 temp1 然后我们将 temp1 赋给θ1 右边的方法和左边的区别是 让我们看这里 就是这一步 如果这个时候你已经更新了θ0 那么你会使用 θ0的新的值来计算这个微分项 所以由于你已经在这个公式中使用了新的 θ0的值 那么这会产生一个与左边不同的 temp1的值 所以右边并不是正确地实现梯度下降的做法</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561083_1129145205_1539945818" alt="_1526561083_1129145205_1539945818_1526561083_1129145205.png"></p>
<ul>
<li>我不打算解释为什么你需要同时更新 <strong>同时更新是梯度下降中的一种常用方法</strong> 我们之后会讲到 <strong>实际上同步更新是更自然的实现方法</strong> 当<strong>人们谈到梯度下降时 他们的意思就是同步更新</strong> 如果用非同步更新去实现算法 代码可能也会正确工作 但是右边的方法并不是人们所指的那个梯度下降算法 而是具有不同性质的其他算法 由于各种原因 这其中会表现出微小的差别 你应该做的是 在梯度下降中真正实现同时更新 这些就是梯度下降算法的梗概</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561100_393689345_1539945882" alt="_1526561100_393689345_1539945882_1526561100_393689345.png"></p>
<ul>
<li>在接下来的视频中 我们要进入这个微分项的细节之中 我已经写了出来但没有真正定义 如果你已经修过微积分课程 如果你熟悉偏导数和导数 这其实就是这个微分项 如果你不熟悉微积分 不用担心 即使你之前没有看过微积分 或者没有接触过偏导数 在接下来的视频中 你会得到一切你需要知道的 如何计算这个微分项的知识 下一个视频中 希望我们能够给出 实现梯度下降算法的所有知识</li>
</ul>
<h3 id="Gradient-Descennt笔记"><a href="#Gradient-Descennt笔记" class="headerlink" title="Gradient Descennt笔记"></a>Gradient Descennt笔记</h3><ul>
<li><p>所以我们有我们的<strong>假设函数</strong>，并且我们有一种测量它适合数据的方式。现在我们需要估计<strong>假设函数</strong>中的<strong>参数</strong>。这就是梯度下降的地方。</p>
</li>
<li><p>设想我们根据它的域<code>θ0</code>和<code>θ1</code>来描绘我们的假设函数（实际上我们将代价函数绘制为参数估计的函数）。我们不是图表x和y本身，而是我们的假设函数的参数范围和选择一组特定参数所产生的成本。</p>
</li>
<li><p>我们在x轴上放置θ0，在y轴上放置θ1，在垂直z轴上放置代价函数。我们图上的点将是使用我们的假设与那些特定theta参数的成本函数的结果。下面的图表描述了这样的设置。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561137_131391682_1539945928" alt="_1526561137_131391682_1539945928_1526561137_131391682.png"></p>
<ul>
<li><p><strong>当我们的代价函数处于图的坑底时，即当其值最小时，我们将知道我们已经成功了。红色箭头显示图表中的最小点</strong>。</p>
</li>
<li><p>我们这样做的方式是<strong>通过获取代价函数的导数（函数的切线）</strong>。<strong>切线的斜率是该点的导数</strong>，它会给我们一个走向的方向。我们逐步降低成本函数的下降速度。每一步的大小由<code>参数α</code>决定，称为<strong>学习率</strong>。</p>
</li>
<li><p>例如，<strong>上图中每个“星号”之间的距离代表由我们的参数α确定的一个步骤。 α越小，步长越小，α越大，步长越大。步进的方向取决于J（θ0，θ1）的偏导数</strong>。根据图表的起始位置，可能会出现不同的点。上面的图片向我们展示了两个不同的起点，最终在两个不同的地方。</p>
</li>
<li><p>梯度下降算法是：<br>重复，直到收敛：<br>  <img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561170_2045903942_1539945980" alt="_1526561170_2045903942_1539945980_1526561170_2045903942.png">j = 0,1代表特征索引号。</p>
</li>
<li><p>在每次迭代j中，应<strong>同时更新参数θ1，θ2，…，θn</strong>。在计算第j次迭代之前更新特定参数会导致错误的实现。</p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561190_832177929_1539945992" alt="_1526561190_832177929_1539945992_1526561190_832177929.png"></p>
<h2 id="Gradient-Descent-Intuition"><a href="#Gradient-Descent-Intuition" class="headerlink" title="Gradient Descent Intuition"></a>Gradient Descent Intuition</h2><blockquote>
<p>在之前的视频中 我们给出了一个数学上关于梯度 下降的定义 本次视频我们更深入研究一下 更直观地感受一下这个 算法是做什么的 以及梯度下降算法的更新过程有什么意义 这是我们上次视频中看到的梯度下降算法 提醒一下 这个<code>参数 α</code> 术语称为<code>学习速率</code> <strong>它控制我们以多大的幅度更新这个参数θj</strong>. 第二部分是导数项 而我在这个视频中要做的就是 给你一个更直观的认识 这两部分有什么用 以及 为什么当把 这两部分放一起时 整个更新过程是有意义的</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561218_1102222000_1539946039" alt="_1526561218_1102222000_1539946039_1526561218_1102222000.png"></p>
<ul>
<li>为了更好地让你明白 我要做是用一个稍微简单的例子 比如我们想最小化的那个 函数只有一个参数的情形 所以 <code>假如我们有一个代价函数J 只有一个参数 θ1</code> 就像我们前几次视频中讲的 θ1是一个实数 对吧？那么我们可以画出一维的曲线 看起来很简单 让我们试着去理解 为什么梯度下降法 会在这个函数上起作用</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561252_904671270_1539946074" alt="_1526561252_904671270_1539946074_1526561252_904671270.png"></p>
<ul>
<li>所以 假如这是我的函数 关于$\theta_1$的函数$J(\theta_1)$是一个实数 对吧？ 现在我们已经对这个点上用于梯度下降法的$\theta_1$ 进行了初始化 想象一下在我的函数图像上 从那个点出发 那么梯度下降 要做的事情是<strong>不断更新 θ1等于θ1减α倍的 <code>d/dθ1J(θ1)</code>这个项</strong> 对吧？哦 顺便插一句 你知道 这个微分项是吧？可能你想问为什么我改变了符号 之前用的是偏导数的符号 如果你不知道偏导数的符号 和<code>d/dθ</code>之间的区别是什么 不用担心 从技术上讲 在数学中 我们称这是一个<code>偏导数</code> 这是一个导数 这取决于函数J的参数数量 但是这是一个 数学上的区别 就本课的目标而言 可以默认为 这些偏导数符号 和<code>d/dθ1</code>是完全一样的东西 不用担心 是否存在任何差异 我会尽量使用数学上的 精确的符号 但就我们的目的而言 这些符号是没有区别的</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561410_371681908_1539946127" alt="_1526561410_371681908_1539946127_1526561410_371681908.png"></p>
<ul>
<li>好的 那么我们来看这个方程 我们要计算 这个导数 求导的目的 基本上可以说 <strong>取这一点的切线</strong> 就是这样一条红色的直线 刚好与函数相切于这一点 让我们看看这条红色直线的斜率 其实这就是导数 也就是说 直线的斜率 也就是这条 刚好与函数曲线相切的这条直线 这条直线的斜率正好是 这个<strong>高度除以这个水平长度</strong> 现在 这条线有 一个正斜率 也就是说它有正导数 因此 我得到的新的θ <strong>θ1更新后等于θ1减去一个正数乘以α</strong>. <strong>α 也就是学习速率也是一个正数</strong> 所以 我要使θ1减去一个东西 所以相当于我将θ1向左移 使θ1变小了</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561401_1069093686_1539946176" alt="_1526561401_1069093686_1539946176_1526561401_1069093686.png"></p>
<ul>
<li>我们可以看到 这么做是对的 因为实际上我<strong>往这个方向移动 确实让我更接近那边的最低点</strong> 所以 梯度下降到目前为止似乎 是在做正确的事</li>
<li>让我们来看看另一个例子 让我们用同样的函数J 同样再画出函数J(θ1)的图像 而这次 我们把参数初始化到左边这点 所以θ1在这里 同样把这点对应到曲线上 现在 导数项<code>d/dθ1J(θ1)</code>在这点上计算时 看上去会是这样 这条线的斜率 这个导数是这条线的斜率 但是这条线向下倾斜 所以这条线具有<strong>负斜率</strong> 对吧？ 或者说 这个函数有负导数 也就意味着在那一点上有负斜率 因此 这个导数项小于等于零 所以 当我<strong>更新θ时 θ被更新为θ减去α乘以一个负数 因此我是在用 θ1减去一个负数 这意味着我实际上是在增加θ1</strong> 对不对？因为这是减去一个负数 意味着给θ加上一个数 这就意味着最后我实际上增加了θ的值 因此 我们将 从这里开始 增加θ 似乎这也是我希望得到的 也就是 让我<strong>更接近最小值</strong>了</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561445_1568980368_1539946208" alt="_1526561445_1568980368_1539946208_1526561445_1568980368.png"></p>
<ul>
<li>所以 我希望这样很直观地给你解释了 导数项的意义 让我们接下来再看一看学习速率α 我们来研究一下它有什么用 这就是我梯度下降法的 更新规则 就是这个等式 让我们来看看如果α 太小或 α 太大  会出现什么情况 这第一个例子 α太小会发生什么呢 这是我的函数J(θ) 就从这里开始 <strong>如果α太小了 那么我要做的是要去 用一个比较小的数乘以更新的值</strong> 所以最终 它就像一个小宝宝的步伐 这是一步 然后从这个新的起点开始 迈出另一步 但是由于α 太小 因此只能迈出另一个 小碎步 所以如果我的学习速率太小 结果就是 只能这样像小宝宝一样一点点地挪动 去努力接近最低点 这样就需要很多步才能到达最低点 所以<strong>如果α 太小的话 可能会很慢 因为它会一点点挪动 它会需要 很多步才能到达全局最低点</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561464_253805835_1539946231" alt="_1526561464_253805835_1539946231_1526561464_253805835.png"></p>
<ul>
<li>那么如果α 太大又会怎样呢 这是我的函数J(θ) 如果α 太大 那么梯度下降法可能会越过最低点 甚至可能无法收敛 我的意思是 比如我们从这个点开始 实际上这个点已经接近最低点 因此导数指向右侧 但如果α 太大的话 我会迈出很大一步 也许像这样巨大的一步 对吧？所以我最终迈出了一大步 现在 我的代价函数变得更糟 因为离这个最低点越来越远 现在我的导数指向左侧 实际上在减小θ 但是你看 如果我的学习速率过大 我会移动一大步 从这点一下子又到那点了 对吗？如果我的学习率太大 下一次迭代 又移动了一大步 越过一次 又越过一次 一次次越过最低点 直到你发现 实际上 离最低点越来越远 所以 <strong>如果α太大 它会导致无法收敛 甚至发散</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561479_1021372496_1539946255" alt="_1526561479_1021372496_1539946255_1526561479_1021372496.png"></p>
<ul>
<li>现在 我还有一个问题 这问题挺狡猾的  如果我们<strong>预先把θ1 放在一个局部的最低点</strong> 你认为下一步梯度下降法会怎样工作？ 所以假设你将θ1初始化在局部最低点 假设这是你的θ1的初始值 在这儿 它已经在一个局部的 最优处或局部最低点 结果是<strong>局部最优点的导数 将等于零</strong> 因为它是那条切线的斜率 而这条线的斜率将等于零 因此 此导数项等于0 因此 在你的梯度下降更新过程中 你有一个θ1 然后用θ1 减α 乘以0来更新θ1 所以这意味着什么 这意味着你已经在局部最优点 它使得θ1不再改变 也就是<strong>新的θ1等于原来的θ1 因此 如果你的参数已经处于 局部最低点 那么梯度下降法更新其实什么都没做 它不会改变参数的值</strong> 这也正是你想要的 因为它使你的解始终保持在 局部最优点 这也解释了为什么即使学习速率α 保持不变时 梯度下降也可以收敛到局部最低点 我想说的是这个意思</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561493_322597671_1539946286" alt="_1526561493_322597671_1539946286_1526561493_322597671.png"></p>
<ul>
<li>我们来看一个例子 这是代价函数J(θ) 我想找到它的最小值 首先初始化我的梯度下降算法 在那个品红色的点初始化 如果我更新一步梯度下降 也许它会带我到这个点 因为这个点的导数是相当陡的 现在 在这个绿色的点 如果我再更新一步 你会发现我的导数 也即斜率 是没那么陡的 相比于在品红点 对吧？因为随着我接近最低点 我的导数越来越接近零 所以 <strong>梯度下降一步后 新的导数会变小一点点</strong> 然后我想再梯度下降一步 在这个绿点我自然会用一个稍微 跟刚才在那个品红点时比 再小一点的一步  现在到了新的点 红色点 更接近全局最低点了 因此这点的导数会比在绿点时更小 所以  我再进行一步梯度下降时 我的导数项是更小的 θ1更新的幅度就会更小 所以你会移动更小的一步 像这样 <strong>随着梯度下降法的运行  你移动的幅度会自动变得越来越小 直到最终移动幅度非常小 你会发现 已经收敛到局部极小值</strong> 所以回顾一下 <strong>在梯度下降法中 当我们接近局部最低点时 梯度下降法会自动采取 更小的幅度</strong> 这是因为当我们接近局部最低点时 很显然在<strong>局部最低时导数等于零</strong> 所以当我们 接近局部最低时 导数值会自动变得越来越小 所以梯度下降将自动采取较小的幅度 这就是梯度下降的做法 所以<strong>实际上没有必要再另外减小α 这就是梯度下降算法</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561559_975182746_1539946335" alt="_1526561559_975182746_1539946335_1526561559_975182746.png"></p>
<ul>
<li>你可以用它来最小化 最小化任何代价函数J 不只是线性回归中的代价函数J 在接下来的视频中 我们要用代价函数J 回到它的本质 线性回归中的代价函数 也就是我们前面得出的平方误差函数 结合梯度下降法 以及平方代价函数 我们会得出第一个机器学习算法 即线性回归算法</li>
</ul>
<h3 id="Gradient-Descent-Intuition笔记"><a href="#Gradient-Descent-Intuition笔记" class="headerlink" title="Gradient Descent Intuition笔记"></a>Gradient Descent Intuition笔记</h3><ul>
<li>在本视频中，我们探索了使用一个参数θ1并绘制其代价函数来实现梯度下降的场景。 我们的单一参数公式为：</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561584_1088750870_1539946381" alt="_1526561584_1088750870_1539946381_1526561584_1088750870.png"></p>
<ul>
<li>无论<img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561607_175617464_1539946398" alt="_1526561607_175617464_1539946398_1526561607_175617464.png">的斜率符号如何，θ1最终收敛到其最小值。 下图显示<strong>当斜率为负值时，θ1的值增加，当为正值时，θ1的值减小</strong>。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561620_33298999_1539946410" alt="_1526561620_33298999_1539946410_1526561620_33298999.png"></p>
<ul>
<li>在附注中，我们应该调整<code>参数α</code>以确保梯度下降算法在合理的时间内收敛。 未能收敛或获得最小值的时间太多意味着我们的步长是错误的。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561636_1697292728_1539946422" alt="_1526561636_1697292728_1539946422_1526561636_1697292728.png"></p>
<ul>
<li>梯度下降如何以固定步长α收敛？<br>收敛背后的直觉是，当我们逼近我们的凸函数的底部时，<img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561656_442389494_1539946439" alt="_1526561656_442389494_1539946439_1526561656_442389494.png">接近0。 至少，派生将始终为0，因此我们得到：</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561667_1647461546_1539946468" alt="_1526561667_1647461546_1539946468_1526561667_1647461546.png"></p>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561675_2093920258_1539946481" alt="_1526561675_2093920258_1539946481_1526561675_2093920258.png"></p>
<h2 id="Gradient-Descent-For-Linear-Regression"><a href="#Gradient-Descent-For-Linear-Regression" class="headerlink" title="Gradient Descent For Linear Regression"></a>Gradient Descent For Linear Regression</h2><blockquote>
<p>在以前的视频中我们谈到 关于梯度下降算法 梯度下降是很常用的算法 它不仅被用在线性回归上 和线性回归模型、平方误差代价函数 在这段视频中 我们要 <strong>将<code>梯度下降</code> 和<code>代价函数</code>结合</strong> 在后面的视频中 我们将用到此算法 并将其应用于 具体的拟合直线的线性回归算法里 这就是 我们在之前的课程里所做的工作</p>
</blockquote>
<ul>
<li>这是<strong>梯度下降算法</strong> 这个算法你应该很熟悉 这是<strong>线性回归模型</strong> 还有<strong>线性假设</strong>和<strong>平方误差代价函数</strong> 我们将要做的就是 <strong>用梯度下降的方法 来最小化平方误差代价函数</strong> 为了 使梯度下降 为了 写这段代码 我们需要的<strong>关键项 是这里这个微分项</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561743_42538752_1539946555" alt="_1526561743_42538752_1539946555_1526561743_42538752.png"></p>
<ul>
<li>所以.我们需要弄清楚 这个偏导数项是什么 并结合这里的 代价函数J 的定义 就是这样 一个求和项 代价函数就是 这个误差平方项 我这样做 只是 <strong>把定义好的代价函数 插入了这个微分式 再简化一下</strong> 这等于是 这一个求和项 <code>θ0 + θ1x(i) - y(i)</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561818_666946699_1539946612" alt="_1526561818_666946699_1539946612_1526561818_666946699.png"></p>
<ul>
<li>实际上我们需要 弄清楚这两个 偏导数项是什么 这两项分别是 j=0 和j=1的情况 因此<strong>我们要弄清楚 θ0 和 θ1 对应的 偏导数项是什么</strong> (<strong>将上面的式子平方化开再分别对θ0 和θ1求偏导</strong>)</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561861_322362701_1539946651" alt="_1526561861_322362701_1539946651_1526561861_322362701.png"></p>
<ul>
<li>所以 偏导数项 从这个等式 到下面的等式 计算这些偏导数项需要一些多元微积分 如果你掌握了微积分 你可以随便自己推导这些 然后你检查你的微分 你实际上会得到我给出的答案 但如果你 不太熟悉微积分 别担心 你可以直接用这些 已经算出来的结果 你不需要掌握微积分 或者别的东西 来完成作业 你只需要会用梯度下降就可以</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561892_649652808_1539951959" alt="_1526561892_649652808_1539951959_1526561892_649652808.png"></p>
<ul>
<li>在定义这些以后 在我们算出 这些微分项以后 <strong>这些微分项 实际上就是代价函数J的斜率</strong> 现在可以将它们放回 我们的梯度下降算法 所以这就是<strong>专用于 线性回归的梯度下降 反复执行括号中的式子直到收敛</strong> <strong>θ0和θ1不断被更新 都是加上一个<code>-α/m</code> 乘上后面的求和项</strong> 所以这里这一项 所以这就是我们的<strong>线性回归算法</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561936_996217166_1539952078" alt="_1526561936_996217166_1539952078_1526561936_996217166.png"></p>
<ul>
<li>这一项就是<strong>关于θ0的偏导数</strong> 在上一张幻灯片中推出的</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561956_1952955701_1539952499" alt="_1526561956_1952955701_1539952499_1526561956_1952955701.png"></p>
<ul>
<li>而第二项 这一项是刚刚的推导出的 <strong>关于θ1的 偏导数项</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526561971_576823992_1539952519" alt="_1526561971_576823992_1539952519_1526561971_576823992.png"></p>
<ul>
<li>提醒一下 <strong>执行梯度下降时 有一个细节要注意 就是必须要 同时更新θ0和θ1</strong></li>
<li>所以 让我们来看看梯度下降是如何工作的 我们用梯度下降解决问题的 一个原因是 <strong>它更容易得到局部最优值</strong> 当我第一次解释梯度下降时 我展示过这幅图</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562028_517004246_1539952542" alt="_1526562028_517004246_1539952542_1526562028_517004246.png"></p>
<ul>
<li>在表面上 不断下降 并且我们知道了 根据你的初始化 你会得到不同的局部最优解 你知道.你可以结束了.在这里或这里。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562040_769496498_1539952555" alt="_1526562040_769496498_1539952555_1526562040_769496498.png"></p>
<ul>
<li>但是 <strong>事实证明 用于线性回归的 代价函数 总是这样一个 弓形的样子</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562056_1050624241_1539952593" alt="_1526562056_1050624241_1539952593_1526562056_1050624241.png"></p>
<ul>
<li>这个函数的专业术语是 这是一个<code>凸函数</code> 我不打算在这门课中 给出凸函数的定义 <code>凸函数(convex function)</code> 但不正式的说法是 它就是一个弓形的函数 因此 <strong>这个函数 没有任何局部最优解 只有一个全局最优解</strong> 并且<strong>无论什么时候 你对这种代价函数 使用线性回归 梯度下降法得到的结果 总是收敛到全局最优值</strong> 因为没有全局最优以外的其他局部最优点</li>
<li>现在 让我们来看看这个算法的执行过程 像往常一样 这是<code>假设函数</code>的图 还有<code>代价函数J</code>的图</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562070_1456917132_1539952610" alt="_1526562070_1456917132_1539952610_1526562070_1456917132.png"></p>
<ul>
<li>让我们来看看如何 初始化参数的值<strong>通常来说 初始化参数为零 θ0和θ1都在零</strong> 但为了展示需要 在这个梯度下降的实现中 我<strong>把θ0初始化为-900 θ1初始化为-0.1</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562104_573736577_1539952665" alt="_1526562104_573736577_1539952665_1526562104_573736577.png"></p>
<ul>
<li>现在进行一次梯度下降,从一点开始向左下方移动一小步,然后就得到了第二个点,假设函数的线改变了一点点.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562122_1863333386_1539952686" alt="_1526562122_1863333386_1539952686_1526562122_1863333386.png"></p>
<ul>
<li>不断的移动代价函数的点梯度不断下降,假设函数越来越拟合数据,,直到收敛到全局最小值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562131_948800849_1539952693" alt="_1526562131_948800849_1539952693_1526562131_948800849.png"></p>
<ul>
<li>现在就可以用它来预测房价了.</li>
<li>“Batch”Gradient Descent:批量梯度下降:在梯度下降的每一步中,我们都用到了所有的训练样本</li>
<li>在梯度下降中,在计算微分求导项时,我们需要进行求和计算,所以在每一个单独的梯度计算中,我们最终都要计算这样一个东西—这个项需要<strong>对所有m个训练样本求和</strong>.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526562146_1798195049_1539952711" alt="_1526562146_1798195049_1539952711_1526562146_1798195049.png"></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.fengwenhua.top/2018/11/15/第一章-Model-and-Cost-Function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="冯文华">
      <meta itemprop="description" content="记录日常学习与生活">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江南小虫虫的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/15/第一章-Model-and-Cost-Function/" class="post-title-link" itemprop="http://www.fengwenhua.top/index.html">第一章: Model and Cost Function</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-14 20:17:43 / 修改时间：12:18:45" itemprop="dateCreated datePublished" datetime="2018-11-14T20:17:43Z">2018-11-14</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/机器学习入门/" itemprop="url" rel="index"><span itemprop="name">机器学习入门</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/11/15/第一章-Model-and-Cost-Function/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/11/15/第一章-Model-and-Cost-Function/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/11/15/第一章-Model-and-Cost-Function/" class="leancloud_visitors" data-flag-title="第一章: Model and Cost Function">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数：</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">10k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">17 分钟</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h1 id="第一章：-Model-and-Cost-Function"><a href="#第一章：-Model-and-Cost-Function" class="headerlink" title="第一章： Model and Cost Function"></a>第一章： Model and Cost Function</h1><h2 id="Model-Representation-模型表示"><a href="#Model-Representation-模型表示" class="headerlink" title="Model Representation:模型表示"></a>Model Representation:模型表示</h2><ul>
<li>我们的第一个学习算法是<code>线性回归算法</code>,了解监督学习过程完整的流程</li>
</ul>
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><ul>
<li>这个例子是预测住房价格的 我们要使用一个数据集 数据集包含俄勒冈州波特兰市的住房价格 在这里 我要根据不同房屋尺寸所售出的价格 画出我的数据集</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486337_877822591_1539943286" alt="_1526486337_877822591_1539943286_1526486337_877822591.png"></p>
<ul>
<li>比方说 我们来看这个数据集 你有一个朋友正想出售自己的房子 如果你朋友的房子是1250平方尺大小 你要告诉他们 这房子能卖多少钱 那么 你可以<strong>做的一件事就是 构建一个模型</strong> 也许是条直线 从这个数据模型上来看 也许你可以告诉你的朋友 他能以大约220000(美元)左右的价格 卖掉这个房子 那么这就是监督学习算法的一个例子</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486364_1763509138_1539943339" alt="_1526486364_1763509138_1539943339_1526486364_1763509138.png"></p>
<ul>
<li>它被称作监督学习是因为对于每个数据来说 我们给出了 “<code>正确的答案</code>” 即告诉我们 根据我们的数据来说 房子实际的价格是多少 而且 更具体来说 这是一个<code>回归问题</code> <strong>回归一词指的是我们根据之前的数据预测出一个准确的输出值</strong> 对于这个例子就是<strong>价格</strong></li>
<li>同时 还有另一种最常见的监督学习方式 叫做<code>分类问题</code> <strong>当我们想要预测离散的输出值</strong> 例如 如果我们正在寻找 癌症肿瘤并想要确定 肿瘤是良性的还是恶性的 这就是0/1离散输出的问题</li>
<li>更进一步来说 <strong>在监督学习中我们有一个数据集 这个数据集被称</strong><code>训练集</code> 因此对于房价的例子 我们有一个训练集 包含不同的房屋价格 我们的任务就是从这个数据中学习预测房屋价格</li>
<li>现在我们给出这门课中经常使用的一些符号定义 我们要定义颇多符号 不过没关系 现在你记不住所有的符号也没关系 随着课程的进展 你会发现记住这些符号会很有用 我将在整个课程中用小写的</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486393_1037348134_1539943381" alt="_1526486393_1037348134_1539943381_1526486393_1037348134.png"></p>
<ul>
<li><code>m</code>:<strong>来表示训练样本的数目</strong> 因此 在这个数据集中 如果表中有47行 那么我们就有47组训练样本 m就等于47</li>
<li><code>x</code><strong>来表示输入变量 往往也被称为特征量</strong> 这就是用x表示输入的特征</li>
<li><code>y</code><strong>来表示输出变量或者目标变量</strong> 也就是我的<strong>预测结果</strong> 那么这就是第二列</li>
<li>在这里使用<code>(x, y)</code>来表示一个<code>训练样本</code> 所以 在这个表格中的单独的一行对应于一个训练样本</li>
<li><p><code>表示某个训练样本</code> 我将使用<code>x上标(i)</code>与<code>y上标(i)</code>来表示 ,即<img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486414_196120304_1539943435" alt="_1526486414_196120304_1539943435_1526486414_196120304.png"></p>
</li>
<li><p>这就是一个监督学习算法的工作方式 我们可以看到这里有我们的训练集里房屋价格 我们把它喂给我们的学习算法 这就是学习算法的工作了 然后输出一个函数 按照惯例 通常表示为小写h <code>h代表hypothesis(假设)</code> <strong>h表示一个函数</strong> 输入是房屋尺寸大小 就像你朋友想出售的房屋 因此 h 根据输入的 x 值来得出 y 值 y值对应房子的价格 因此 <strong>h是一个从x到y的函数映射</strong></p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486441_1477225763_1539943461" alt="_1526486441_1477225763_1539943461_1526486441_1477225763.png"></p>
<ul>
<li>人们经常问我为什么这个函数被称作<code>假设(hypothesis)</code>你们中有些人可能知道<code>hypothesis</code>的意思 从字典或者其它什么方式可以查到 其实在机器学习中 这是一个在早期被用于机器学习的名称 它有点绕口 对这类函数来说 这可能不是一个很恰当的名字 对表示从房屋的大小到价格的函数映射 我认为这个词”hypothesis” 可能不是最好的名称 但是这是人们在机器学习中使用的标准术语 所以不用太纠结人们为什么这么叫它</li>
<li><strong>当设计学习算法的时候 我们接下来需要去思考的是 怎样得到这个假设h</strong></li>
<li>对于这一点在接下来的几个视频中 我将选择最初的使用规则 h代表hypothesis 我们将会这么写<img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486475_1276561196_1539943500" alt="_1526486475_1276561196_1539943500_1526486475_1276561196.png">为了方便 有时非书面形式也可以这么写 <code>hθ(x)</code> 我就写成<code>h(x)</code> 这是缩写方式 但一般来说我会保留这个下标θ 从这个图片中 所有这一切意味着我们要预测一个关于x的 线性函数 y 对吧? 所以这就是<strong>数据集和函数的作用:<code>用来预测</code></strong> 这里是y关于x的线性函数 <code>hθ(x)=θ0+θ1*x</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486492_526471029_1539943529" alt="_1526486492_526471029_1539943529_1526486492_526471029.png"></p>
<ul>
<li>那么为什么是一个线性函数呢? 有时候 我们会有更复杂的函数 也许是非线性函数 但是 由于线性方程是简单的形式 我们将先从线性方程的例子入手 当然 最终我们将会建立更复杂的模型 以及更复杂的学习算法 好吧 让我们也给这模型 起一个名字 这个模型被称为<code>线性回归(linear regression)模型</code> 另外 这实际上是关于单个变量的线性回归 这个变量就是x 根据x来预测所有的价格函数 同时 对于这种模型有另外一个名称 称作<code>单变量线性回归</code> 单变量是对一个变量的一种 特别的表述方式 总而言之 这就是线性回归 在接下来的视频中 我们将开始讨论如何去实现这种模型</li>
</ul>
<h3 id="笔记–Model-Representation模型表示"><a href="#笔记–Model-Representation模型表示" class="headerlink" title="笔记–Model Representation模型表示"></a>笔记–Model Representation模型表示</h3><ul>
<li>为了建立将来使用的符号，我们将使用 <code>x^i</code>来表示<code>“输入”变量</code>（在这个例子中是居住区域），也被称为<code>输入要素</code>，而 <code>y^i</code> 表示我们试图<code>预测的“输出”或目标变量</code>（价格）。一对 <code>(x^i，y^i)</code>被称为<code>训练样例</code>，我们将用来学习的<code>数据集----m</code>个训练样例的列表<code>(x^i，y^i)</code>;i= 1，。 。 。 ，<code>m</code>—- 被称为<code>训练集</code>。请注意，符号中的上标“（<code>i</code>）”仅仅是训练集的<code>索引</code>，与幂运算无关。我们也将用<code>X</code>来表示<code>输入值的空间</code>，用<code>Y</code>来表示<code>输出值的空间</code>。在这个例子中，X = Y =ℝ。</li>
<li>为了更形式化地描述监督学习问题，我们的目标是在给定训练集的情况下，去学习一个函数<code>h：X→Y</code>，使得<code>h（x）</code>是<code>y</code>的相应值的<code>“好”预测器</code>。由于历史原因，这个函数<code>h</code>被称为<code>假设</code>。从形象上看，这个过程是这样的：</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486547_637097161_1539943596" alt="_1526486547_637097161_1539943596_1526486547_637097161.png"></p>
<ul>
<li>当我们试图预测的目标变量是<code>连续的</code>，比如在我们的住房例子中，我们把学习问题称为<code>回归问题</code>。当y只能接受少量的<code>离散值</code>时（比如，如果考虑到居住面积，我们想要预测一个住宅是房子还是公寓），我们称之为<code>分类问题</code>。</li>
</ul>
<h2 id="Cost-Function-代价函数"><a href="#Cost-Function-代价函数" class="headerlink" title="Cost Function(代价函数)"></a>Cost Function(代价函数)</h2><ul>
<li>这里将定义<code>代价函数</code>的概念 这有助于我们 <code>弄清楚如何把最有可能的直线与我们的数据相拟合</code></li>
<li><p>在线性回归中我们有一个像这样的训练集 记住<code>M代表了训练样本的数量</code> 所以 比如说<code>M = 47</code> 而我们的假设函数 也就是用来进行预测的函数 是这样的线性函数形式<img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486475_1276561196_1539943500" alt="_1526486475_1276561196_1539943500_1526486475_1276561196.png"></p>
</li>
<li><p>接下来我们会引入一些术语 这些<code>θ0</code>和<code>θ1</code> 这些θi我把它们称为<code>模型参数</code> 在这个视频中 我们要做的就是<strong>谈谈如何选择这两个参数值θ0和θ1</strong></p>
</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486641_873314729_1539943679" alt="_1526486641_873314729_1539943679_1526486641_873314729.png"></p>
<ul>
<li>选择不同的参数θ0和θ1 我们会得到不同的假设函数，如果θ0是1.5 θ1是0 那么假设函数会看起来是这样， 因为你的假设函数是<code>h(x)=1.5+0*x</code> 是这样一个常数函数 恒等于1.5 如果θ0=0并且θ1=0.5 那么假设会看起来像这样 它会通过点(2,1) 这样你又得到了h(x) 或者hθ(x) 但是有时我们为了简洁会省略θ 因此 h(x)将等于0.5倍的x 就像这样 最后 如果θ0=1并且θ1=0.5 我们最后得到的假设会看起来像这样 让我们来看看 它应该通过点(2,2) 这是我的新的h(x)或者写作hθ(x) 对吧？ 你还记得之前我们提到过·hθ(x)的 但作为简写 我们通常只把它写作h(x)·</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486670_715550732_1539943705" alt="_1526486670_715550732_1539943705_1526486670_715550732.png"></p>
<ul>
<li>在线性回归中 我们有一个训练集 可能就像我在这里绘制的 <strong>我们要做的就是 得出θ0 θ1这两个参数的值 来让假设函数表示的直线 尽量地与这些数据点很好的拟合</strong> 也许就像这里的这条线一样</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486709_1177455732_1539943731" alt="_1526486709_1177455732_1539943731_1526486709_1177455732.png"></p>
<ul>
<li>那么我们如何得出θ0 θ1的值 来使它很好地拟合数据的呢？<strong>我们的想法是 我们要选择 能使h(x) 也就是 输入x时我们预测的值 最接近该样本对应的y值的参数θ0 θ1</strong>所以 在我们的训练集中我们会得到一定数量的样本 我们知道x表示卖出哪所房子 并且知道这所房子的实际价格 所以 我们要尽量选择参数值 使得 在训练集中 给出训练集中的x值 我们能合理准确地预测y的值</li>
<li>让我们给出标准的定义 <code>在线性回归中 我们要解决的是一个最小化问题 所以我要写出关于θ0 θ1的最小化 而且 我希望这个式子极其小 是吧 我想要h(x)和y之间的差异要小 我要做的事情是尽量减少假设的输出与房子真实价格 之间的差的平方</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486729_441195085_1539943756" alt="_1526486729_441195085_1539943756_1526486729_441195085.png"></p>
<ul>
<li>接下来我会详细的阐述 别忘了 我用符号( x(i),y(i) )代表第i个样本 所以我想要做的是<strong>对所有训练样本进行一个求和</strong> 对i=1到i=M的样本 将对假设进行预测得到的结果 此时的输入是第i号房子的面积 对吧 将第i号对应的预测结果 减去第i号房子的实际价格 所得的差的平方相加得到总和 而我希望尽量减小这个值 也就是<code>预测值和实际值的差的平方误差和</code> 或者说预测价格和 实际卖出价格的差的平方 我说了这里的m指的是训练集的样本容量 对吧 这个<code>井</code>号是<code>训练样本“个数”</code>的缩写 对吧</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486765_95100177_1539943786" alt="_1526486765_95100177_1539943786_1526486765_95100177.png"></p>
<ul>
<li>而为了让表达式的数学意义 变得容易理解一点 我们实际上考虑的是 这个数的<code>1/m</code> 因此我们要尝试尽量减少我们的平均误差 也就是<strong>尽量减少其1/2m</strong> 通常是这个数的一半 前面的这些只是为了使数学更直白一点 因此对这个求和值的二分之一求最小值 应该得出相同的θ0值和相同的θ1值来 请大家一定弄清楚这个道理 没问题吧？在这里hθ(x)的这种表达 这是我们的假设 它等于θ0加上θ1与x(i)的乘积 而这个表达 表示关于θ0和θ1的最小化过程 这意味着我们要找到θ0和θ1 的值来使这个表达式的值最小 这个表达式因θ0和θ1的变化而变化对吧？ 因此 简单地说 我们正在把这个问题变成 <code>找到能使 我的训练集中预测值和真实值的差的平方的和 的1/2M最小的θ0和θ1的值</code> 因此 这将是我的线性回归的整体目标函数</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486784_343575659_1539943825" alt="_1526486784_343575659_1539943825_1526486784_343575659.png"></p>
<ul>
<li>为了使它更明确一点 我们要改写这个函数 按照惯例 我要定义一个<code>代价函数</code> 正如屏幕中所示 这里的这个公式 我们想要做的就是<code>关于θ0和θ1 对函数J(θ0,θ1)求最小值</code> 这就是我的代价函数 <code>代价函数也被称作平方误差函数</code> 有时也被称为 <code>平方误差代价函数</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486807_2121076322_1539943869" alt="_1526486807_2121076322_1539943869_1526486807_2121076322.png"></p>
<ul>
<li>事实上 我们之所以要求出 误差的平方和 是因为<code>误差平方代价函数 对于大多数问题 特别是回归问题 都是一个合理的选择</code> 还有其他的代价函数也能很好地发挥作用 但是平方误差代价函数可能是解决回归问题最常用的手段了 在后续课程中 我们还会谈论其他的代价函数 但我们刚刚讲的选择是对于大多数线性回归问题非常合理的 好吧 所以这是代价函数 到目前为止 我们已经 介绍了代价函数的数学定义 也许这个函数<code>J(θ0,θ1)</code>有点抽象 可能你仍然不知道它的内涵 在接下来的几个视频里 我们要更进一步解释 代价函数J的工作原理 并尝试更直观地解释它在计算什么 以及我们使用它的目的</li>
</ul>
<h3 id="Cost-Function-代价函数-笔记"><a href="#Cost-Function-代价函数-笔记" class="headerlink" title="Cost Function(代价函数)笔记"></a>Cost Function(代价函数)笔记</h3><ul>
<li>我们可以<code>通过使用代价函数来衡量我们的假设函数的准确性</code>。 这个假设的所有结果的平均差异（实际上是一个平均值的更漂亮的版本）与来自x的输入和实际输出y的输入。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486842_1687157529_1539943891" alt="_1526486842_1687157529_1539943891_1526486842_1687157529.png"></p>
<ul>
<li>To break it apart，结果是<img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486856_1339445624_1539944023" alt="_1526486856_1339445624_1539944023_1526486856_1339445624.png">，其中<img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486875_1819812278_1539944058" alt="_1526486875_1819812278_1539944058_1526486875_1819812278.png">是 $h_\theta(x_i)-y_i$  平方的平均值，或<code>预测值与实际值之间的差值</code>。</li>
<li>该函数被称为“<code>平方误差函数(Squared error function)</code>”或“<code>均方误差(Mean squared error)</code>”。 <strong>由于平方函数的导数项将抵消该项，平均值被减半以作为计算梯度下降的便利</strong>。 以下图片总结了成本函数的作用：</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486916_672453545_1539944069" alt="_1526486916_672453545_1539944069_1526486916_672453545.png"></p>
<h2 id="Cost-Function-Intuition-I"><a href="#Cost-Function-Intuition-I" class="headerlink" title="Cost Function - Intuition I"></a>Cost Function - Intuition I</h2><ul>
<li>在上一个视频中 我们给了代价函数一个数学上的定义 在这个视频里 让我们通过一些例子来获取一些直观的感受 看看代价函数到底是在干什么 回顾一下 这是我们上次所讲过的内容 我们想找一条直线来拟合我们的数据 所以我们用 θ0 θ1 等参数 得到了这个假设 而且通过选择不同的参数 我们会得到不同的直线拟合 所以拟合出的数据就像这样 然后我们还有一个代价函数 这就是我们的优化目标</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486985_1746525095_1539944106" alt="_1526486985_1746525095_1539944106_1526486985_1746525095.png"></p>
<ul>
<li>在这个视频里 为了更好地 将代价函数可视化 我将使用一个简化的假设函数 就是下面这个函数 然后我将会用这个简化的假设 也就是 <code>θ1*x</code> 我们可以将这个函数看成是 把 <code>θ0 设为0</code> 所以我只有一个参数 也就是 <code>θ1</code> 代价函数看起来与之前的很像 唯一的区别是现在 h(x) 等于 <code>θ1*x</code> 只有一个参数 θ1 所以我的 优化目标是将 <code>J(θ1)</code>最小化 用图形来表示就是 如果 <code>θ0</code> 等于零 也就意味这我们选择的假设函数 会经过原点 也就是经过坐标 (0,0) 通过利用简化的假设得到的代价函数</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487023_1285650805_1539944167" alt="_1526487023_1285650805_1539944167_1526487023_1285650805.png"></p>
<ul>
<li>我们可以试着更好地理解 代价函数这个概念 我们要理解的是这两个重要的函数 第一个是假设函数 第二个是代价函数 注意这个假设函数 h(x) 对于一个固定的 θ1 ，h(x)是关于 x 的函数 所以这个假设函数就是一个关于 x 这个房子大小的函数 与此不同的是 代价函数 J 是一个关于参数 θ1 的函数 而 θ1 控制着这条直线的斜率</li>
<li>现在我们把这写函数都画出来 试着更好地理解它们 我们从假设函数开始 比如说这里是我的<strong>训练样本 它包含了三个点 (1,1) (2,2) 和 (3,3)</strong> 现在我们随便选择一个值 θ1 ，这里<strong>选择 θ1 等于1</strong> ，选择 θ1=1之后， 那么我的假设函数看起来就会像是这条直线 我将要指出的是 <strong>当我想要描绘出我的假设函数时， 我的横轴被标定为X轴 X轴是表示房子大小的量 现在暂时把 θ1 定为1 我想要做的就是 算出在 θ1 等于 1 的时候 J(θ1) 等于多少</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487060_1394238758_1539944241" alt="_1526487060_1394238758_1539944241_1526487060_1394238758.png"></p>
<ul>
<li>所以我们 按照这个思路来计算代价函数的大小 和之前一样 代价函数定义如下 是吧 对这个误差平方项进行求和 这就等于 这样一个形式 简化以后就等于 三个0的平方和 当然还是0</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487093_1136770439_1539944268" alt="_1526487093_1136770439_1539944268_1526487093_1136770439.png"></p>
<ul>
<li>现在 在代价函数里 我们发现所有这些值都等于0 因为<strong>对于我所选定的这三个训练样本 ( 1 ,1 ) (2,2) 和 (3,3) 如果 θ1 等于 1 那么 h(x(i)) 就会正好等于 y(i) 所以 <code>h(x) - y</code> 所有的这些值都会等于零</strong> 这也就是为什么<strong>J(1) 等于零</strong></li>
<li>所以 我们现在知道了 J(1) 是0 让我把这个画出来 我将要在屏幕右边画出我的代价函数 J 要注意的是 因为我的代价函数是关于参数 θ1 的函数 当我描绘我的<strong>代价函数时 X轴就是 θ1</strong> 现在我有 <strong>J(1) 等于零</strong> 让我们继续把函数画出来 结果我们会得到这样一个点</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487138_631239648_1539944315" alt="_1526487138_631239648_1539944315_1526487138_631239648.png"></p>
<ul>
<li>现在我们来看其它一些样本 θ1 可以被设定为 某个范围内各种可能的取值 所以 θ1 可以取负数 0 或者正数 所以如果 <code>θ1 等于0.5</code>会发生什么呢 继续把它画出来 现在要把 θ1 设为0.5 在这个条件下 我的假设函数看起来就是这样 这条线的斜率等于0.5 现在让我们计算 J(0.5) 所以这将会等于1除以2m 乘以那一块 其实我们不难发现后面的求和 就是这条线段的高度的平方 加上这条线段高度的平方 再加上这条线段高度的平方 三者求和 对吗？ 就是 y(i) 与预测值 h(x(i)) 的差 对吗 所以第一个样本将会是0.5减去1的平方 因为我的假设函数预测的值是0.5 而实际值则是1 第二个样本 我得到的是1减去2的平方 因为我的假设函数预测的值是1 但是实际房价是2 最后 加上 1.5减去3的平方 那么这就等于1除以2乘以3 因为训练样本有三个点所以 m 等于3 对吧 然后乘以括号里的内容 简化后就是3.5 所以这就等于3.5除以6 也就约等于0.68</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487160_1122698449_1539944357" alt="_1526487160_1122698449_1539944357_1526487160_1122698449.png"></p>
<ul>
<li>让我们把这个点画出来 不好意思 有一个计算错误 <code>这实际上该是0.58</code> 所以我们把点画出来 大约会是在这里 对吗</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487174_936519972_1539944425" alt="_1526487174_936519972_1539944425_1526487174_936519972.png"></p>
<ul>
<li>现在 让我们再多做一个点 让我们试试θ1等于0 J(0) 会等于多少呢 如果θ1等于0 那么 h(x) 就会等于一条水平的线 对了 就会像这样是水平的 所以 测出这些误差 我们将会得到 J(0) 等于 1除以 2m 乘以1的平方 加上2的平方 加上3的平方 也就是 1除以6乘以14 也就是2.3左右 所以让我们接着把这个点也画出来 所以这个点最后是2.3</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487181_1047071511_1539944461" alt="_1526487181_1047071511_1539944461_1526487181_1047071511.png"></p>
<ul>
<li>当然我们可以接着设定 θ1 等于别的值 进行计算 你也可以把 θ1 设定成一个负数 所以如果 θ1 是负数 那么 h(x) 将会等于 打个比方说 －0.5 乘以x 然后 θ1 就是 -0.5 那么这将会 对应着一个斜率为-0.5的假设函数 而且你可以 继续计算这些误差 结果你会发现 对于0.5 结果会是非常大的误差 最后会得到一个较大的数值 类似于5.25 等等 对于不同的 θ1 你可以计算出这些对应的值 对吗 结果你会发现 你算出来的这些值 你得到一条这样的曲线 通过计算这些值 你可以慢慢地得到这条线 这就是 J(θ) 的样子了</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487193_1227128932_1539944492" alt="_1526487193_1227128932_1539944492_1526487193_1227128932.png"></p>
<ul>
<li>我们来回顾一下 <strong>任何一个 θ1 的取值对应着一个不同的 假设函数 或者说对应着左边一条不同的拟合直线。 对于任意的θ1 你可以算出一个不同的 J(θ1) 的取值</strong> 举个例子 你知道的 θ1 等于1时对应着穿过这些数据的这条直线 当 θ1 等于0.5 也就是这个玫红色的点 也许对应着这条线 然后 θ1 等于0 也就是蓝色的这个点 对应着 这条水平的线 对吧</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487236_2027776779_1539944521" alt="_1526487236_2027776779_1539944521_1526487236_2027776779.png"></p>
<ul>
<li>所以对于任意一个 θ1 的取值 我们会得到 一个不同的 J(θ1) 而且我们可以利用这些来描出右边的这条曲线 现在你还记得 <strong>学习算法的优化目标 是我们想找到一个 θ1 的值 来将 J(θ1) 最小化</strong> 对吗 这是我们线性回归的目标函数 嗯 看这条曲线 <strong>让 J(θ1) 最小化的值 是 θ1 等于1 然后你看 这个确实就对应着最佳的通过了数据点的拟合直线 这条直线就是由 θ1=1 的设定而得到的 然后 对于这个特定的训练样本 我们最后能够完美地拟合 这就是为什么最小化 J(θ1) 对应着寻找一个最佳拟合直线的目标</strong></li>
<li>总结一下 在这个视频里 我们看到了一些图形 来理解代价函数 要做到这个 我们简化了算法 让这个函数只有一个参数 θ1 也就是说我们把 θ0 设定为0 在下一个视频里 我们将回到原来的问题的公式 然后看一些 带有 θ0 和 θ1 的图形 也就是说不把 θ0 设置为0了 希望这会让你更好地理解在原来的线性回归公式里 代价函数 J 的意义</li>
</ul>
<h3 id="Cost-Function-Intuition-I笔记"><a href="#Cost-Function-Intuition-I笔记" class="headerlink" title="Cost Function - Intuition I笔记"></a>Cost Function - Intuition I笔记</h3><ul>
<li>如果我们试图用视觉术语来思考它，我们的训练数据集就散布在<code>x-y平面</code>上。 我们试图做一条直线（由$h_\theta(x)$定义）来穿过这些散布的数据点。</li>
<li>我们的目标是获得最佳线路。 尽可能最好的线是这样的，以便<strong>线上散射点的平均垂直距离将是最小的</strong>。 理想情况下，该线应该通过我们训练数据集的所有点。 在这种情况下，$J(\theta_0,\theta_1)$值将为0.以下示例显示了代价函数为0的理想情况。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487322_1703355374_1539944559" alt="_1526487322_1703355374_1539944559_1526487322_1703355374.png"></p>
<ul>
<li>当$\theta_1=1$我们得到1的斜率时，它会经历我们模型中的每个单一数据点。 相反，当$\theta_1=0.5$我们看到从适合度到数据点的垂直距离增加时。</li>
<li>这使我们的代价函数增加到0.58。 绘制几个其他点产生到以下图表：</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487533_1540848643_1539944572" alt="_1526487533_1540848643_1539944572_1526487533_1540848643.png"></p>
<ul>
<li>因此，作为一个目标，我们应该尽量减少代价函数。 在这种情况下，$\theta_1=1$是我们整体最低的。</li>
</ul>
<h2 id="Cost-Function-Intuition-II"><a href="#Cost-Function-Intuition-II" class="headerlink" title="Cost Function - Intuition II"></a>Cost Function - Intuition II</h2><ul>
<li>这节课中 我们将更深入地学习代价函数的作用 这段视频的内容假设你已经认识轮廓图 如果你对轮廓图不太熟悉的话 这段视频中的某些内容你可能会听不懂 但不要紧 如果你跳过这段视频的话 也没什么关系 不听这节课对后续课程理解影响不大</li>
<li>和之前一样 这是我们的几个重要公式 包括了假设h、参数θ、代价函数J 以及优化目标 跟前一节视频不同的是 我还是把θ写成$\theta_0$,$\theta_1$的形式 便于这里我们要对代价函数进行的可视化</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487815_1577285052_1539944607" alt="_1526487815_1577285052_1539944607_1526487815_1577285052.png"></p>
<ul>
<li>和上次一样 首先来理解假设h和代价函数J 这是<strong>房价数据组成的训练集数据</strong> 让我们来构建某种假设 就像这条线一样 很显然这不是一个很好的假设 但不管怎样 如果我<strong>假设θ0等于50 θ1等于0.06的话 那么我将得到这样一个假设函数</strong> 对应于这条直线 给出θ0和θ1的值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487858_570681900_1539944661" alt="_1526487858_570681900_1539944661_1526487858_570681900.png"></p>
<ul>
<li>我们要在右边画出代价函数的图像 上一次 我们是只有一个θ1 也就是说 画出的代价函数是关于θ1的函数 但现在我们有两个参数 θ0和θ1 因此图像就会复杂一些了 当只有一个参数θ1的时候 我们画出来是这样一个弓形函数 而现在我们有了两个参数 那么代价函数 仍然呈现类似的某种弓形 实际上这取决于训练样本</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487889_1193089078_1539944694" alt="_1526487889_1193089078_1539944694_1526487889_1193089078.png"></p>
<ul>
<li>你可能会得到这样的图形 因此这是一个三维曲面图 两个轴分别表示θ0和θ1 随着你改变θ0和θ1的大小 你便会得到不同的代价函数 J(θ0,θ1) 对于某个特定的点 (θ0,θ1) 这个曲面的高度 也就是竖直方向的高度 就表示代价函数 J(θ0,θ1) 的值 不难发现这是一个弓形曲面</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487921_1156596403_1539944720" alt="_1526487921_1156596403_1539944720_1526487921_1156596403.png"></p>
<ul>
<li>我们来看看三维图 这是这个曲面的三维图 水平轴是θ0、θ1 竖直方向表示 J(θ0,θ1) 旋转一下这个图 你就更能理解这个弓形曲面所表示的代价函数了 在这段视频的后半部分 为了描述方便 我将不再像这样给你用三维曲面图的方式解释代价函数J 而还是<strong>用轮廓图来表示</strong> .<code>contour plot</code> 或 <code>contour figure</code> 意思一样</li>
<li>下图就是一个轮廓图 两个轴分别表示 θ0 和 θ1 而这些一圈一圈的椭圆形 <strong>每一个圈就表示 J(θ0,θ1) 相同的所有点的集合</strong> 具体举例来说 我们选三个点出来 这三个桃红色的点 都表示相同的 J(θ0,θ1) 的值 对吧 横纵坐标分别是θ0 θ1 这三个点的 J(θ0,θ1) 值是相同的 如果你之前没怎么接触轮廓图的话 你就这么想 你就想象一个弓形的函数从屏幕里冒出来  因此<strong><code>最小值</code> 也就是这个弓形的最低点就是这个点 对吧 也就是这一系列同心椭圆的<code>中心点</code></strong> 想象一下这个弓形从屏幕里冒出来 所以这些椭圆形 都从我的屏幕上冒出相同的高度 弓形的最小值点是这个位置 因此轮廓图是一种很方便的方法 能够直观地观察 代价函数J</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487962_306972773_1539944750" alt="_1526487962_306972773_1539944750_1526487962_306972773.png"></p>
<ul>
<li>接下来让我们看几个例子 在这里有一点 这个点表示<strong>θ0等于800 θ1大概等于-0.15</strong> 那么这个<strong>红色的点 代表了某个 (θ0,θ1) 组成的数值组</strong> 而这个点也对应于左边这样一条线 对吧 θ0等于800 也就是跟纵轴相交于大约800 斜率大概是-0.15 当然 这条线并不能很好地拟合数据 对吧 以这组 θ0 θ1 为参数的这个假设 h(x) 并不是数据的较好拟合 并且你也发现了 这个代价值 就是这里的这个值 距离最小值点还很远 也就是说这个代价值还是算比较大的 因此不能很好拟合数据</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526487994_1194723141_1539944788" alt="_1526487994_1194723141_1539944788_1526487994_1194723141.png"></p>
<ul>
<li>让我们再来看几个例子 这是另一个假设 你不难发现 这依然不是一个好的拟合 但比刚才稍微好一点 这是我的 θ0 θ1 点 这是 <strong>θ0 的值 大约为360 θ1 的值为0</strong> 我们把它写下来 θ0=360 θ1=0 因此这组θ值对应的假设是 这条水平的直线 也就是<strong>h(x) = 360 + 0 × x</strong> 这就是<code>假设</code> 这个假设同样也有某个<code>代价值</code> 而这个<strong>代价值就对应于这个代价函数在这一点的高度</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526488011_1542676591_1539944816" alt="_1526488011_1542676591_1539944816_1526488011_1542676591.png"></p>
<ul>
<li>最后一个例子 这个点其实不是最小值 但已经非常靠近最小值点了 这个点对数据的拟合就很不错 它对应这样两个θ0 和 θ1 的值 同时也对应这样一个 h(x) 这个点虽然不在最小值点 但非常接近了 因此误差平方和 或者说 <strong>训练样本和假设的距离的平方和 这个距离值的平方和 非常接近于最小值</strong> 尽管它还不是最小值</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526488021_554234801_1539944853" alt="_1526488021_554234801_1539944853_1526488021_554234801.png"></p>
<ul>
<li>好的 通过这些图形 我希望你能更好地 理解这些<code>代价函数 J</code> 所表达的值 它们是什么样的 它们对应的假设是什么样的 以及什么样的假设对应的点 更接近于代价函数J的最小值 当然 我们真正需要的是一种有效的算法 能够自动地找出这些使代价函数J取最小值的参数θ0和θ1来 对吧 我想我们也不希望编个程序 把这些点画出来 然后人工的方法来读出这些点的数值 这很明显不是一个好办法 事实上 我们后面就会学到 我们会遇到更复杂、更高维度、更多参数的情况 这在我们在后面的视频中很快就会遇到 而这些情况是很难画出图的 因此更无法将其可视化 因此<strong>我们真正需要的 是编写程序来找出这些最小化代价函数的θ0和θ1的值</strong> 在下一节视频中 我们将介绍一种算法 能够自动地找出能使代价函数 J 最小化的参数θ0和θ1的值</li>
</ul>
<h3 id="Cost-Function-代价函数-Intuition-II笔记"><a href="#Cost-Function-代价函数-Intuition-II笔记" class="headerlink" title="Cost Function(代价函数) - Intuition II笔记"></a>Cost Function(代价函数) - Intuition II笔记</h3><ul>
<li><code>轮廓图（或者叫等值线图）</code>是包含许多等高线的图。<strong>两个变量函数的轮廓线在同一行的所有点上具有恒定值</strong>。这种图的一个例子就是下面的图。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526488042_674655632_1539944900" alt="_1526488042_674655632_1539944900_1526488042_674655632.png"></p>
<ul>
<li>采取任何颜色并沿着“圆”走，人们会期望获得相同的成本函数值。例如，上面绿线上的三个绿色点对于J（θ0，θ1）具有相同的值，因此它们沿着同一条线发现。带圆圈的x显示θ0= 800和θ1= -0.15时左侧图形的成本函数值。再取一个h（x）并绘制其轮廓图（等值线图），可以得到以下图表：</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526488050_330535995_1539944915" alt="_1526488050_330535995_1539944915_1526488050_330535995.png"></p>
<ul>
<li>当θ0= 360且θ1= 0时，等值线图中J（θ0，θ1）的值<strong>更靠近中心，从而降低了代价函数误差</strong>。现在给我们的假设函数一个稍微正向的斜率会导致更好的数据拟合。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526488073_471902153_1539944930" alt="_1526488073_471902153_1539944930_1526488073_471902153.png"></p>
<ul>
<li>上面的图尽可能地降低了成本函数，因此，θ1和θ0的结果分别趋于0.12和250左右。将我们图上的这些值绘制在右侧似乎将我们的观点<strong>置于最内层“圈子”的中心</strong>。</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.fengwenhua.top/2018/11/15/零-机器学习介绍/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="冯文华">
      <meta itemprop="description" content="记录日常学习与生活">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="江南小虫虫的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2018/11/15/零-机器学习介绍/" class="post-title-link" itemprop="http://www.fengwenhua.top/index.html">零:机器学习介绍</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-11-14 20:01:44 / 修改时间：12:12:26" itemprop="dateCreated datePublished" datetime="2018-11-14T20:01:44Z">2018-11-14</time>
            

            
              

              
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
                  ，
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/机器学习/机器学习入门/" itemprop="url" rel="index"><span itemprop="name">机器学习入门</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/11/15/零-机器学习介绍/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/11/15/零-机器学习介绍/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/11/15/零-机器学习介绍/" class="leancloud_visitors" data-flag-title="零:机器学习介绍">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数：</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">7.2k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">12 分钟</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        
          
            <h1 id="零：机器学习介绍"><a href="#零：机器学习介绍" class="headerlink" title="零：机器学习介绍"></a>零：机器学习介绍</h1><blockquote>
<p><a href="https://www.coursera.org/learn/machine-learning/" target="_blank" rel="noopener">视频地址</a></p>
</blockquote>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><h3 id="定义1"><a href="#定义1" class="headerlink" title="定义1"></a>定义1</h3><pre><code>给予计算机学习能力的领域。
</code></pre><ul>
<li>Samuel的定义可以回溯到50年代，他编写了一个西洋棋程序。 这程序神奇之处在于，编程者自己并不是个下棋高手。 但因为他太菜了，于是就通过编程， 让西洋棋程序自己跟自己下了上万盘棋。通过观察 哪种布局（棋盘位置）会赢，哪种布局会输， 久而久之，这西洋棋程序明白了什么是好的布局， 什么样是坏的布局。然后就牛逼大发了，程序通过学习后， 玩西洋棋的水平超过了Samuel。这绝对是令人注目的成果。 尽管编写者自己是个菜鸟，但因为 计算机有着足够的耐心，去下上万盘的棋， 没有人有这耐心去下这么多盘棋。通过这些练习， 计算机获得无比丰富的经验，于是渐渐成为了 比Samuel更厉害的西洋棋手。<h3 id="定义2"><a href="#定义2" class="headerlink" title="定义2:"></a>定义2:</h3>  由Tom Mitchell提出，来自卡内基梅隆大学.一个好的学习一个程序被认为能从经验E中学习，解决任务 T，达到 性能度量值P，当且仅当，有了经验E后，经过P评判， 程序在处理 T 时的性能有所提升</li>
<li>在西洋棋那例子中，<ul>
<li><code>经验E</code> :程序上万次的自我练习的经验</li>
<li><code>任务T</code> :就是下棋。</li>
<li><code>性能度量值p</code>:，就是它在与一些新的对手比赛时，赢得比赛的概率。</li>
</ul>
</li>
<li>目主要的两种类型学习算法被我们称之为<code>监督学习</code>和<code>无监督学习</code>。<ul>
<li><code>监督学习</code>:我们将教计算机如何去完成任务</li>
<li><code>无监督学习</code>:让它自己进行学习。<h2 id="监督学习-Supervised-Learning"><a href="#监督学习-Supervised-Learning" class="headerlink" title="监督学习(Supervised Learning)"></a>监督学习(Supervised Learning)</h2><h3 id="例子1"><a href="#例子1" class="headerlink" title="例子1:"></a>例子1:</h3></li>
</ul>
</li>
<li>假设你想预测房价， 之前，某学生已经从某地收集了数据集其中一个数据集是这样的。 这是横坐标，即不同房子的面积，单位平方脚（^-^） 纵轴上是房价，单位 千美元。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485168_1551005604_1539942902" alt="_1526485168_1551005604_1539942902_1526485168_1551005604.png"></p>
<ul>
<li>根据给定数据，假设你朋友有栋房子，750平尺（70平米） 想知道这房子能卖多少，好卖掉。 那么，学习算法怎么帮你呢？学习算法可以： 绘出一条直线，让直线尽可能匹配到所有数据。 基于此，看上去，那个房子应该、可能、也许、大概 卖到15万美元（一平米两千刀！）。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485361_1959833259_1539942923" alt="_1526485361_1959833259_1539942923_1526485361_1959833259.png"></p>
<ul>
<li>但这不是唯一的学习算法。 可能还有更好的。比如不用直线了， 可能平方函数会更好， 即二次多项式更符合数据集。如果你这样做， 预测结果就应该是20万刀（一平三千刀，涨价好快）。 后面我们会介绍到如何选择 是选择直线还是平方函数来拟合。 没有明确的选择，就不知哪个能给你的朋友 更好的卖房建议。只是这些每个都是很好的学习算法例子。 也是监督学习的例子。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485380_1269206818_1539942939" alt="_1526485380_1269206818_1539942939_1526485380_1269206818.png"></p>
<p>术语监督学习，意指<strong>给出一个算法， 需要部分数据集已经有正确答案</strong>。比如给定房价数据集， 对于里面每个数据，算法都知道对应的正确房价， 即这房子实际卖出的价格。<strong>算法的结果就是 算出更多的正确价格</strong>，比如那个新房子， 你朋友想卖的那个。用更术语的方式来定义， <strong><code>监督学习又叫回归问题(regression problem)</code>，（应该是回归属于监督中的一种） 意指要预测一个<code>连续值</code>的输出</strong>，比如房价。 虽然从技术上，一般把房价记到美分单位。 所以实际还是个离散值，但通常把它看作实际数字， 是一个标量值，一个连续值的数，而术语<strong>回归， 意味着要预测这类连续值属性的种类</strong>。</p>
<h3 id="例子2"><a href="#例子2" class="headerlink" title="例子2:"></a>例子2:</h3><ul>
<li>另一个监督学习的例子，我和一些朋友 之前研究的领域。让我们来看医学记录， 并预测胸部肿瘤是恶性良性。 如果某人发现有胸部肿瘤，恶性肿瘤有害又危险， 良性肿瘤则是少害。 显然人们很关注这个。让我们看一个收集好的数据集，</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485436_1662003455_1539942955" alt="_1526485436_1662003455_1539942955_1526485436_1662003455.png"></p>
<ul>
<li>假设在数据集中，<strong>横轴表示肿瘤的大小， 纵轴我打算圈上0或1，是或否， 即肿瘤是恶性的还是良性的。</strong> 所以如图所示，可以看到这个大小的肿瘤块 是良性的，还有这些大小的都是良性的。 不幸地是也看到一些恶性肿瘤，比如这些大小的肿瘤。 所以，有5个良性块，在这一块， 还有5个恶性的，它们纵轴值为1.</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485466_72624286_1539942971" alt="_1526485466_72624286_1539942971_1526485466_72624286.png"></p>
<ul>
<li>现在假设某人杯具地得胸部肿瘤了， 大小大概是这么大。 对应的机器学习问题就是，你能否估算出一个概率， 即肿瘤为恶或为良的概率？ 专业地说，这是个<code>分类问题(classification problem)</code>。 <strong>分类是要预测一个<code>离散值</code>输出</strong>。 这里是0或1，恶性或良性。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485496_2090819363_1539943001" alt="_1526485496_2090819363_1539943001_1526485496_2090819363.png"></p>
<ul>
<li>事实证明， 在分类问题中，有时会有超过两个的值， 输出的值可能超过两种。举个具体例子， 胸部肿瘤可能有三种类型，所以要预测离散值0，1，2，3 ，假设总共有三种癌症。<ul>
<li>0就是良性肿瘤，没有癌症。</li>
<li>1 表示1号癌症</li>
<li>2 是2号癌症，</li>
<li>3 就是3号癌症</li>
<li>这同样是个<strong>分类问题，因为它的输出的离散值集合</strong> 分别对应于无癌，1号，2号，3号癌症</li>
</ul>
</li>
<li>我再露一小手，在分类问题中，还有另一种作图方式 来描述数据—我画你猜。<strong>要用到些许不同的符号集合来描绘数据</strong>。如果肿瘤大小作为唯一属性， 被用于预测恶性良性，可以把数据作图成这样。 使用不同的符号来表示良性和 恶性，即阴性和阳性。所以，不再统一画叉叉了， 改用圈圈来代表良性肿瘤，就像这样。 仍沿用X（叉叉）代表恶性肿瘤。希望你能明白。 我所做的就是，把在上面的数据， 映射下来。再用不同的符号， 圈和叉来分别代表良性和恶性。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485537_359149279_1539943034" alt="_1526485537_359149279_1539943034_1526485537_359149279.png"></p>
<ul>
<li><strong>在上例中，只使用了一个特征属性，即肿瘤块大小， 来预测肿瘤是恶性良性</strong>。在其它机器学习问题里， 有着不只一个的特征和属性。 例子，现在不只是知道肿瘤大小， 病人年龄和肿瘤大小都知道了。这种情况下， 数据集如表图所示，有些病人，年龄、肿瘤已知， 不同的病人，会有一点不一样， 肿瘤恶性，则用叉来代表。所以，假设 有一朋友得了肿瘤。肿瘤大小和年龄 落在此处。那么依据这个给定的数据集，学习算法 所做的就是画一条直线，分开 恶性肿瘤和良性肿瘤，所以<strong>学习算法会 画条直线</strong>，像这样，把两类肿瘤分开。 然后你就能判断你朋友的肿瘤是…了 如果它在那边，学习算法就说 你朋友的肿瘤在良性一边，因此更可能 是良性的。</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485695_454140810_1539943078" alt="_1526485695_454140810_1539943078_1526485695_454140810.png"></p>
<ul>
<li>好，本例中，总共有两个特征， 即<strong>病人年龄和肿瘤大小</strong>。在别的ML问题中， 经常会用到更多特征，我朋友研究这个问题时， 通常使用这些特征：比如块的厚度，即胸部肿瘤的厚度 肿瘤细胞大小和形状的一致性， 等等。它表明， 最有趣的学习算法（本课中将学到） 能够处理，无穷多个特征。不是3到5个这么少。 在这张幻灯片中，我已经列举了总共5个不同的特征。 但对于一些学习问题， 真要用到的不只是三五个特征， 要用到无数多个特征，非常多的属性， 所以，你的学习算法要使用很多的属性 或特征、线索来进行预测。</li>
<li>那么，你如何<strong>处理无限多特征呢</strong>？甚至你如何存储无数的东西 进电脑里，又要避免内存不足？ 事实上，等我们介绍一种叫<strong>支持向量机</strong>的算法时， 就知道存在一个简洁的数学方法，能让电脑处理无限多的特征。 想像下，我不是这边写两个特征， 右边写三个特征。而是，写一个无限长的特征表， 不停地写特征，似乎是个无限长的特征的表。 但是，我们也有能力设计一个算法来处理这个问题。</li>
<li>本课中，我们介绍监督学习。 其基本思想是，<strong>监督学习中，对于数据集中的每个数据， 都有相应的正确答案（训练集） ,算法就是基于这些来做出预测</strong>.就像那个房价， 或肿瘤的性质。后面介绍了回归问题。 即<strong>通过回归来预测一个连续值输出</strong>。 我们还谈到了<strong>分类问题， 目标是预测离散值输出</strong>。</li>
<li>下面是个小测验题目： 假设你有家公司，希望研究相应的学习算法去 解决两个问题。第一个问题，你有一堆货物的清单。 假设一些货物有几千件可卖， 你想预测出，你能在未来三个月卖出多少货物。 第二个问题，你有很多用户， 你打算写程序来检查每个用户的帐目。 对每个用户的帐目， 判断这个帐目是否被黑过（hacked or compromised）。 请问，这两个问题是分类问题，还是回归问题？</li>
<li>问题一是个回归问题 因为如果我有几千件货物， 可能只好把它当作一个实际的值，一个连续的值。 也把卖出的数量当作连续值。 第二个问题，则是分类问题，因为可以把 我想预测的一个值设为0，来表示账目没有被hacked 另一个设为1，表示已经被hacked。 就像乳癌例子中，0表示良性，1表示恶性。 所以这个值为0或1，取决于是否被hacked， 有算法能预测出是这两个离散值中的哪个。 因为只有少量的离散值，所以这个就是个分类问题。</li>
</ul>
<h3 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h3><ul>
<li>在监督式学习中，我们给了一个数据集，并且已经知道我们的正确输出应该是什么样子，并且有输入和输出之间有关系的想法。</li>
<li>监督学习问题分为“<code>回归</code>”和“<code>分类</code>”问题。<ul>
<li>在<code>回归问题</code>中，我们试图预测<code>连续输出中</code>的结果，这意味着我们试图将<code>输入变量</code>映射到某个<code>连续函数</code>。</li>
<li>在<code>分类问题</code>中，我们试图预测<code>离散输出</code>的结果。换句话说，我们试图将<code>输入变量</code>映射到<code>离散类别</code></li>
</ul>
</li>
<li>例子<ol>
<li>回归 - 给定一个人的照片，我们必须根据给定的图片来预测他们的年龄</li>
<li>分类 - 给予患有肿瘤的患者，我们必须预测肿瘤是恶性的还是良性的。</li>
</ol>
</li>
</ul>
<h2 id="无监督学习-Unsupervised-Learning"><a href="#无监督学习-Unsupervised-Learning" class="headerlink" title="无监督学习(Unsupervised Learning)"></a>无监督学习(Unsupervised Learning)</h2><blockquote>
<p>在上一节视频中 我们已经讲过了监督学习 回想起上次的数据集 每个样本 都已经被标明为 正样本或者负样本 即良性或恶性肿瘤</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485792_1823245979_1539943092" alt="_1526485792_1823245979_1539943092_1526485792_1823245979.png"></p>
<ul>
<li>因此 <strong>对于监督学习中的每一个样本 我们已经被清楚地告知了 什么是所谓的正确答案</strong> 即它们是良性还是恶性 在无监督学习中 我们用的数据会和监督学习里的看起来有些不一样 <strong>在无监督学习中 没有属性或标签这一概念 也就是说所有的数据都是一样的,没有区别</strong></li>
<li>所以在无监督学习中 我们只有一个数据集 没人告诉我们该怎么做 我们也不知道每个数据点究竟是什么意思,相反 它只告诉我们:<strong>现在有一个数据集 你能在其中找到某种结构吗?</strong> 对于给定的数据集,无监督学习算法可能判定,<strong>该数据集包含两个不同的聚类</strong>,这是第一个聚类,然后这是另一个聚类 .<strong>无监督学习算法 会把这些数据分成两个不同的聚类</strong>,所以这就是所谓的<code>聚类算法</code></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485838_932487436_1539943106" alt="_1526485838_932487436_1539943106_1526485838_932487436.png"></p>
<h3 id="例子1-1"><a href="#例子1-1" class="headerlink" title="例子1"></a>例子1</h3><ul>
<li>我们来举一个聚类算法的例子 Google 新闻的例子 如果你还没见过这个页面的话 你可以到这个URL:news.google.com 去看看谷歌新闻每天都在干什么呢？ 他们每天会去收集成千上万的网络上的新闻然后将他们<strong>分组组成一个个新闻专题</strong></li>
<li>比如 让我们来看看这里,这里的URL链接连接着不同的有关BP油井事故的报道,所以 让我们点击 这些URL中的一个 恩 让我们点一个 然后我们会来到这样一个网页 这是一篇来自华尔街日报的 有关……你懂的 有关BP油井泄漏事故的报道 标题为《BP杀死了Macondo》 Macondo 是个地名 就是那个漏油事故的地方 如果你从这个组里点击一个不同的URL 那么你可能会得到不同的新闻 这里是一则CNN的新闻 是一个有关BP石油泄漏的视频 如果你再点击第三个链接 又会出现不同的新闻 这边是英国卫报的报道 也是关于BP石油泄漏 所以 谷歌新闻所做的就是 去搜索成千上万条新闻 然后自动的将他们聚合在一起 因此 有关同一主题的 新闻被显示在一起 实际上 聚类算法和无监督学习算法 也可以被用于许多其他的问题</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485861_1249334585_1539943121" alt="_1526485861_1249334585_1539943121_1526485861_1249334585.png"></p>
<h3 id="例子2-1"><a href="#例子2-1" class="headerlink" title="例子2"></a>例子2</h3><ul>
<li>这里我们举个它在基因组学中的应用</li>
<li>下面是一个关于基因芯片的例子 基本的思想是 <strong>给定一组不同的个体,对于每个个体,检测它们是否拥有某个特定的基因</strong> 也就是说，你要去分析有多少基因显现出来了 因此 这些颜色 红 绿 灰 等等 它们 展示了这些不同的个体 是否拥有一个特定基因 的不同程度</li>
<li>然后你能做的就是 运行一个聚类算法 把不同的个体归入不同的类 或归为不同类型的人</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485895_1918759847_1539943134" alt="_1526485895_1918759847_1539943134_1526485895_1918759847.png"></p>
<ul>
<li>这就是无监督学习 我们没有提前告知这个算法 这些是第一类的人 这些是第二类的人 这些是第三类的人等等 相反我们只是告诉算法 你看 这儿有一堆数据 我不知道这个数据是什么东东 我不知道里面都有些什么类型 叫什么名字 我甚至不知道都有哪些类型 但是 请问你可以自动的找到这些数据中的类型吗？ 然后自动的 按得到的类型把这些个体分类 虽然事先我并不知道哪些类型 因为对于这些数据样本来说 我们<code>没有给算法一个正确答案,所以这就是无监督学习</code></li>
</ul>
<h3 id="其他例子"><a href="#其他例子" class="headerlink" title="其他例子"></a>其他例子</h3><ul>
<li>无监督学习或聚类算法在其他领域也有着大量的应用</li>
</ul>
<h4 id="它被用来组织大型的计算机集群"><a href="#它被用来组织大型的计算机集群" class="headerlink" title="它被用来组织大型的计算机集群"></a>它被用来组织大型的计算机集群</h4><ul>
<li>我有一些朋友在管理 大型数据中心 也就是 大型计算机集群 并试图 找出哪些机器趋向于 协同工作 如果你把这些机器放在一起 你就可以让你的数据中心更高效地工作</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485943_1884734823_1539943144" alt="_1526485943_1884734823_1539943144_1526485943_1884734823.png"></p>
<h4 id="用于社交网络的分析"><a href="#用于社交网络的分析" class="headerlink" title="用于社交网络的分析"></a>用于社交网络的分析</h4><ul>
<li>所以 如果可以得知 哪些朋友你用email联系的最多 或者知道你的Facebook好友 或者你Google+里的朋友 知道了这些之后 我们是否可以自动识别 哪些是很要好的朋友组 哪些仅仅是互相认识的朋友组</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526485965_1931576913_1539943156" alt="_1526485965_1931576913_1539943156_1526485965_1931576913.png"></p>
<h4 id="还有在市场分割中的应用"><a href="#还有在市场分割中的应用" class="headerlink" title="还有在市场分割中的应用"></a>还有在市场分割中的应用</h4><ul>
<li>许多公司拥有庞大的客户信息数据库,那么,给你一个客户数据集,你能否自动找出不同的市场分割,并自动将你的客户分到不同的细分市场中.从而有助于我在不同的细分市场中进行更有效的销售</li>
<li>这也是无监督学习 我们现在有 这些客户数据 但我们预先并不知道 有哪些细分市场 而且 对于我们数据集的某个客户 我们也不能预先知道 谁属于细分市场一 谁又属于细分市场二等等 但我们必须让这个算法自己去从数据中发现这一切</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486000_1961140321_1539943168" alt="_1526486000_1961140321_1539943168_1526486000_1961140321.png"></p>
<h4 id="天文数据分析-通过这些聚类算法"><a href="#天文数据分析-通过这些聚类算法" class="headerlink" title="天文数据分析 通过这些聚类算法"></a>天文数据分析 通过这些聚类算法</h4><ul>
<li>我们发现了许多 惊人的、有趣的 以及实用的 关于星系是如何诞生的理论 所有这些都是聚类算法的例子 而<strong>聚类只是无监督学习的一种</strong></li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486023_1506023228_1539943178" alt="_1526486023_1506023228_1539943178_1526486023_1506023228.png"></p>
<h4 id="鸡尾酒宴问题"><a href="#鸡尾酒宴问题" class="headerlink" title="鸡尾酒宴问题"></a>鸡尾酒宴问题</h4><ul>
<li>恩 我想你参加过鸡尾酒会的 是吧？ 嗯 想象一下 有一个宴会 有一屋子的人 大家都坐在一起 而且在同时说话 有许多声音混杂在一起 因为每个人都是在同一时间说话的 在这种情况下你很难听清楚你面前的人说的话</li>
<li>因此 比如有这样一个场景 宴会上只有两个人 两个人 同时说话 恩 这是个很小的鸡尾酒宴会 我们准备好了两个麦克风 把它们放在房间里 然后 因为这两个麦克风距离这两个人 的距离是不同的 每个麦克风都记录下了 来自两个人的声音的不同组合</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486053_1219325593_1539943187" alt="_1526486053_1219325593_1539943187_1526486053_1219325593.png"></p>
<ul>
<li>也许A的声音 在第一个麦克风里的声音会响一点 也许B的声音 在第二个麦克风里会比较响一些 因为2个麦克风 的位置相对于 2个说话者的位置是不同的 但每个麦克风都会录到 来自两个说话者的重叠部分的声音</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486064_1893704755_1539943199" alt="_1526486064_1893704755_1539943199_1526486064_1893704755.png"></p>
<ul>
<li>一个研究员录下的两个说话者的声音,<strong>无监督学习算法可以将两种混合在一起的音频分开</strong></li>
<li>所以,你可以看到,像这样的无监督学习算法,看起来 为了 构建这个应用程序 做这个音频处理 似乎需要写好多代码啊 或者需要链接到 一堆处理音频的Java库 貌似需要一个非常复杂的程序,分离出音频等</li>
<li>实际上,要实现混合音频分离的效果,只需要一行代码就可以了</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486082_1327522151_1539943212" alt="_1526486082_1327522151_1539943212_1526486082_1327522151.png"></p>
<ul>
<li>当然 研究人员 花了很长时间才想出这行代码的,我不是说这是一个简单的问题 但事实上 如果你 使用正确的编程环境 许多学习 算法是用很短的代码写出来的</li>
<li>所以这也是为什么在 这门课中我们要 使用<strong>Octave</strong>的编程环境</li>
<li>Octave是一个免费的 开放源码的软件 使用Octave或Matlab这类的工具 许多学习算法 都可以用几行代码就可以实现 在后续课程中 我会教你如何使用Octave 你会学到 如何在Octave中实现这些算法 或者 如果你有Matlab 你可以用它 事实上 在硅谷 很多的机器学习算法 我们都是先用Octave 写一个程序原型 因为在Octave中实现这些 学习算法的速度快得让你无法想象</li>
<li>在这里 每一个函数 例如 SVD 意思是奇异值分解 但这其实是解线性方程 的一个惯例 它被内置在Octave软件中了</li>
<li>如果你试图 在C + +或Java中做这个 将需要写N多代码 并且还要连接复杂的C + +或Java库 所以 你可以在C++或 Java或Python中 实现这个算法 只是会 更加复杂而已</li>
<li>如果你使用Octave的话 会学的更快 并且如果你用 Octave作为你的学习工具 和开发原型的工具 你的学习和开发过程 会变得更快</li>
<li>而事实上在硅谷 很多人会这样做 他们会先用Octave 来实现这样一个学习算法原型 只有在确定 这个算法可以工作后 才开始迁移到 C++ Java或其它编译环境 事实证明 这样做 实现的算法 比你一开始就用C++ 实现的算法要快多了</li>
<li>我们谈到了无监督学习 它是一种学习机制 你给算法大量的数据 要求它找出数据中 蕴含的类型结构 以下的四个例子中 哪一个 您认为是 无监督学习算法 而不是监督学习问题</li>
</ul>
<p><img src="https://raw.githubusercontent.com/fengwenhua/ImageBed/master/_1526486136_2099457706_1539943241" alt="_1526486136_2099457706_1539943241_1526486136_2099457706.png"></p>
<pre><code>* 恩 没忘记垃圾邮件文件夹问题吧？ 如果你已经标记过数据 那么就有垃圾邮件和 非垃圾邮件的区别 我们会将此视为一个监督学习问题
* 新闻故事的例子 正是我们在本课中讲到的 谷歌新闻的例子 我们介绍了你可以如何使用 聚类算法这些文章聚合在一起 所以这是无监督学习问题
* 市场细分的例子 我之前有说过 这也是一个无监督学习问题 因为我是要 拿到数据 然后要求 它自动发现细分市场
* 最后一个例子 糖尿病 这实际上就像我们 上节课讲到的乳腺癌的例子 只不过这里不是 好的或坏的癌细胞 良性或恶性肿瘤我们 现在是有糖尿病或 没有糖尿病 所以这是 有监督的学习问题 像处理那个乳腺癌的问题一样 我们会把它作为一个 有监督的学习问题来处理
</code></pre><h3 id="笔记-1"><a href="#笔记-1" class="headerlink" title="笔记"></a>笔记</h3><ul>
<li>无监督的学习使我们能够很少或根本不知道我们的结果应该是什么样子。 我们可以从数据中推导出结构，我们不一定知道变量的影响。</li>
<li>我们可以通过基于数据中变量之间的关系对数据进行聚类来推导出这种结构。</li>
<li>在无监督学习的基础上，没有基于预测结果的反馈。</li>
<li>例：<ul>
<li><code>聚类(Clustering)</code>：搜集一百万个不同的基因，并找到一种方法，将这些基因自动分组，这些基因组通过不同的变量（例如寿命，位置，角色等）相似或相关。</li>
<li><code>非聚类(Non-clustering)</code>：“鸡尾酒会算法”，可以让你在混乱的环境中找到结构。 （即在鸡尾酒会上从声音网格中识别个别的声音和音乐）。</li>
</ul>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">冯文华</p>
              <p class="site-description motion-element" itemprop="description">记录日常学习与生活</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                  
                    
                  
                  <a href="https://github.com/fengwenhua" title="GitHub &rarr; https://github.com/fengwenhua" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://lfasd.top" title="http://lfasd.top" rel="noopener" target="_blank">Lfasd</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">by fengwenhua</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
    <span title="站点总字数">54k</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
    <span title="站点阅读时长">1:31</span>
  
</div>










        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  
    
      
  
  <script type="text/javascript" color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>













  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.5.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.5.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=6.5.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=6.5.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.5.0"></script>



  



  








  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: true,
        notify: true,
        appId: 'sDPAkIqXzfXIejgNvNYYQ1Ji-gzGzoHsz',
        appKey: '3yyuJ2OpBOhCms7yzBmA0Xbq',
        placeholder: 'Just go go',
        avatar:'mm',
        meta:guest,
        pageSize:'10' || 10,
        visitor: true
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      
        // ref: https://github.com/ForbesLindesay/unescape-html
        var unescapeHtml = function(html) {
          return String(html)
            .replace(/&quot;/g, '"')
            .replace(/&#39;/g, '\'')
            .replace(/&#x3A;/g, ':')
            // replace all the other &#x; chars
            .replace(/&#(\d+);/g, function (m, p) { return String.fromCharCode(p); })
            .replace(/&lt;/g, '<')
            .replace(/&gt;/g, '>')
            .replace(/&amp;/g, '&');
        };
      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                content = unescapeHtml(content);
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
  

  


  
  

  

  

  

  

  
  <style>
    .copy-btn {
      display: inline-block;
      padding: 6px 12px;
      font-size: 13px;
      font-weight: 700;
      line-height: 20px;
      color: #333;
      white-space: nowrap;
      vertical-align: middle;
      cursor: pointer;
      background-color: #eee;
      background-image: linear-gradient(#fcfcfc, #eee);
      border: 1px solid #d5d5d5;
      border-radius: 3px;
      user-select: none;
      outline: 0;
    }

    .highlight-wrap .copy-btn {
      transition: opacity .3s ease-in-out;
      opacity: 0;
      padding: 2px 6px;
      position: absolute;
      right: 4px;
      top: 8px;
    }

    .highlight-wrap:hover .copy-btn,
    .highlight-wrap .copy-btn:focus {
      opacity: 1
    }

    .highlight-wrap {
      position: relative;
    }
  </style>
  <script>
    $('.highlight').each(function (i, e) {
      var $wrap = $('<div>').addClass('highlight-wrap')
      $(e).after($wrap)
      $wrap.append($('<button>').addClass('copy-btn').append('复制').on('click', function (e) {
        var code = $(this).parent().find('.code').find('.line').map(function (i, e) {
          return $(e).text()
        }).toArray().join('\n')
        var ta = document.createElement('textarea')
        document.body.appendChild(ta)
        ta.style.position = 'absolute'
        ta.style.top = '0px'
        ta.style.left = '0px'
        ta.value = code
        ta.select()
        ta.focus()
        var result = document.execCommand('copy')
        document.body.removeChild(ta)
        
          if(result)$(this).text('复制成功')
          else $(this).text('复制失败')
        
        $(this).blur()
      })).on('mouseleave', function (e) {
        var $b = $(this).find('.copy-btn')
        setTimeout(function () {
          $b.text('复制')
        }, 300)
      }).append(e)
    })
  </script>


  

</body>
</html>
<script type="text/javascript" src="/js/src/love.js"></script>
